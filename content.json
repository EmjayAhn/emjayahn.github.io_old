{"pages":[{"title":"Categories","text":"","link":"/Categories/index.html"},{"title":"about","text":"since 2018년 10월,데이터 사이언스를 공부하고 있습니다.","link":"/about/index.html"},{"title":"all-archives","text":"","link":"/all-archives/index.html"},{"title":"all-categories","text":"","link":"/all-categories/index.html"},{"title":"all-tags","text":"","link":"/all-tags/index.html"}],"posts":[{"title":"181018_DailyScrum","text":"181018오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다.","link":"/2018/10/18/181018-TodayWhatILearned/"},{"title":"181019 TodayWhatILearned","text":"181019오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다. 181019오늘 한 일 Web Crawling 공부 : Selenium을 활용한 크롤링 Pandas 활용하여 여러가지 data import 하고, 정렬 바꾸는 연습 Terminal 환경설정 zsh, oh-my-zsh 설치했다. git을 적극적으로 활용할 계획이므로, git사용에 유리한 zsh 설정을 마쳤다. 팀 프로젝트에 활용할 데이터 셋 탐색 github 블로그 Deploy 에 성공했다. Daily 스크럼을 작성하는 것이 하루의 계획과 정리를 하는데 유용하기에, 꾸준히 작성하면서, github 블로그에도 올려볼 계획이다. 내일 할 일 Pandas 내용 정리, 반복 숙달 Naver Article Crawling, NBA Data Crawling 확률과 통계 정리 (연속확률분포 부분) 뭘 느꼈는가 점차 학습하는 내용과 알아야 될 내용이 많아지면서, 내가 배운 것들, 알고 있는 것들. 정확히는 어떤 것에 관해 존재는 알고 있으나 내 머릿속에서 당장 꺼내서 쓰기에는 어려운 것들이 많아지고 있다. 또한, 내 머릿속에서 꺼내기 쓰기 힘들어 구글을 통해 찾고자하면 내가 기억했던, 알고 있는 정보들과 조금은 내용이 다르고, 이것을 Searching 하는데 쓰는 시간이 아깝다는 생각이 들었다. 앞으로는 매일 학습하는 내용을 바탕으로, 직접 찾기 쉬운 형태로 정리할 필요성을 느꼈다. 개인. WIKI화.","link":"/2018/10/19/181019_TodayWhatILearned/"},{"title":"181022 TodayWhatILearned","text":"181022 TWIL오늘 할 일은 무엇인가 블로그 테마 바꾸기 Selenium 과제 두번째 것 (17:47 - 19:00) 확률 분포 다시 한번 정리하기 데이터구조 (PseudoLinkedList, Stack, Queue)정리 새 맥북 환경 설정 조금씩 마저 더하기 DailyScrum Upload 오늘 한 일은 무엇인가 Selenium NBA Page Crawling 해서, DataFrame으로 정리 (17:47 - 18:55) Stack 정리 Github 설정 내일 할 일은 무엇인가 확률 분포 공부 Queue 정리 무엇을 느꼈는가 맥북을 바꾸면서, 환경 설정하는데 너무 많은 시간이 든다. 틈틈히 하고 있는데도 아직 이전 맥북의 환경에서 100% 똑같은 환경을 만들지 못하고 있다. 장비를 바꿀 때, 혹은 다른 작업 환경에서 연속성을 이어 나가기 위해 나에게 맞는 환경설정도 정리할 필요성을 느낀다. 오늘 공부한 PseudoLinkedList, Stack, Queue 의 활용성에 대해 깊이 고민 해 볼 수 있어서 뿌듯하다. 특히나 PseudoLinkedList의 경우 내가 사용하지 않았던 자료구조나 class를 목적에 맞게 customize 할 수 있는 측면에서 활용성이 높다고 생각된다. 다른 사람이 작성한 class 를 잘 읽어보고, 그 활용도를 높이기 위해 overriding 하는 방법을 틈틈히 챙겨 봐야겠다.","link":"/2018/10/22/181022-TodayWhatILearned/"},{"title":"181023 TodayWhatILearned","text":"181023 TWIL오늘 할 일은 무엇인가 Queue 정리 하기 확률 분포 공부 &lt;스터디&gt;멀티 스레딩 개념을 포함한 네이버 크롤링 코드짜기 오늘 한 일은 무엇인가 &lt;스터디&gt; 멀티 스레딩에 관해 이야기, 공부해볼 정보들 공유, 어떻게 코드를 구성할 것인지 의견을 나누었다. Queue 개념 정리 및 코드 구현 연습 베르누이 분포, 이항분포 정리 및 Searching 블로그 수정 내일 할 일은 무엇인가 오후 4시 멀티 스레드 관련한 내용으로 온라인 스터디 모임 예정 멀티스레딩 개념을 활용한 네이버 크롤링 코드 짜기 각종 확률 분포 및 검정 부분까지 복습 무엇을 느꼈는가 오늘 공부한 testing 시작부분부터는 그간 배웠던 것을 한꺼번에 적용한다. 배웠던 여러가지 분포를 활용하여,가설을 세우고 이 가설이 선택한 분포의 관점에서 봤을 떄, 가설을 선택할지 기각할지에 관한 내용은 점차data를 통해 prediction 을 해가는 과정에 있는 듯한 느낌을 받았다. 아직 검정 과정과 앞으로 공부하게 될 여러가지 분석 모델을 이해하기에 앞서배운 확률분포 부분의 내용이부족한듯하여, 계속 꾸준히 앞부분을 공부해야될 것 같다.","link":"/2018/10/23/181023-TodayWhatILearned/"},{"title":"181026_TodayWhatILearned","text":"181026 TWIL오늘 한 일은 무엇인가 Xpath, Scrapy 를 활용한 Crawling 공부 DataScience 와 DataEngineering 의 차이점 찾아보기 내일 할 일은 무엇인가 확률분포 공부 Scrapy 활용해보기 무엇을 느꼈는가 DataScience 와 DataEngineering 의 공통점과 차이점에 대해 알아보았는데, 아직은 현업에서 각분야가 하고 있는 일이 어떻게 다른지 확실한 감이 오지 않는 것 같다. 내가 명확히 재밌어하는 것은 아직까지는어떤 디테일한 분야가 아니라, 수학적인 것, 프로그래밍 적인 것들을 배운 것을 적용해보고 응용하는 것에흥미를 느끼는 것 같은데 이보다 더 앞서 생각해보고 결정하려고 하니 감이 잘 오지 않는 것 같다. 지금 현재로서는, 다양한 기계학습 알고리즘과 머신러닝 등을 이용하여 prediction 등을 통해 새로운 Insight를 얻어내는 것에 관심이 있는 것 같다. 이 분야를 공부하고 배워 나가면서 엔지니어링 분야를 필요에 의해 점차공부해 나가는 방향으로 삼고 싶다.","link":"/2018/10/26/181026-TodayWhatILearned/"},{"title":"181027-TodayWhatILearned","text":"181027 TWIL오늘 한 일은 무엇인가 확률 수학 공부 자기전 (스터디)데이터구조 부분 1강 듣기내일 할 일은 무엇인가 확률분포를 다시 복습하면서 검정, 추정에서 이어지는 부분 꼼꼼히 공부 데이터 구조 강의 수강 계획 세우기 무엇을 느꼈는가 검정방법론에 대해 좀더 꼼꼼히 보았다. 수학적인 수식들은 수업시간에 다 이해가 되는 편이지만 이 수식들을말로써 표현하고, 글로 풀어쓰는 순간 머리가 빠릿빠릿 안돌아가는 느낌이어서, 한단계한단계 논리적으로 따져가며공부하니 이제야 좀 편해진것 같다. 가설 검정 같은 때에도, 말로 풀어쓰는 것 보다 간단하게 수식으로 표현하고,각각의 p-value 를 확인한뒤 원래 작성했던 H_0, H_a 에 대해 생각해보면 쉽게 되었으나 이것을 말로 표현하고글로 구성하려고 하니 간단한것도 복잡하게 생각했던 것 같다. 결국 내가 알게 된것을 상대방과 논의하고앞으로 만나게 될 클라이언트들을 대상으로 설명해야하는 것이 모두 이런 부분에서 시작되는 것임을 느꼈기에,내 생각과 가정 -&gt; 수식으로 표현 -&gt; 다시 말 혹은 글로 표현 하는 것을 습관처럼 해야겠다.","link":"/2018/10/27/181027-TodayWhatILearned/"},{"title":"181028-TodayWhatILearned","text":"181028 TWIL 오늘 한 일은 무엇인가 검정과 추정 공부 Blog 테마 수정 내일 할 일은 무엇인가 ‘추정’에서 수식 부분 다시 보기 (Study) 데이터 구조 강의 듣기 무엇을 느꼈는가 MaximumLikelihood 를 실제로 손으로 써가며 풀어보는 과정에서, 지금까지 배웠던 수학적 테크닉들이 모두 쓰이는 것을 보고 뿌듯하면서도 재미있었다. 뿌듯한 이유는 아마도 한줄 한줄 써가는 내용이 여태 공부한 수학 개념들로 이루어진것 때문일 것이다. 그 중에서도 다음 식을 전개하는 과정에서 눈에 잘 들어오지 않아 전개하지 못할 때도 있었다. 이것은 아마 앞부분 개념이 그 순간에 적용이 되지 않기 때문이라고 생각된다. 행렬의 내용중 몇가지 특성들과 라그랑주 멀티플라이어에 대한 수식을 틈이 생길떄 챙겨서 봐야겠다. 오늘은 수학을 공부하느라 프로그래밍은 하지 못했다. 중간에 졸린 걸 해소해보고자 블로그 테마 색깔 수정과 그 수정을 위해 블로그의 코드 구조를 본게 전부 였다. 30분 푹빠져서 하다가 주객전도 되지않으려 다음으로 미뤘다. 위 주제들에 관해 알게된 것도 많은 하루였고, 뿌듯한 하루였다.","link":"/2018/10/28/181028-TodayWhatILearned/"},{"title":"181024 TodayWhatILearned","text":"181024 TWIL오늘 할 일은 무엇인가 멀티스레딩으로 네이버 크롤링 코드 작성 오후 4시 스터디 확률 분포 공부 오늘 한 일은 무엇인가 멀티스레딩 개념을 활용한 크롤링 코드 작성 오후 4시 스터디 확률분포 공부 내일 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Pandas 정리해보기 무엇을 느꼈는가 Documentation 과 각종 개념 자료들을 혼자 보고 공부하면서, 파이썬이라는 언어가 단순히 책 한권 끝냈다고 해서 끝나는 언어가 아님을 깨달았다. 기본적인 문법을 금방 익숙해져서, ‘역시 쉬운 언어인가’라고 생각했다가 오늘 다양한 자료를 찾아보고 읽어보면서 언어 하나만해도 아직 공부할게 무궁무진 하다는 것을 깨달았다.","link":"/2018/10/24/181024-TodayWhatILearned/"},{"title":"181029-TodayWhatILearned","text":"181029 TWIL 오늘 한 일은 무엇인가 자료구조 Binary Tree, Stack 으로 Queue구현, Stack 응용 추정 부분 복습 스터디 나갈 방향 이야기 A star algorithm search 내일 할 일은 무엇인가 ‘추정’에서 수식 부분 다시 보기 무엇을 느꼈는가 스터디 첫 모임을 가졌다. 프로그래밍에 관해 알고있는 것들을 활용하고 응용하는 쪽으로 어떻게 하면 될지 많은 의견을 나누었다. 상상의 나래를 펼치니, 타오르는 열정과 함께 만들어보고 싶은 것은 많았으나…. 아직 아는게 많지 않기에.. 다시 현실에 눈을 돌렸다. Maze 문제에서 다른 알고리즘을 통해 구현해보려고 의견이 모아졌다. 직접 찾아서 적용해보는 첫 algorithm 이기에, 직접 찾아가면서 공부하고, 이것을 적용하는 경험을 통해 또 다른 배울 것이 있을 것 같아 매우 기대된다.","link":"/2018/10/29/181029-TodayWhatILearned/"},{"title":"181025_TodayWhatILearned","text":"181025 TWIL오늘 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Selenium, Scrapy, Pillow 활용 공부 오늘 한 일은 무엇인가 Selenium, Pillow 활용 공부 내일 할 일은 무엇인가 확률분포 공부 무엇을 느꼈는가 동영상과 이미지를 처리하는 방법에 대해 공부하면서, Selenium 을 활용해 많은 활용도를 느꼈다.주말을 이용해 동영상이나 이미지들을 Crawling 해서 class 형태로 만드는 방법을 시도해봐야겠다.","link":"/2018/10/25/181025-TodayWhatILearned/"},{"title":"181030-TodayWhatILearned","text":"181030 TWIL 오늘 한 일은 무엇인가 검정, 추정 부분 수식 꼼꼼히 다시 보기 A star(A*) Algorithm 개념 읽기 내일 할 일은 무엇인가 선형회귀분석 정리 (패키지 별로 특징, 메서드 파라미터 위주) 데이터 전처리 부분 공부 Test data 처리는 어떻게 하는 건지 공부해보기 무엇을 느꼈는가 추정부분 수식을 공부하면서 eigenvalue 부분이 기억이 잘 안나던 것을 시간이 될 때 자료들을 챙겨봐야겠다. 오늘 공부한 선형 회귀분석을 배우니 조금이나마 프로젝트를 어떻게 진행해야되는 건지 감이 잡힌 것 같다. 미약한시작을 하기위해 오늘 공부한 개념들을 사용해서 여러가지 돌려보고 데이터를 파악해볼 수 있을 것 같다.하지만, 우리가 가지고 있는 데이터셋이 바로 적용 할 수 없는 현실적인 데이터이기 때문에, 여러가지 전처리작업이 필요할 것 같다. 오늘 배운 것을 적용해보기 위해 내일은 데이터 전처리를 공부해보고, 또한 test data의생성 역시 공부해보아야할 부분인 것 같다.","link":"/2018/10/30/181030-TodayWhatILearned/"},{"title":"181031-TodayWhatILearned","text":"181031 TWIL 오늘 한 일은 무엇인가 선형회귀분석 공부 행렬의 미분 공부 (PROJECTmini)A star Algorithm 개념 정리 (PROJECT)프로젝트 진행 data 탐구 데이터 전처리 공부 내일 할 일은 무엇인가 Project 모임 데이터 전처리, 데이터들의 의미 파악, 예측할 delay 부분 정의하기 무엇을 느꼈는가 오늘은 아침부터 하루종일 컴퓨터 앞에 앉아 있어서, 눈과 목이 너무 아프다. 그래도 새로운 알고리즘 내용에관한 이해와, 앞으로 진행할 메인 프로젝트의 data들의 의미를 파악해서 뿌듯한 하루였다. Data의 의미를파악했으나 이것을 처리하기 위해서 numpy 와 pandas 의 documentation 을 읽으면서 데이터들을 다뤘다.라이브러리와 패키지들의 메소드와 클래스들을 자유자재로 다루기 위해선, 계속 사용해보면서 익혀야 함을 느꼈다.메소드들을 알면 복잡하게 코드를 안짜도 이미 내장되어있는 메소드로 손쉽게 처리할 수 있기 때문이다. data를 혼자 곰곰히 보다, 시간에 관한 column 의 의미를 파악했으나 이것을 손쉽게 합쳐서 데이터들을재정렬하는데 오늘 실패했다. 의미는 파악했으니, 내일 좀더 시도해보면 시간에 관한 data 를 정리할 수 있지않을까 한다.","link":"/2018/10/31/181031-TodayWhatILearned/"},{"title":"181101-TodayWhatILearned","text":"181101 TWIL 오늘 한 일은 무엇인가 Project 모임 Data 일부 전처리 Crawling 공부 내일 할 일은 무엇인가 Database - MySQL 공부 Pandas 라이브러리 살펴보기 miniProject Crawling 주제 선정, 코드짜기 무엇을 느꼈는가 프로젝트 모임에서 우리가 예측할 Data (Departure Delay)를 정의하고, 나는 오늘 시각 data 의 전처리를 하였다. Data 들이 의미없는 값들을 가지고 있을 때 어떻게 처리해야 할지 고민하다가 확실한 답은 못얻은 채, 우선 시각 데이터의 formatting 만 바꾸었다. data들을 어떻게 채워넣어주어야 할지는 조금더 공부해 보아야 할 것 같다. 첫 프로젝트 모임의 느낌이 매우 좋았다. 모임을 하기 전까지는 내가 했던 것들이 맞는가 하는 의구심과 project를 하기에는 아직 부족한 지식과 실력이라는 걱정이 앞섰다. 오늘 모임을 하면서 각자 살펴보았던 data 의 특징들과 앞으로 어떻게 data 를 다듬을 것인지 얘기하면서, 더 좋은 방향과 몰랐던 것들, 알았던 것들을 서로 나누면서 발전적인 대화가 되었다는게 매우 뿌듯하다. 모임에서 받은 좋은 느낌을 이어서, 오늘 내가 맡기로한 부분을 해결하기 위해 앉았고, 또 나름 해결한 부분이 있는 것 같아 작은 성취감을 맛보았다. 협업을 하기 위해 git에 관해 좀더 공부하고 나눌 필요성이 느껴졌다. 특히나 code conflict 가 실제로 발생하기 전에 어떻게 다루어야 하는 것인지 좀더 깊게 공부할 필요성을 느꼈다.","link":"/2018/11/01/181101-TodayWhatILearned/"},{"title":"181102-TodayWhatILearned","text":"181102 TWIL 오늘 한 일은 무엇인가 Database - MySQL 공부 A * 알고리즘 스터디 NaN 값 처리에관한 자료 서칭 내일 할 일은 무엇인가 Project 모임 Pandas 라이브러리 정리 miniProject Crawling 코드짜기 A * 알고리즘 짜기 무엇을 느꼈는가 오늘 스터디에서 A* 알고리즘를 주제로 얘기를 좀더 나누었다. 각자 공부해오신 내용을 바탕으로 알고리즘의흐름이 어떻게 되어가는가 좀더 구체적으로 생각해보고자 했다. 좀더 해결법에 가까워진 느낌을 받았으나,이제는 좀더 구체적으로 구현해보면서 다가오는 문제들을 해결해보고자 했다. 모든걸 완벽하게 이해하고 실현하는것만이 답은 아니기 때문이라고 생각한다. 주말동안은 프로젝트, 미니프로젝트, 알고리즘 적용 코딩, 수학 공부, DB공부.. 산더미지만 하나씩 그어나가야겠다.","link":"/2018/11/02/181102-TodayWhatILearned/"},{"title":"181103-TodayWhatILearned","text":"181103 TWIL 오늘 한 일은 무엇인가 Crawling miniProject 코드 작성 Pandas 라이브러리 정리 Project 주말동안 할일 정하기 내일 할 일은 무엇인가 Project Data 회기 돌려보기 회기 부분 공부 크롤링 프로젝트 개선 사항 고민 (시간이 되면) A* 알고리즘 코드 짜보기 무엇을 느꼈는가 whoscored 사이트의 Player 정보를 크롤링 하는 miniProject 의 코드를 작성하였다. Selenium을통해서 크롤링하는데, headless 적용을 하면 에러가 나는 부분을 어떻게 처리해야할지 고민해보아야 한다.Chrome webdriver 를 열어놓는 환경과 headless 환경의 차이가 있는 것 같은데, 이 부분은 document를 살펴보아야 할 것같다. window창을 열어놓는것 과 그렇지 않은 것의 차이점이 있는지 확인해야한다. 함수로 짠 코드를 class 화 시킬 때는 다루는 범위가 커져야 한다는 강박관념이 있다. class 화 시켰을때의 편의성 부분을 고민하면서 위 생각으로 흐름이 이어지는 것 같은데, 그렇지 않기 위해 class의 장점을좀더 체감해볼 필요가 있다. Linear Regression 강의를 들으면서, LineByLine 수식을 이해하는데는 문제가 없으나 이야기의 큰 그림을 놓치는 경향이 있는 것 같다. 내일은 이 부분을 중점적으로 공부해보고, 메인 프로젝트에 적용해보는 것 까지해봐야겠다.","link":"/2018/11/03/181103-TodayWhatILearned/"},{"title":"181104-TodayWhatILearned","text":"181104 TWIL 오늘 한 일은 무엇인가 회기 부분 공부 Project Data 회기 돌려보기 내일 할 일은 무엇인가 Project 모임 회기 부분 정리 크롤링 프로젝트 개선 사항 고민 (시간이 되면) A* 알고리즘 코드 짜보기 무엇을 느꼈는가 다리를 다치고 통증으로 인해 집중해서 코드를 짜기 힘들었다. 수학 이론을 공부하면서 내일 있을 프로젝트모임을 준비했고, 팀원들께서 만든 코드를 정리하며 데이터를 다시 추출하였다. 이 데이터들을 통해 회기의 몇몇가지를 돌려보았다. 이것을 토대로 내일 팀원들과 함께 이야기를 나눠보며 좀더 어떻게 할 수 있을지 고민해봐야겠다.","link":"/2018/11/04/181104-TodayWhatILearned/"},{"title":"181105-TodayWhatILearned","text":"181105 TWIL 프로젝트 STEP 전처리 Nan 값 처리 해결 Partial Regression Plot 그려보기 오늘 한 일은 무엇인가 프로젝트 모임 Crawling miniProject 와 mainProject 진행 내일 할 일은 무엇인가 프로젝트, 오늘까지 진행사항 코드 정리 수학 회기 전체 정리 프로젝트 생각하기 무엇을 느꼈는가 오늘은 프로젝트 모임을 통해서, 많은 것을 얻었다. OLS function 을 실제로 돌림에 있어서 많은 제약조건이필요함을 알게되었고, 그만큼 데이터전처리가 매우 중요하다는 것을 알게 되었다. 다양한 feature 들마다 다양한 방법으로 전처리를 해주어야 한다. 처음엔 NaN값의 처리 방식에 대해 고민했으나,이제는 각 feature 의 특성들마다 전처리 해주는 방식이 달라져야 하고, 또 이번 고비가 넘어가게 되면좋은 prediction 결과를 얻기 위해 다양한 feature 의 조합이 필요함이 피부에 와닿았다. 만족스런 결과물을 얻기 위해선, 아는게 많은 것 보다, 그 결과물을 만들고자 하는 구성원이 중요함을 느꼈다.알고있는 지식은 해결해야할 문제보다 항상 작기 마련이다. 또한 알고있는 지식이 완벽한지는 계속 스스로 의문을던지며 업데이트 해야만한다. 하지만 이보다 더 중요한 것은 문제에 부딪힐 때마다 의욕적이고, 해결해보고자 하는 팀원들덕분에 오늘의 보람과 뿌듯함을 얻을 수 있었던 것 같다.","link":"/2018/11/05/181105-TodayWhatILearned/"},{"title":"181107-TodayWhatILearned","text":"181107 TWIL 오늘 한 일은 무엇인가 Crawling Project 정리 및 제출 내일 할 일은 무엇인가 Prediction Project","link":"/2018/11/07/181107-TodayWhatILearned/"},{"title":"181108-TodayWhatILearned","text":"181108 TWIL 오늘 한 일은 무엇인가 Project Data EDA, OLS 돌려보기 내일 할 일은 무엇인가 Project Data EDA, OLS 돌려보기 무엇을 느꼈는가 프로젝트 과정에서 Performance 가 안나오는 이유에 대해 알게된 계기였다. EDA 는 계속 하더라도 부족함이 많은것이고,데이터를 처리할 때 line by line 근거가 있어야 한다고 느꼈다. 처음부터 전 과정을 진행하는데 있어 매우 시행착오가 많았고, 또 앞으로도 많을 것이지만 계속 반복해서 시행해보는 것이 중요할 것 같다.","link":"/2018/11/08/181108-TodayWhatILearned/"},{"title":"181109-TodayWhatILearned","text":"181109 TWIL 오늘 한 일은 무엇인가 Crawling miniproject 발표 Prediction main project data 탐색 내일 할 일은 무엇인가 Project 모임 무엇을 느꼈는가 Crawling miniproject 의 troubleshooting 시간이 있었다. 발표를 끝내고 나서는 하나를 마쳤다는 시원한 감이 있었지만, comment 들을 듣고 많이 부족함을 느낀 시간이었다. 코드를 작성할 때, 공통된 요소의 추상화가 선택적이 아니라 필수적임을 알게되었다. 간단한 것이라도 반복적으로 작성하는 것은 python convention 에도, 또 프로그래밍의 근본적인 취지에도 어긋나는 것이다. 처음부터 일반화된 포맷으로 작성하기엔 아직 실력이 많이 부족하다. 간단한 구현에서 module 화까지 내가 짜는 대부분의 코드를 연습하다 보면 늘겠지… 메인 프로젝트의 data 간의 연관성이 잘 보이지 않는다. 지난 시간에 알게된 하루 주기로의 delay가 반복됨을 알고 있었으나, datetime 형식으로, 혹은 int나 float 형식으로 ols 에 집어 넣으면 주기성을 잡아내지 못하는 것 같다. 시간 단위로 X feature 로 들어가면 좋을 것 같은데, ols formula 에 시간을 집어넣으면 계속 category 화 된다. 이렇기 때문에 그 해당 정확한 시간이 있지 않으면 coefficient 가 먹지 않지…. 시간을 표현하기 위해 60진법도 찾아보고, 1분 단위로 int 숫자에 mapping 할까도 생각해보았으나, 결국 mapping 해서 ols 에 돌리면 2400 이후 값은 없는데도 x 축에 들어가게 된다….","link":"/2018/11/09/181109-TodayWhatILearned/"},{"title":"181111-TodayWhatILearned","text":"181111 TWIL 오늘 한 일은 무엇인가 Project 모임 내일 할 일은 무엇인가 Project_Data 탐색 시계열 분석 공부 무엇을 느꼈는가 어제보다는 약간의 진보가 있었지만, 이에 대한 이유는 명확히 몰라 사실 분석이라기보다 얻어걸린 기분이든다. EDA 를 통해 작성한 모델링에서의 식은 아직까지 완성된 느낌을 받지 못해 답답하고, 방향성을 잘못 접근하고 있는 것인가 하는 느낌을 받기도 했지만, 마지막에 조금 기분이 나아졌다. 내일 오전에는 프로젝트에 밀렸던 시계열 데이터 분석에 관한 기초적인 공부를 다시 하고, 오후에는 프로젝트 데이터를 좀더 살펴보아야겠다.","link":"/2018/11/11/181111-TodayWhatILearned/"},{"title":"181110-TodayWhatILearned","text":"181110 TWIL 오늘 한 일은 무엇인가 Prediction main project data 탐색 내일 할 일은 무엇인가 Project 모임 무엇을 느꼈는가 어제 고민했던, 시간을 사용하여 Regression 을 돌리는데는 성공하였다. 단위를 바꿔주는 방법으로 60진법도 생각하고, radian 으로 바꾸는 방법도 생각해보다가 epoch 방법으로 기준시점에서 지난 초 수 로 scale 과 unit 을 바꿔준디 돌렸더니, 돌아는간다… 문제는 전혀 예측했던 모형이 아니었다. scatter plot 의 외형만 보고 판단했다는 것을 마지막에 깨달은 것 같다. 모델을 하기위해 묶어보고 새로운 feature 라고 생각되는 것도 빼보지만 아직 경향성이나 공통적인 특정 같은 것은 보이지 않는다.","link":"/2018/11/10/181110-TodayWhatILearned/"},{"title":"181114-TodayWhatILearned","text":"181114 TWIL 오늘 한 일은 무엇인가 flight prediction 내일 할 일은 무엇인가 프로젝트 모임 무엇을 느꼈는가 계속 비행기 delay시간을 예측하는 프로젝트를 진행하고 있다. 여러가지 feature 들을 다루면서 arrival delay 를 예측하는 것을 목표로 삼고 있는데, 가지고 있는 데이터에서 공통적인 특징을 발견할 수가 없다. 외부 데이터를 사용해야할지, 갸지고 있는 데이터 셋에서 새로운 컬럼을 만들어야 할지 모르겠다. 많은 컬럼을 만들고 지우면서, 단순히 아이디어, 혹은 이러지 않을까라는 추측으로 컬럼을 만드는 것이 방법론 적으로 잘못된 것인가하는 의문이 들기도 한다. correlation 이 높은 것을 가지고 OLS 를 돌리는 것 외에 새로운 컬럼을 가지고 만드는 것. 어떻게 다루어야 할지 고민이다. 이제 due date 가 얼마 남지 않은 만큼 최대한 남은 기간 열심히 돌려보고 만들어보면서 계속 trial and error 로 찾아보아야 겠다.","link":"/2018/11/14/181114-TodayWhatILearned/"},{"title":"181119-TodayWhatILearned","text":"181119 TWIL 오늘 한 일은 무엇인가 시계열 공부 MA, AR, ARMA 공부 및 수식 꼼꼼히 전개 스터디 모임 : 프로그래밍 구현계획짜기 내일 할 일은 무엇인가 ARIMA, SARIMA 수식 정리 DataStructure 코드 정리 A* 알고리즘 공부 회기분석 리뷰 무엇을 느꼈는가 할게 산더미인듯한 느낌이지만, 하나씩 그어나가다보면 되겠지라는 마음으로…체력적으로 힘듬이 느껴진다..","link":"/2018/11/19/181119-TodayWhatILearned/"},{"title":"181120-TodayWhatILearned","text":"181120 TWIL 오늘 한 일은 무엇인가 시계열 공부(SARIMA) BST 공부 내일 할 일은 무엇인가 A* 알고리즘 공부 회기분석 리뷰 무엇을 느꼈는가 오늘 시계열을 강의가 마무리 되면서, 가장 중요한 교훈은 ‘세상의 모든 시계열 데이터를 현재 배운 모델로구현할 수 없다’인 것 같다. 회기 분석보다 훨씬더 어려웠던, 수식 전개들이 많았는데 모델링 할 수 있는 실제데이터가 많이 없다는 것이 안타까웠다. 그럼에도 불구하고 이전 프로젝트에서 다루었던 데이터를 ARIMA 등을 적용할 수 있을지 다시한번 프로젝트파일을 열어봐야겠다. 내일은 Projectmini 에서 하기로한 A* 알고리즘을 구현하는 것을 고민해 봐야한다. 프로젝트 기간동안미뤄두었던 것을 하는 것이라, 의지는 충만하지만, 이전에 막혔던 부분에서 다시 어디서부터 봐야할지 복기 하는시간이 오래 걸릴 것 같다.","link":"/2018/11/20/181120-TodayWhatILearned/"},{"title":"181121-TodayWhatILearned","text":"181121 TWIL 오늘 한 일은 무엇인가 A* Alogorithm : Pseudo Code 작성 ~ PACF 공부 내일 할 일은 무엇인가 시계열 모형의 추정 부분 나머지 공부 NLTK, KoNLPY 메소드 정리 무엇을 느꼈는가 A* 알고리즘에 관련된 많은 자료들을 읽어보고, 정리하면서 어떻게 구체적으로 이 알고리즘이 흘러가는지파악했다. 기초적인 알고리즘들은 체계적으로 정리되어있는 자료들이 많지만, 이렇게 응용이 된 알고리즘들은업계와 학계에서 사용하면서 점차 개발됨에 따라 다양하고, 어느 방면에 효율성을 맞추느냐에 따라서 정말 다양하게파생된 것들이 많다. 그 중에서도 가장 간단하고 기본적인 원리가 설명되어있는 documentation 을 찾고,이해하려고 하다보니 시간이 많이 걸린 것 같다. 이렇게 기본적인 이해를하고, 다양한 응용된 문서를 보니, 그 각각이 추구하고 있는 방향도 읽혀지는 것 같다.내일부터는 이렇게 파악된 것을 바탕으로 본격적인 코드 구현을 해보아야할 것 같다. 오늘 잠깐 생각한 바로는,우리가 배웠던 자료구조를 사용하는 것이 효율적인지, 파이썬에 내장되어있는 자료구조를 사용 할 것인지 간에효율을 결정하는데, 어떤 factor 를 기준으로 정해야 할지 모르겠다. 스터디 때 이 부분도 물어보고, 이야기를나눠보면 좋을 것 같다. AR, MA 가 p, q 차로 되었을 때, k에 따른 PACF 함수의 모양을 증명해 보면서, AR, MA 의 ACF,COV general form 을 계속 찾아보면서 증명하게 되었다. 사실은 이 부분까지 다시한번더 증명하고 이를참조하는 방향으로 증명을 써 내려가야하는 것이 맞지만, 이전에 했다는 핑계로 참조했는데, 찜찜하다.증명은 어렵지 않았으나, 외워야 한다는 것이 좀 압박이다.","link":"/2018/11/21/181121-TodayWhatILearned/"},{"title":"181112-TodayWhatILearned","text":"181112 TWIL 오늘 한 일은 무엇인가 프로젝트 발표 및 Feedback 내일 할 일은 무엇인가 프로젝트 모임 &amp; 시계열 공부 무엇을 느꼈는가 메인프로젝트를 다루는 기간동안의 중간결과를 발표하는 시간을 가졌다. 전달력이 충분하지 못한 느낌을 많이 받았다. 내 머릿속에 있는 것을 다른 사람에게 전달하는 능력 역시 많이 필요함을 깨달은 시간이었다. 프로젝트 내용에 있어서는, 아직 부족한것이 많다. 퍼포먼스도 눈에 띄게 나오는 것이 없다. 원인은 아마도 눈에 띄는 데이터들의 경향성을 보지 못한 탓이 아닐까 싶다. 가지고 있는 train 데이터 전체를 두고는 뚜렷한 경향성을 보지 못하고 있다. 여러가지 제한 조건을 두면서 쪼개서 봐야하는 작업이 더 필요한 것 같다.","link":"/2018/11/12/181112-TodayWhatILearned/"},{"title":"181123-TodayWhatILearned","text":"181123 TWIL Today’s To-Do-List (스터디) 스터디 원들과 A* Alogorithm 개념 정립 (스터디) 스터디 원들과 함께할 깃 레포 설정, Code Convention 정하기 (개인공부) AWS EC2, crontab, shell-script 공부 (개인공부) 기본 통계량 복습 (개인공부) KoNLPY, NLTK 훑어보기 오늘 한 일은 무엇인가 (스터디) A* Alogorithm 공부 (스터디) A* 구현, Class 단위로 윤곽 잡기 (개인공부) 확률분포, 기본 통계량 꼼꼼히 보기 (개인공부) shell-script 내일 할 일은 무엇인가 확률분포, 기본 통계량 스터디 main프로젝트 Review (Procedure 정리, 시계열 적용 고민) AWS 복습 (스터디) git repo 설정, A* Algorithm 코드 작성 무엇을 느꼈는가 열심히 개념을 정립하는 것도 중요하지만, 계속 해서 정리하고 스스로에게 Feedback 을 주는 것이 중요하다. 어떤 Procedure 중에 있는 것인가를 아는 것이 가장 중요하다. 앞으로의 적용을 항상 고민해야한다. 회기분석 프로젝트를 마친 것을 다시한번 복기할 필요가 있기에, 이번 주말을 통해 개인적으로 복기를 해보려고 한다. 이에 앞서 이번주부터 시작한 기본 통계량 부터의 복습을 같이 하면서 프로젝트 과정중 잊었던 개념들을 적용하는 연습을 주말동안 다시한번 차근차근 해보아야 한다.","link":"/2018/11/23/181123-TodayWhatILearned/"},{"title":"181126-TodayWhatILearned","text":"181126 TWIL To-do-list : 오늘 할일 이미지 데이터 처리, 사운드 데이터 처리 공부 Shell script 정리 Pandas - 50page 수학 8장-2절 오늘 한 일은 무엇인가 Shell script 공부 내일 할 일은 무엇인가 알고리즘 복습 Shell scipt cheatsheet 작성 Pandas - 100 page 수학 9장-2절 --- ## 무엇을 느꼈는가 - RedBlack tree 를 배우면서, 왜 color violation 을 해결하고, 색을 기준으로 정렬했을 때, O(logN) 으로 장점이 되는지에 관해 궁금하다. 자세한 수학적 증명보다, color 와 알고리즘의 효율이 어디서 연결 되는지 궁금하다. 이런 궁금증들은 이렇게 메모 해두었다가, 여유가 생길 때 꼭 찾아 보아야 겠다. - shell script 기본 문법을 공부하면서, Python 이 얼마나 직관적인 언어인지 느꼈다. 직접 해보면서, while, if 조건문 등을 쓸때, 띄어쓰기에 예민했고, DO ~ DONE 의 couple 이 중요했다. 또한, if , case 가 끝날 때는, 직관대로 done 이 아니라 fi, esac 등으로 닫아 줘야 하는 것이 위트 있지만서도, 쉽게 기억될 것은 아님을 알게 되었다. DO 도 OD 였다는데, 지금 OD 는 안되네.. DO 는 DONE 으로 쓰자 !!!!","link":"/2018/11/26/181126-TodayWhatILearned/"},{"title":"181125-TodayWhatILearned","text":"181125 TWIL 오늘 한 일은 무엇인가 (스터디) A* Alogorithm git 정리 (프로젝트) 프로젝트 리뷰 (프로젝트) 깃허브 다루기 (개인공부) 판다스 공부 (개인공부) 자연어 처리 메서드 정리 내일 할 일은 무엇인가 (개인공부) 자연어 처리 메서드 마저 정리 (개인공부) AWS 정리 (개인공부) 연속확률분포 정리(수식 꼼꼼하게) 무엇을 느꼈는가 지난주 프로젝트 리뷰 시간에 박사님께서 주신 Comment를 바탕으로 지난 프로젝트 했던 것을 복기하는 시간을 가졌다. 그 중 오늘 집중해서 탐구 했던 것은, Partial Regression 에서 경향이 보였을 때, 이를 구체적으로 어떻게 다루는지 연구하였다. 우리가 Partial Regression plot 을 보고 두가지 경향이 있을때, 한 경향에 대해 뽑아 냈고, 이 뽑아낸 경향을 바탕으로 다시한번 본 Feature 들이 어떤 특성을 가지고 있는지 알 수 있었다. 여기서 필요한 것은 Partial Regression 을 해석 할 수 있는 개념과 이를 바탕으로 다시한번 DataFrame 을 정렬하고, 여기서 Data 들의 특성을 뽑아 낼수 있는 판단력이 필요했다. 당연히 이를 구현하기 위해 Pandas를 다루는데 능숙함 역시 필요했다. 커멘트 받은 것을 이렇게 다시 구현했다는 것에 큰 보람을 느끼며, 프로젝트 진행 당시 좀더 꼼꼼하게 이런것을 생각 할 수 있었으면 좋았을 것이라는 아쉬움도 함꼐 느낀다. 퍼포먼스에 치중하는 것이 아니라 데이터 자체에 어떤 특성이 있는지 파악하려고 노력하는것이 무엇보다도 중요함을 느낀다. 판단을 위해서는 Data 를 보기 편하게 정렬하고, 시각화 하는 것이 매우 중요하다. 앞으로 이를 위해 알고 있는, 알아야되는, 한번 봤는데 잘 기억 안나는 것이 어쩌면 다행이기에, method 들을 간단하게 나마 정리하려고 한다. 개인적인 기록으로 git, github 를 사용하는 것은 어렵지 않았다. 하지만 팀 협업을 하려고보니, git 을 다루는게 아직 능숙지 않았다는 것을 깨달았다. 오늘은 git과 github 에 대해 가볍게 공부했고, 이를 바탕으로 어떤식으로 협업해야하는 것인지 간단하게 나마 알게 된것 같다.","link":"/2018/11/25/181125-TodayWhatILearned/"},{"title":"181128-TodayWhatILearned","text":"181128 TWIL To-do-list : 오늘 할일 ~ Classification 성능평가 수학 9장 2절 Pandas ~ 100pages 오늘 한 일은 무엇인가 ~ Classification 성능평가 수학 9장 2절 내일 할 일은 무엇인가 Pandas 100page 무엇을 느꼈는가 프로젝트를 한번 해서 그런지, 지금 공부하는 내용도 앞으로 프로젝트에서 어떻게 활용해야할지 염두? 걱정?하며 공부를 하게 되는 것 같다. 메소드를 정리할 필요가 있고, 배우는 각 개념들이 어떤 판단을 바탕으로 사용해야하는지 생각하며 공부하게 되는 것 같다. 시간이 부족하여, 메소드들을 좀더 디테일하게 보지 못하는 아쉬움이 있지만, 이런 것들을 주말 등을 통하여보충할 필요가 있다.","link":"/2018/11/28/181128-TodayWhatILearned/"},{"title":"181127-TodayWhatILearned","text":"181127 TWIL To-do-list : 오늘 할일 알고리즘 복습 Shell script cheatsheet 작성 Pandas - 100page 수학 9장-2절 오늘 한 일은 무엇인가 알고리즘 복습 Shell script cheatsheet 작성 내일 할 일은 무엇인가 Pandas - 100page 수학 9장-2절 Classification Intro, Scikit-Learn 의 전처리 부분 code 정리 무엇을 느꼈는가 알고리즘 복습이 계획했던 것 보다 훨씬더 오래 걸려서, 기존에 계획했던 Pandas 와 수학은 보지 못했다. 오늘 공부한 분류도 다시 공부해야하는 만큼, 내일 공부할 양은 매우 많을 것 같다. 정리하는 내용들을 내가 보기 편하고, 쉽게 사용 할 수 있도록 하는 방법을 많이 고민해보아야겠다. 파일들이 누적되고 쌓이면서, 이제는 효율적으로 검색하거나 찾아서 볼 수 있는 것이 매우 중요한 것 같다. 어떻게 정리하거나 검색할 수 있는 Tool이 있을지 틈나는대로 생각해봐야겠다.","link":"/2018/11/27/181127-TodayWhatILearned/"},{"title":"181129-TodayWhatILearned","text":"181129 TWIL To-do-list : 오늘 할일 Pandas ~ 150page wiki 구성 방안 REST API, NGINX, AWS 공부 오늘 한 일은 무엇인가 Pandas 80page wiki 구성 REST API, NGINX, AWS 공부 내일 할 일은 무엇인가 Pandas 150page 마무리 복습 --- ## 무엇을 느꼈는가 - 지금껏 구성해온 개인 wiki 혹은 cheatsheet등을 로컬 서버등을 이용해 wiki page 를 이용하고 싶었다. 가장 많이 사용한다는 gollum api 를 설치해보았지만, 생각보다 편하지 않았다. gitlab이나 github 등은 온라인에서 작성이 가능하지만, 로컬에서 그 해당 파일을 갖기가 불편했다. 또 검색기능도 중요했다. - 결국, 지금 작성하고 있는 git blog 에 wiki tab 관리로 하기로 했다. - 돌고돌아 gitblog 의 tab 을 활용해서 wiki 하기로 했다. 앞으로 좀더 깔끔하게 정리 파일을 관리하고, - 직접 참고할 자료이니, 무엇보다도 내가 보기 편해야 할 것이다.","link":"/2018/11/29/181129-TodayWhatILearned-1/"},{"title":"181203-TodayWhatILearned","text":"181203 TWIL To-do-list : 오늘 할일 Pandas Cookbook 2장 복습, 3장 ROC 커브, Logistic Regression 시간 날 때, 새로운 데이터셋 EDA 오늘 한 일은 무엇인가 ROC, AUC, Logistic Regression 정리 Flask_app : classification model application 코드 해석 Pandas 2장 복습 내일 할 일은 무엇인가 Heap, Graph 부분 복습 Pandas 3장, 4장 새로운 데이터셋 EDA, Regression 함수 적용 --- ## 무엇을 느꼈는가 - Pandas를 조금씩 정리하고, 새로운 것들을 알게 되면서 이전에는 몰랐던 편리한 기능들이 많이 있음을 알게 되었다. 손에 익히게 되면서 아주 조금씩 판다스에대해서 편해지기 시작하였다. Pandas 를 심도 있고 공부해보고자 했던 처음 이유인 '데이터셋을 보고 먼저 손이 나가도록 하자'는 목표에는 아직도 멀었지만, 하루하루 조금씩 노력해온 1주 반동안, 이전보다 조금더 편해졌다는 것이 느껴진다. - 이렇게 체감적으로, 한파트 한파트씩 아주 더디지만 조금씩 편해지고, 손에 익는 다는 것을 느끼면서, 점점더 재미있어 질것이라 믿는다. ---","link":"/2018/12/03/181203-TodayWhatILearned/"},{"title":"181204-TodayWhatILearned","text":"181204 TWIL 오늘 한 일은 무엇인가 Heap 구조 공부, 코드 짜보기 Logistic Regression method 정리 내일 할 일은 무엇인가 QDA, LDA, 나이브 베이지안 모델 복습, 정리 특히, 수식들 정리 + Method보다는 predict 결과가 나오는 데까지 확률계산, 흐름 중점적으로 공부 (Study) A* 알고리즘 구현 Pandas 3장 무엇을 느꼈는가 Heap 구조를 공부하면서, class 단위로 짜져 있는 코드들을 혼자서 구현하려면 어디서부터 계획을 세워야되는가에 대해 고민했다. 또한, 어떤 이슈가 있을 때, 어떤 알고리즘과 자료구조를 선택해야하는지에 대한 판단근거 역시 앞으로 정리 해야할 필요성을 느낀다. 우선 간단하게 알고 있는 데이터 구조 전체에 대해 서로 비교하여 장, 단점 정도 빠른시일 내에 정리해야겠다.","link":"/2018/12/04/181204-TodayWhatILearned/"},{"title":"181205-TodayWhatILearned","text":"181205 TWIL To-do-list : 오늘 할일 QDA, LDA, 나이브 베이지안 모델 복습, 정리 수식, Predict 과정 중심적으로 공부 (Study) A* 알고리즘 구현 Pandas 3장 오늘 한 일은 무엇인가 (Study) A* 알고리즘 손코딩 QDA, LDA, 나이브 베이지안 모델 복습, 정리 내일 할 일은 무엇인가 (Study) A* 알고리즘 구현 Pandas 3장 --- ## 무엇을 느꼈는가 - 스터디에서 다같이 알고리즘을 파악하고, 이를 바탕으로 손코딩을 진행했다. Class 단위로 짜는 거였고, 우리끼리 자료들을 바탕으로 알고리즘을 공부하고, 이를 구현하는 것까지 개인적으로 매우 Challenging 했다. 오늘 손코딩을 마무리 하면서, 머릿속에 있는 알고리즘을 어떻게 풀어내는지, 설계부터 각각의 함수까지 계획을 세우는데 매우 큰 보람과 배움이 있었다. - 내일부터는 스터디원들과 함께 오늘 작성한 것을 바탕으로 실제 코딩을 들어갈 계획이다. 아직 미숙한 점이 많기에, 손코딩을 진행한 것들에 대해 Debugging 이 많이 필요하겠지만, 너무 재미있는 시간이었다. - 오후 남은 시간에는 QDA 부터 나이브 베이지안 모델까지(gaussain, bernoulli, multinomial) 수식과 그 메소드들을 하나하나 뜯어보고 분석하는 시간을 가졌다. 단순히 패키지의 메소드를 돌리면 되겠지가 아니라, 메소드를 돌리기전에 간단한 data에 대해 결과를 얻기까지 직접 손으로 풀어보고 계산하여, 메소드를 돌렸을 때 결과와 비슷한지 확인 하는 시간을 통해, 각 분포가 모델링 되는데까지 과정을 이해하는데 도움이 많이 되었다. - 또한 자료에 나오는 다양한 시각화 메소드, 시각화 하는데 필요한 domain 설정, Numpy와 Seaborn 의 메소드들을 파악해보면서, 시각자료의 의도를 생각해보기도 하였다. - 많지 않은 범위를 이렇게 공부하다보니, 꽤많은 시간이 필요하여 오늘의 다른 목표들을 채우지는 못했지만, 스터디와 개인 공부 두가지를 통해, 하루를 매우 생산적으로 보낸 것 같아 뿌듯하다.","link":"/2018/12/05/181205-TodayWhatILearned/"},{"title":"181207_TodayWhatILearned","text":"181207 TWIL 오늘 한 일은 무엇인가 Pandas 3장, 4장(일부) (Project) 데이터셋 정하기 내일 할 일은 무엇인가 MySQL 복습, QUIZ 풀기 새로운 데이터셋 EDA, Regression Pandas 4장, 7장 지난 프로젝트(Flight_Delay Regression) Update 알고리즘 추가과제 하나씩 풀기 --- ## 무엇을 느꼈는가 - 와.. 오늘은 그간의 파이썬과 알고리즘, 데이터 구조의 공부한 것을 평가받는 첫 시간이었다. 시간내에 주어진 문제를 효율적으로 푸는 것이 생각보다 어려웠다. pandas의 method 나 numpy method 를 자주 사용하면서, 기본적인 내장 method 는 오히려 더 어색했다. 함수를 짜서, 테스트 케이스를 통과하다가 중간에 발생한 에러는 debugging을 하지 못했다. 또한, 코드 역시 비효율적이고 못생겼다. pyint 의 PEP 8 점수 역시 매우 낮았다. - 다시한번 많이 부족함을 느꼈고, 더 공부해야되고 알아야 하는 것이 한참이나 많다. - 쌓이고 쌓이는 ToDoList 에서 우선순위를 매번 잘 매기고, 너무 한 issue 에만 묻혀있지 말아야하며, 동시에 다양한 주제를 공부해야하므로 정리를 잘해야하고, 그 정리를 다음번에 참조할 수 있게 잘 기록해야하며, scheduling 을 효율적으로 해야하고, 무엇보다 그 순간에 매우 집중해야한다. ---","link":"/2018/12/07/181207-TodayWhatILearned/"},{"title":"181208-TodayWhatILearned","text":"181208 TWIL 오늘 한 일은 무엇인가 Decision Tree 연습문제 18.4.2 풀기 Imporve Flights_Delay Regression Project Insurance Cost dataset EDA MySQL Quiz 마무리 내일 할 일은 무엇인가 Classification 복습 MySQL 복습 MySQL Query Code Imporvement Insurance Data EDA Pandas 정리 Regression 수학 12-5장 복습 --- ## 무엇을 느꼈는가 - Query 문을 작성하면서, 다양한 방법으로 같은 결과를 얻을 수 있음을 알게 되었다. 같은 결과를 뽑아내는 Query 문의 비교에서, 오늘은 간결하고, 깔끔한 코드를 작성하려고 노력을 하였다. 하지만, 결국 가장 중요한 것은 Query 문이 얼마나 빠르게 동작 하느냐의 문제인 것 같다. 내일은 간결하면서도, 빠르게 동작하는 코드를 작성하는데 좀더 고민해봐야겠다. - 지난 Flights_delay regression project 를 개선하는데 있어, 코드적으로, 기술적으로 부분회기 plot 를 바탕으로 다시 모델링을 하는 방법에 대해 알게 되었다. 여기까지 알게 된 것을 바탕으로, 새로운 Medical cost(Insurance) data 를 regression 으로 모델링 해보려고 한다. 오늘 간단한 EDA 를 진행하였으나, 조금더 데이터를 자세하게 보는 방법론에 대해 공부를 하면서 진행해 보려고 한다. ---","link":"/2018/12/08/181208-TodayWhatILearned/"},{"title":"181209-TodayWhatILearned","text":"181209 TWIL 오늘 한 일은 무엇인가 Classification 복습 (Entropy, DecisionTree) MySQL Query Code Imporvement, CodeReview Insurance Data regression Algorithm 추가 과제 (2진수, 8진수, 16진수) 내일 할 일은 무엇인가 MySQl, NOSQL 복습, 정리 Insurance Regression 과정 정리, 알게된 것들 정리 Pandas 4장, 7장, 9장 정리 Algorithm 추가 과제 마무리 수학 12-5장까지 복습 --- ## 무엇을 느꼈는가 - Insuracne Data regression 을 진행하면서, 지난 프로젝트에서는 다가가지 못한 Step 들을 수행하였다. 잔차의 정규성을 검토하면서, 이를 발전 시킬 수 있는 방법에 대해 코드를 작성하였고, EDA 과정을 꼼꼼히 한 덕분인지, regression formula 를 돌리는데 있어 각 스텝마다, 작은 근거들이 생겨났다. - 아직은 수업 자료에서 모든 내용을 담을 만큼 Performance 와 스텝간의 근거들이 명확하지 않기 때문에 공부할 것이 한참남았지만, 오늘 새로운 데이터셋을 좀더 꼼꼼히 EDA 를 하면서, 새롭게 알게되고, 적용할 수 있는 것들이 생겨 뿌듯했다. ---","link":"/2018/12/09/181209-TodayWhatILearned/"},{"title":"181215-TodayWhatILearned","text":"181210 TWIL To-Do-list DataThon 발표 준비 Pandas 4장 공부 알고리즘 문제풀기 (Study) 프로젝트 미니(웹어플리케이션) 준비 (Project) Classification Project Data EDA 오늘 한 일은 무엇인가 Datathon 발표준비 알고리즘 문제 풀기 (Study) 프로젝트 미니(웹어플리케이션) 준비 내일 할 일은 무엇인가 Pandas 4장 공부 Support Vector Machine 공부 알고리즘 문제풀기 NoSQL, MySQL syntax 정리 무엇을 느꼈는가 데이터톤 발표를 준비하면서, 제출했던 코드와 과정을 다시 살펴보니 Markdown 이나주석이 부족함을 느꼈다. 다시 볼 때 좀더 편할 수 있도록, 코드와 과정을 다시 이어 나가는데 시간을 덜 소비하도록 나름 신경써서 작성하며 진행했는데, 다시 보려고 하니 머릿속에 있었던 것들이 다 작성되어 있지 않았다. 지금은 데이터톤에서 얼마 지나지 않았기 때문에, 기억에 남는 것이겠다. 하지만 추후에 다시 볼때는 기억이 나지 않아, 내가 작성한 코드와 문서임에도 불구하고 그 맥락을 이해하기 위해 처음부터 읽어 볼 것이다. 앞으로는 좀더 주석과 마크다운 문서에 신경을 많이 써야겠다. 짤막하게라도, 데이터 분석과정 중에 들었던 생각들을 작성해 놓아야, 그 시간이 지난뒤에 Develop 을 하던, 복기를 하던 계속 생각의 흐름을 이어 나갈 수 있을 것이다. 일일코딩, DailyCommit 등에 관한 글을 읽었다. 개인의 다짐과도 비슷하고, 개인 프로젝트로 개발자들이 많이 하는 것 같다. 글을 읽은 직후에는 나도 하고 싶다는 생각을 했지만, 과연 할 수 있을 것인가 하며 반문을 하였다. 다짐의 문제라고 하기엔, 너무 정신 없는 나날을 보내고 있기에.. 도전할 것인지 하루만 더 고민해봐야겠다.","link":"/2018/12/15/181215-TodayWhatILearned/"},{"title":"181211-TodayWhatILearned","text":"181210 TWIL To-Do-list Insurance EDA 정리 Pandas 12-5장까지 수학복습 MST 복습오늘 한 일은 무엇인가 Insurance EDA 정리 (STUDY) A* 알고리즘 시각화 선형회귀 개념 복습 내일 할 일은 무엇인가 데이터톤 무엇을 느꼈는가 파이썬으로 작성한 A*알고리즘의 시각화코드를 작성하였다. 제일 빠른 길을 찾아 주었으나, 이것을 시각화하는것이 생각보다 어렵다. 그리고 좀더 동적으로 시각화를 해주고 싶은데, 좀더 삽질을 많이해봐야겠다.결과데이터를 보여주면 되겠지 했지만, 답을 얻는 과정을 세세하게 시각화하여 보여주는 것 역시 어려운 문제였다.비단 알고리즘을 보여주는 것만이 아니라, 지금 공부하는 모든 것이 아마 그럴 것이다. A* 알고리즘을 적용해서 주어진 미로의 최적의 길을 찾는 것은 미로의 크기가 커질수록 검증이 어렵다. 우리가작성한 알고리즘으로 풀어준 path가 진짜 제일 빠른 길인지 확인하는 방법이 무엇인가 하는 생각이 든다. 크기가작은 미로에서 우리가 한길 한길 찾아가는 정답과 맞아서, 알고리즘이 제대로 작동하고 있다고 생각했다. 하지만,점차 크기가 커지면서 정답이 맞는가 확인하는 것은 어려웠다. 작은단위에서 맞는 것이라고 해서, 큰 단위에서 내놓은 답이 과연 정답일 것인지는 어떻게 검증해야하는가. 우리가 소단위에서 다 맞춘 알고리즘이라고 해서 전부 믿어야 하는 것인가라는 생각이든다. 내일은 데이터톤이다. 여태까지 공부한 것을 적용해보고, 실제로 제한시간내에 데이터를 분석해야한다. 아직도 많이 모르고,아는 것도 확신하기 어려운데, 잘 할 수 있을지 걱정된다.","link":"/2018/12/11/181211-TodayWhatILearned/"},{"title":"181216-TodayWhatILearned","text":"181216 TWIL 오늘 한 일은 무엇인가 DataThon 발표 Perceptron 공부 (Project) Quara Dataset EDA question_text 에서 vectorize 하기전에 특징값들을 뽑아내기 나이브 베이지안 돌려보기 (Study) 스터디때 나눌 WebApplication 의 구조, MVC model 나누기 내일 할 일은 무엇인가 (Study) 스터디원 블로그 개설, Flask에 비유한 WebApplication, MVC model 공부하기 (Stydy) PROJECTmini WebApplication 계획 세우기 (Project) EDA 짬짬히 계속하기 SVM 공부 무엇을 느꼈는가 Datathon에서 분석했던 내용을 발표하는 시간을 가졌다. 발표를 하면서 부족하다고 생각했던 점과 comment 를잊기 전에 정리해본다. 후기 및 생각과 느낌 프레젠테이션 능력이 부족하다. 긴장, 생각의 흐름을 말로 표현하는 것이 부족했다. 나름대로 이야기 할 것을 리스트업해갔지만, 잘 눈에 들어오지 않았다. 스크립트를 다 작성해가는 것이 좋은 것일까? 프레젠테이션 혹은 데이터를 모르는 사람도 읽을 수 있는 마크다운 정리가 부족했다. 데이터톤 당시 시간에 쫓기는 것도 있었고, 데이터를 분석하고 코드를 작성하면서 나중에 하면 되겠지 라고 생각했다. 결과는 제대로 마무리와 정리를 하지 못한 채로 제출했고, 이는 발표할 때 쓰는 자료로서는 0점에 가까웠다. 프로젝트나, 코드를 작성할 때 comment 를 좀더 세세하게 작성하도록 노력해야겠다. 지적해주신 comment Regression 에서 intercept 의 의미 실제로 모델링한 결과를 현실 데이터에서 사용하기 위해서는 intercept 를 꼭 추가해야한다고 말씀해주셨다. comment 를 듣자 마자, 조금 찾아봤을 때, intercept 가 error 의 mean 값을 잡아준다고 한다. 이 부분은 좀더 보충이 필요하다. 더미변수를 사용하지만, Intercept 의 효과에 대해서.. 단지 해석의 의미로만 상수항을 생각하였는데, 좀더 본질적인 이유가 있는 것 같다. 꼭 보충할 것! 데이터를 분석하는 과정에서 insight를 얻었을 때, 이를 꼭 알기 쉽게 기록하라. 개인적으로 느꼈던 후기와 생각에서와 비슷한 취지의 말씀이었다. 자신과 다른사람이 알 수 있게 insight 를 꼭 기록하라고 말씀하셨다. R square 를 기준으로 분석을 진행할 때는 조심하여야 한다.","link":"/2018/12/16/181216-TodayWhatILearned/"},{"title":"181220-TodayWhatILearned","text":"181220 TWIL 오늘 한 일은 무엇인가 (Project) Classification Project 모임 딥러닝 엔지니어 현업자 특강 Celery 복습 간단한 알고리즘 문제 풀기 Linear Algebra(Gilbert) 1강 내일 할 일은 무엇인가 Linear Algebra(Gilbert) 2강 (Project) Classification Project Classification 개념 다시 보기 무엇을 느꼈는가 즐기자","link":"/2018/12/20/181220-TodayWhatILearned/"},{"title":"181225-TodayWhatILearned","text":"181225 TWIL 오늘 한 일은 무엇인가 (Project) Text Preprocessing 내일 할 일은 무엇인가 (Project) Project 모임 Linear Algebra 강의 2강, 3강 듣기 무엇을 느꼈는가 모든 전처리가 그렇겠지만, 텍스트 데이터의 전처리는 유독 할게 많다. 실제 사람이 사용하는 언어 데이터이다 보니,예외사항들이 많고 모델 성능에 이 전처리들이 큰 영향을 미친다고 하기에, 열심히 전처리를 하고 있다. 오늘은 embedding 데이터를 활용해 줄임말들 (I’d, We’re 등) 늘려주는 작업을(I would, We are 등) 해줬다.기존에 짰던 영어이냐 아니냐를 분류하려고 만든 알고리즘의 성능이 위 작업을 통해 좀 더 좋아질 것이라 예상된다. 또한오늘 한 작업이 main modeling 을 하기 위해 진행할 Tokenizing 에도 좋은 영향을 줄 것이다. Stopwords 들을빼거나 더할 때도, What’s 보다는 What is 로 늘려주었을 때, 훨씬더 세밀해 질 것이다. 내일은 spelling 체크, 띄어쓰기 체크해서 올바르게 고쳐주는 작업을 해야한다. 그리고, Baseline 모델을잡기 위해 본격적인 modeling 에 들어가야한다.","link":"/2018/12/26/181225-TodayWhatILearned/"},{"title":"190109-TodayWhatILearned","text":"190109 TWIL 오늘 한 일은 무엇인가 BLOG RENEWAL 내일 할 일은 무엇인가 Graph모형 공부 LinearAlgebra 1강, 2강 무엇을 느꼈는가 새해를 맞아 블로그를 새 테마로 바꾸었다. 기존에 hueman theme 에 익숙해져 있어서, 새로운 테마의 기능을수정하고, 전처럼 편해지려면 또 적응의 시간이 필요할 것 같다. 블로그의 테마는 작년부터 글의 양이 늘어나면 늘어날수록그 욕구가 더 심해 졌다. 특정 카테고리에서 글이 누적되가면서, 어떤 글들이 담겨있는지 제목을 통해 직관적으로보고싶었다. hueman 은 글마다 썸네일들이 있고, 글의 순서가 조금 불편하게 배치되어 있다. 시리즈성 글들을 올린다거나,주제가 1, 2, 3 등으로 나뉘는 글들이 있을 때, 글 제목으로 연속성을 보기가 힘들었다. 위의 이유로 선택한 이번 테마는 내가 중점적으로 생각한 부분을 조금이나마 개선할 수 있는 것 같다. 틈틈히새로운 테마의 세팅도 마쳐야겠다.","link":"/2019/01/09/190109-TodayWhatILearned/"},{"title":"REBOOT","text":"** REBOOT ** Text Classification Project 를 한다는 핑계로 그간 TodayWhatILearned의 작성을 하지 못했다.프로젝트를 하는 동안은 매일 어떤 것을 공부할 계획이고, 어떤 공부를 했는지 남길 만한 내용이 없었던 것도 사실이다.프로젝트 동안 미뤄뒀던 공부들, 보고싶었던 주제들을 이제 다시 새로운 마음가짐을 가지고 시작할 것이다.새해가 밝은 만큼 블로그를 만들기 시작하면서 다짐했던 초심을 상기하자. To-Do-List Graph모형, 네트워크 추론 공부 (수식) - 새로운 패키지, 코드 정리하면서 공부 LinearAlgebra 1강, 2강 다시 시작 오늘 한 일은 무엇인가 Graph모형 공부 LinearAlgebra 1강, 2강 내일 할 일은 무엇인가 네트워크 추론 공부(수식 위주로 공부)","link":"/2019/01/08/190108-TodayWhatILearned/"},{"title":"Basic Classification with Pytorch","text":"Basic Classification with Pytorch 이번 post 는 pytorch 를 활용해 기초적인 분류 모델링을 해보면서, pytorch에 익숙함을 높이는 것이 목적입니다. 123456789101112import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport numpy as npimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings(\"ignore\")%matplotlib inline%config InlineBackend.figure_format = 'retina' 1. Binary Classification Modeling Sigmoid Loss : Binary Cross Entropy 1.1 Generate Data123456789101112# plotting functiondef plot_scatter(W_, xy, labels): for k, color in [(0, 'b'), (1, 'r')]: idx = labels.flatten() == k plt.scatter(xy[idx, 0], xy[idx, 1], c=color) x1 = np.linspace(-0.1, 1.1) x2 = -W_[1] / W_[2] * x1 - W_[0] / W_[2] plt.plot(x1, x2, '--k') plt.grid() plt.show() 1234567# Generate dataW = np.array([-4./5, 3./4., 1.0])xy = np.random.rand(30, 2)labels = np.zeros(len(xy))labels[W[0] + W[1] * xy[:, 0] + W[2] * xy[:, 1] &gt; 0] = 1 1plot_scatter(W, xy, labels) 1.2 Train data Generate 한 data 로 부터, x 축 값, y 축 값, augmented term 으로 3가지 column 을 만들어 train data 로 만들어 줍니다. 또한 대응 되는 label 도 model 에 적합한 모양으로 바꾸어 줍니다. 1234x_train = torch.FloatTensor([[1.0, xval, yval] for xval, yval in xy])y_train = torch.FloatTensor(labels).view(-1, 1)print(x_train[:5])print(y_train[:5]) tensor([[1.0000, 0.0192, 0.6049], [1.0000, 0.0485, 0.2529], [1.0000, 0.2412, 0.9115], [1.0000, 0.9764, 0.1665], [1.0000, 0.9021, 0.5825]]) tensor([[0.], [0.], [1.], [1.], [1.]])1.3 Modeling Linear Model 형태와 Sigmoid 함수, Loss function 은 cross entropy 를 활용해 모델링을 합니다. 여기선, 내장되어있는 함수들을 되도록 사용하지 않고, Low level 로 코드를 작성해 보겠습니다. 12345678910111213141516171819202122# Low level modelingparameter_W = torch.FloatTensor([[-0.5, 0.7, 1.8]]).view(-1, 1)parameter_W.requires_grad_(True)optimizer = optim.SGD([parameter_W], lr=0.01)epochs = 10000for epoch in range(1, epochs + 1): # Prediction y_hat = F.sigmoid(torch.matmul(x_train, parameter_W)) # Loss function loss = (-y_train * torch.log(y_hat) - (1 - y_train) * torch.log((1 - y_hat))).sum().mean() # Backprop &amp; update optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch {} -- loss {}\".format(epoch, loss.data)) epoch 1000 -- loss 6.368619441986084 epoch 2000 -- loss 4.5249152183532715 epoch 3000 -- loss 3.654862403869629 epoch 4000 -- loss 3.122910261154175 epoch 5000 -- loss 2.7545464038848877 epoch 6000 -- loss 2.4800000190734863 epoch 7000 -- loss 2.2651939392089844 epoch 8000 -- loss 2.091233253479004 epoch 9000 -- loss 1.9466761350631714 epoch 10000 -- loss 1.82413184642791751parameter_W.data.numpy() array([[-16.748823], [ 16.618748], [ 17.622692]], dtype=float32) 아래 그림을 보면, Train이 잘 된 것을 알 수 있습니다. 1plot_scatter(parameter_W.data.numpy(), xy, labels) 2. Multiclass Classification2.1 Generate Data 이번에는 3개의 label 을 가지고 있는 classification 을 Modeling 해 보겠습니다. 또한, High Level 로 pytorch 의 추상 클래스를 이용해 모델링 해보겠습니다. 123456789101112def plot_scatter(W1, W2, xy, labels): for k, color in [(0, 'b'), (1, 'r'), (2, 'y')]: idx = labels.flatten() == k plt.scatter(xy[idx, 0], xy[idx, 1], c=color) x1 = np.linspace(-0.6, 1.6) x2 = -W1[1] / W1[2] * x1 - W1[0] / W1[2] x3 = -W2[1] / W2[2] * x1 - W2[0] / W2[2] plt.plot(x1, x2, '--k') plt.plot(x1, x3, '--k') plt.show() 123456789# Generate dataW1 = np.array([-1, 3./4., 1.0])W2 = np.array([-1./5, 3./4., 1.0])xy = 2 * np.random.rand(100, 2) - 0.5labels = np.zeros(len(xy))labels[(W1[0] + W1[1] * xy[:, 0] + W1[2] * xy[:, 1] &gt; 0)] = 1labels[(W2[0] + W2[1] * xy[:, 0] + W2[2] * xy[:, 1] &lt; 0)] = 2 1plot_scatter(W1, W2, xy, labels) 2.2 Train data1234x_train = torch.FloatTensor([[1.0, xval, yval] for xval, yval in xy])y_train = torch.LongTensor(labels)print(x_train[-5:])print(y_train[-5:]) tensor([[ 1.0000, 0.9641, 1.3851], [ 1.0000, -0.4445, 1.0595], [ 1.0000, 1.0854, -0.1216], [ 1.0000, 0.8707, 0.1640], [ 1.0000, 0.7043, 1.3483]]) tensor([1, 0, 0, 0, 1])2.3 Modeling1234567class MultiModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3, 3) def forward(self, x): return self.linear(x) 1234567891011121314151617model = MultiModel()optimizer = optim.SGD(model.parameters(), lr=0.01)epochs = 10000for epoch in range(1, epochs + 1): y_hat = model(x_train) loss = F.cross_entropy(y_hat, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch {} -- loss {}\".format(epoch, loss.data)) epoch 1000 -- loss 0.6635385155677795 epoch 2000 -- loss 0.5513403415679932 epoch 3000 -- loss 0.4890784025192261 epoch 4000 -- loss 0.44675758481025696 epoch 5000 -- loss 0.4151267111301422 epoch 6000 -- loss 0.39017024636268616 epoch 7000 -- loss 0.369760125875473 epoch 8000 -- loss 0.35262930393218994 epoch 9000 -- loss 0.3379631042480469 epoch 10000 -- loss 0.32520908117294312.4 Accuracy 계산 Accuracy 가 96 % 로 비교적 잘 분류 된 것을 확인 할 수 있습니다. 12accuracy = (torch.ByteTensor(model(x_train).max(dim=1)[1] == y_train)).sum().item() / len(y_train)print(\"Accuracy: {}\".format(accuracy)) Accuracy: 0.96","link":"/2019/05/06/Basic-Classification-with-Pytorch/"},{"title":"[CS231n]Lecture02-Image Classification Pipeline","text":"Lecture 02: Image Classification Pipeline 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Image Classification 의 기본 TASK 위 사진을 보고, → ‘CAT’ 혹은 ‘고양이’ 라고 classification 자연스럽게 따라오는 문제는 “Sementic Gap” : 우리가 준 data (pixel 값 [0, 255]) 와 Label 간의 gap 또한, 이 과정에서 극복해야 하는 Challenges Viewpoint Variation ( 같은 객체에 대해 시점이 이동해도 robust) Illumination ( 빛, 밝기, 명암 등에도 robust) Deformation ( 다양한 Position, 형태의 변형에도 robust) Occlusion ( 다른 물체나 환경에 의해 가려지는 data 에도 robust) Background Clutter ( 배경과 비슷하게 보이는 객체에도 robust) Intraclass Variation ( 한 종류의 클랫스에도 다양한 색과 모습의 객체가 있을 수 있다.) 2. 기존의 시도와 New Era Hard Coded Algorithm 과 여러 규칙 (rule-based로 해석된다) 들을 통해서 Image를 Classify 하는 노력들이 있어왔다. 이들의 문제는, (1) 위에 언급한 문제들이 Robust 하지 않다. (2) 객체가 달라지면, (고양이, 호랑이, 비행기 등) 객체마다 다 다른 규칙을 성립해줘야 한다. 즉 한마디로 요약하자면, Algorithm의 확장성이 없다. 이런 문제에 좀더 강한 방법이 지금 우리가 공부하고 있는, Data-Driven Approach Image 와 Label pair 의 dataset 을 모은다. Machine Learning 알고리즘을 이용해 classifier 를 학습시킨다. Classifier 를 new images 에 테스트에 평가한다. 3. First Classifier : Nearest Neighbor3-1. Nearest Neighbor 의 기본 알고리즘 train set 에서의 모든 data 와 label 을 기억한다. test Image 와 가장 가까운 train Image 의 label로 test image를 predict 한다. 이 때 ‘가장 가까운’ 을 계산 할 때, L1 distance 와 L2 distance 가 쓰일 수 있다. 이 외에도, 다양한 distance 지표가 쓰일 수 있다. 3-2. Nearest Neighbor Classifier 의 문제 이미지 classification 에는 잘 사용되지 않는다. train 보다 predict 하는데 훨씬 오래 걸린다. train 은 train data set 의 기억만 하면 되지만, predict 할 때는 전체 train data 에 대해 거리를 측정해야하고, sorting 해야하는 문제가 발생한다. Time Complexity - train O(1), predict O(N) (N은 train data 수) 위 그림을 보면, 연두색 공간에 노란색 class 가 포함 되어 있는 것을 볼 수 있다. 이는 generalize 면에서 부족한 모델이라고 볼 수 있다. 같은 알고리즘 이지만, 이를 해결 하는 방법은 K 개의 가까운 neighbor 로 부터 majority voting 을 받은 것으로 classify 를 하는 것이다. 3-3. K-Nearest NeightborsSingle Nearest 만 보는 것이 아니라, K 개의 가까운 point 의 투표를 통해 해당 test data 의 label 을 예측한다. 이 때, Voting 하는 방법에는 majority voting ( 다수결 ) 과 weighted voting ( 가중치를 주어 투표: distance 가 가까운 것에 가중치를 준다.) 가중치를 주는 방법에는 distance 가 커지면 곱해지는 weight 을 줄이는 방법으로 1 / (1+distance) 등을 weight 을 곱해준다. 3-4. k-Nearest Neighbor on images NEVER USED 차원의 저주 문제 knn 이 잘 동작하기 위해서는 dataset 공간을 조밀하게 커버할 만큼의 충분한 training space 가 필요하다. 하지만, data 의 차원이 늘어날 수록 그 충분한 data 의 수가 exponential 하게 늘어난다. 4. Setting HyperparametersModel 최적의 hyperparameter 를 찾기 위해서는 data set 을 구분하여, unseened data 를 사용하여 성능 검증을 하고, model selection 을 해야한다. 이는 단순이 hyperparameter 를 찾는 용도 뿐만 아니라 우리가 세운 가설을 서로 비교 할 때는 data set 을 정확히 구분하고, test set 을 통해 비교하고, 선택해야한다. 그 방법에는 train, validation, test set 으로 dataset 을 나누는 방법과 cross validation 방법이 있다. 첫 번째 방법으로는, Validation set 을 통해 hyperparameter(가설)를 검증하고 선택하여, Test set 을 사용하여 Evaluate 과 Reporting 등을 한다. 딥러닝 모델링에서는 이 방법으로 많이 사용한다. 두 번째 방법은, data set 의 크기가 크지 않을 때, Train set 안에서 folds 들을 나누어 각 fold 가 돌아가며 validation set 이 되며, 이들의 평균값으로 가설을 비교한다. 이는 딥러닝 모델에서는 적합하지 않은 형태이다. 모델 자체의 연산이 많은데다가, 같은 모델에 대해 많은 validation 이 효율적이지 않기 때문이다. 또한 data가 많지 않은 상태에서 딥러닝 모델을 선택하는 것은 옳지 않다. 5. Second Classifier : Linear ClassifierLinear Classifier 는 Neural Network 의 기본 골격이다. (1) image data 와 W (parameters or weights) 을 통해 연산을 해주고, (2) function 을 통과해 Classification 을 해준다. 특히 Linear Classifier 의 경우 아래 와 같이, (1) image data 와 W 를 dot product 를 해주고 (2) linear function f 를 통과한다. $$f(x, W) = Wx$$ 6. ReferenceLecture 2 | Image Classification Syllabus | CS 231N","link":"/2019/05/13/CS231n-Lecture02-Summary/"},{"title":"[CS231n]Lecture05-CNN","text":"Lecture 05: Convolutonal Neural Networks 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Convolution Layer1-1. Fully Connected Layer 와 Convolution Layer 의 비교32 x 32 x 3 image 가 있다고 하자. network 에 주입하기 위해, 1 x 3072 로 핀 data 를 상상해 보자. Fully Connected Layer 의 경우, 아래 그림 처럼, Weight 과 dot product 가 수행되어, activation 값이 나오게 된다. 이 때, activation 의 갯수는 W 의 크기에 따른다. Fully Connected Layer 의 경우, 사진이라는 공간적 구조(Spatial Structure) 가 중요한 data 에 대해, 공간적인 정보를 다 잃어버리는 문제가 발생한다. 공간적 정보를 보존하기 위해 Convolution Layer 를 사용한다. 1-2. Convolution Layer Overview 이 때, 실제 convolution 계산은 image 의 filter 크기 만큼의 matrix 를 vectorize 한 후, filter vector 와 dot product 로 수행한다고 한다. 따라서 이 때 헷갈리지 않도록 할 것은, 한번의 Convolution 연산 결과는 하나의 scalar value 가 된다. 따라서, 하나의 filter 가 이미지 한 장을 훑어 내려간다면, 원본 이미지보다는 조금 작은 depth 가 1인 activation map 이 결과로 나온다. 따라서 activation map 의 깊이(channel 수)는, filter 의 갯수 와 동일하다. 여러개의 필터는 이미지의 각기 다른 특징을 추출하려는 의도에서 사용된다. 그 후, 이 Convolution 의 결과인 activation map 을 비선형 함수(ReLU 등)에 통과 시킨다. 여러 유명한 ConvNet 들은 이렇게, Convolution Layer 와 비선형함수를 반복적으로 나열 한 Network 라고 볼수 있다 1-3. Convolution Layer 의 결과물이렇게 여러 계층의 Convolution Layer를 쌓는 것은, 가장 아래 Layer 부터 높은 Layer 까지 단순한 feature → 복잡한 feature 를 뽑아 내는 것으로 볼 수 있다. 1-4. Convolution Layer 연산 filter 가 이미지를 훑고 지나가면서 convolution 연산을 한 후, 나온 결과는 원본 이미지보다 그 크기가 작아지게 된다. 그 정도는 filter 의 크기와 filter 가 훑고 지나가는 간격인 stride 에 따라 바뀌게 된다. $$output ;size = (N-F)/stride + 1$$ 문제점: convolution 연산의 문제는 이미지의 모서리에 있는 정보는 가운데에 있는 이미지의 정보보다 적게 추출 되는 문제가 있다.(filter 가 모서리를 넘어서는 이동 할 수 없으므로) convolution layer 를 반복적으로 지나가다 보면, map의 크기가 매우 빠르게 작아지게 된다. 이를 위해 적용하는 것이 Padding 이다. 1-5. Padding모서리에 정보를 얻기 위해 이미지이 외곽에 숫자를 채워 주는 방법. 이 때, 많이 사용하는 방법은 zero-padding. zero-padding 외에도 다양한 방법이 있다. 2. Pooling LayerParameter 의 갯수를 줄이기 위해, 우리가 ConvLayer 를 통해 뽑아낸 image 를 작게 만드는 Layer 이다. 즉, Downsampling 을 위한 것. Maxpooling 의 intuition : 앞선 layer filter 가 각 region 에서 얼마나 활성 되었는지 보는 것이다. 3. Typical Architecture[[(Conv → RELU) * N → Pool] * M → (FC → RELU) * K ] → SOFTMAX N : ~ 5 M : Large K : 0 ~ 2 ResNet, Google net 등은 이 방식을 훨씬더 뛰어넘음 4. ReferenceLecture 5 | Convolutinoal Neural Networks Syllabus | CS 231N","link":"/2019/05/20/CS231n-Lecture05-Summary/"},{"title":"[CS231n]Lecture03-LossFunction/Optimization","text":"Lecture 03: Loss Function &amp; Optimization 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Introduction Loss Function : 우리가 가지고 있는 W matrix 가 얼마나 안좋은지 정량화(Quantify) Optimization : 위의 Loss Function 을 minimize 해서 가장 좋은 parameter (W) 를 찾는 과정 2. Loss Function주어진 data 가 다음과 같을 때, $${(x_i, y_i)}_{i=1}^{N}$$ Loss 는 “Average of over examples” 즉, $$L = \\frac{1}{N}\\sum_{i}L_i(f(x_i, W), y_i)$$ 딥러닝 알고리즘의 General Setup W 가 얼마나 좋고, 나쁜지를 정량화하는 손실함수 만들기 W 공간을 탐색하면서 이 loss를 minimize 하는 W 를 찾기 2-1. Loss Example: Multiclass SVM LossSVM Loss 는 다음과 같다. 주어진 data example (x_i, y_i) 에 대해서, score vector s 는 다음과 같다. $$s = f(x_i, W)$$ 이 때, SVM loss는 $$L_i = \\sum_{j\\neq y_i}\\begin{cases} 0 \\quad\\quad\\quad\\quad\\quad\\quad\\quad if ;s_{y_i} \\geq s_j +1 \\ s_j - s_{y_i} + 1 \\quad\\quad otherwise \\end{cases} \\ = \\sum_{j\\neq y_i}max(0, s_j-s_{y_i}+ 1)$$ x_i 의 정답이 아닌 클래스의 score (s_j) + 1 (safety margin) 과 정답 클래스 score s_yi 를 비교하여, Loss 를 계산한다. SVM Loss 의 최대, 최솟값은 ? min : 0, max : infinite W 를 작게 초기화 하면, s 가 거의 0에 가까워 진다. 이 때, SVM Loss 는 어떻게 예상되는가? 정답이 아닌 class, 즉 class - 1 개의 score 원소들을 순회하면서 모두 더할 때, score 는 0에 가깝고, 이를 average 취하면 class 갯수 - 1 만큼의 Loss 값이 나온다. 이 특징은 debugging strategy 로 사용할 수 있다. 초기 loss 가 C-1 에 가깝지 않으면 bug 가 있는 것으로 의심해볼 수 있다. 만약 include j = y_i 이면, SVM Loss 는 어떻게 되는가? Loss Funtion 이 바뀌는 것은 아니다. 단지 전체 loss의 minimum 이 1이 될 뿐이므로 해석의 관점에서 관례상 맞지 않아 정답 class 는 빼고 계산한다. 우리가 average 를 취하지 않으면? 이 역시 바뀌는 것이 없다. 전체 class 수는 정해져 있고, 이를 나누는 average 는 scaling 만 할 뿐이다. Loss 를 max(0, s_j - s_yi + 1) ^2 를 사용하면? 이는 squared hinge function 으로 때에 따라서 사용할 수 있는 loss function 이다. 다른 Loss function 이며, 이는 위의 loss 와 다르게 해석 할 수 있다. 기존의 SVM loss 는 class score 가 각각 얼마나 차이가 나는지에 대해서는 고려하지 않는 것이라고 한다면, squared 가 들어감으로써, 차이가 많이 나는 score class 에 대해서는 좀더 가중하여 고려하겠다는 의미로 해석 할 수 있다. 2-2. Regularization만약 위 Loss Function 에 대해서, L = 0 으로 만드는 W 를 찾았다고 할때, 과연 이 W 는 유일한가? 그렇지 않다. W 일 때, L=0 이라면, 2W 역시 L=0 이다. 또한 L을 0으로 만드는 다양한 W 중에서 단지 training data 에만 fit 하는 classifier 를 원하는 것이 아니라, test data에서 좋은 성능을 발휘하는 classifier 를 찾기를 원한다. 이런 Overfitting 을 막기 위해서는 모델의 W 를 다른 의미에서 조절해줄 수 있는 Regularization term 을 추가할 수 있다. 즉, Model이 training set 에 완벽하게 fit 하지 못하도록 Model 의 복잡도에 penalty 를 부여하는 것을 말한다. Regularization 의 종류들: L2 Regularization L1 Regularization Elastic net(L1 + L2) Max norm Regularization Dropout Batch normalization, stochastic depth 2-3 Loss example: Softmax Classifier (Multinomial Logistic Regression)deeplearning 에서 훨씬 더 자주 보게 되는 loss 의 종류 중 하나이다. 위에서 살펴본 SVM loss 의 단점은 그 값 자체에 어떤 의미를 부여하기는 힘들다는 점이다. 반면에, Softmax Classifier 는 그 값 자체를 확률적 해석이 가능하기 때문이다. (cf. 콜모고로프의 공리를 통해 softmax 의 layer 의 output 이 확률로 해석 될 수 있다.) Softmax Function 은 다음과 같다. $$P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}},\\quad where \\quad s = f(x_i;W)$$ 3. OpitmizationOptimization 을 한마디로 요약하자면, 우리의 loss 를 최소화 하는 W 를 찾기 가 되겠다. 그 방법에는, (바보 같은 접근인: 강의표현) Random Search Gradient 를 구하는 방법 Numerical Gradient 수치적 접근 : 이 방법은 근사치를 구하는 것이며, 매우 느린 단점이 있다. 하지만, 쉽게 작성할 수 있다는 장점이 있다. Analytic Gradient 해석적 접근 : 미분식을 구해야하는 단점이 있다. 하지만 빠르고 정확하다. 실제로는, Analytic Gradient 방법을 사용한다. 하지만 debugging 을 위해 numerical gradient 를 사용한다. 이를 gradient check이라 한다. 3-1. Gradient Descent &amp; Stochastic Gradient DescentGradient Descent 를 방법을 이용해서 optimization 을 진행할 수 있다. 하지만 데이터의 숫자와 차원이 매우 큰 경우, parameter (W) 를 update 하는데 그 연산량이 매우 큰 단점과 위험이 있다. 이를 해결하기 위해 minibatch 를 사용하여 확률적 접근을 사용한다. 4. Image Feature ExtractionCNN 등이 등장하기 전에 Image 에서 Feature 를 뽑아내는 방법에 대해 소개한다. Feature를 뽑아내는 개념으로 생각할 수 도 있지만, Feature Transform 이라는 표현을 사용한다. Color Histogram : 이미지의 color distribution 을 사용하여 해당 이미지의 feature 로 사용할 수 있다. (출처: wikipedia ) For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image’s color space, the set of all possible colors. Histogram of Oriented Gradients (HoG) : CNN 이 등장하기 전, 매우 인기있는 Image Feature 중 하나라고 알고 있다. Edge 를 검출하는 방법이다. pixel 사이에, 값의 gradient 가 가장 큰 neighbor 가 edge 일 것이다라는 개념을 사용하여 edge 를 검출한다. 사진을 8 x 8 patch 를 만들어, 각 patch 마다 9 directional oriented gradients 를 계산하여, 이를 feature 로 사용하는 방법이다. Bag of Words : NLP 에서도 자주 사용되는 개념인 BoW 에서 차용한 개념으로, 이미지 데이터들에서 일정 크기의 patch 를 모아 clustering 을 통해 visual words (codebook) 을 만든다. 그리고 feature 뽑아내고 싶은 image 를 patch 형태로 바꾸고, codebook 에서 찾아 histogram 을 만들어 이를 feature 로 사용한다. 5. ReferenceLecture 3 | Loss Functions and Optimization Syllabus | CS 231N","link":"/2019/05/16/CS231n-Lecture03-Summary/"},{"title":"[CS231n]Lecture04-Backprop/NeuralNetworks","text":"Lecture 04: Backpropagation and Neural Networks 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Backpropagation1-1. 핵심Backprop의 한줄요약: 각 parameter 에 대해 Loss Function 의 gradient 를 구하기 위해 사용하는 graphical representation of ChainRule 언뜻 보면 어려울 수 도 있지만, just ChainRule 위의 그림을 예를 들어 살펴 보면, y 에 대한 f의 gradient 는 q에 대한 f의 gradient * y 에 대한 q 의 gradient 으로 볼 수 있다. 이를 해석하자면, f 에게 미치는 y 의 영향 = f 에게 미치는 q의 영향 * q 에게 미치는 y 의 영향으로 볼 수 있다. Backpropagation 의 가장 중요한 특징!! gradient 를 구하기 위해 node 를 기준으로 앞과 뒤만 보면 된다. 또한, 위 그림을 보게 되면, Forward passing 과 마찬가지로, Backprop 시에도, 이전 노드에서 전달 되는 Gradient 를 node 에서 local gradient 와의 연산으로 다음 Node 에 gradient 를 전달해 줄 수 있다. 1-2. Back prop 시, 각 node 의 역할 Add gate : gradient distributor add gate 를 기점으로, 각 입력(forward 방향의 입력)의 gradient로 local gradient 를 구하면 1이므로, 전달 되는 gradient 와 각각 1씩 곱해져 전달 되게 된다. 이 현상을 보게 되면 동등하게 나눠주는 역할을 하므로 gradient distributor 라고 볼 수 있다. Max gate : gradient router Max gate 는 gradient 를 한쪽에는 전체, 다른 쪽에는 0 을 준다. 해석적으로 보자면, max 연산을 통해 forward 방향에서 영향을 준 branch 에게 gradient 를 전달해 주는 것이 합적 $$max(x, y) = \\begin{cases} x \\quad\\quad if \\quad x &gt; y \\\\ y \\quad\\quad if \\quad x &lt; y \\end{cases}$$ 수식으로 보자면, x 에 대한 gradient, y 에 대한 gradient 가 각각 (1, 0), (0, 1) 로 local gradient 가 계산되기 때문이다. gradient 가 전달될 길을 결정해주는 면에서, 네트워크에서 path 를 설정해 주는 router의 기능과 비슷하다. Mul gate : gradient switcher + scaler 곱셈연산의 gradient 의 경우, x 에 대한 gradient 는 y 가 되므로, 서로 바꿔주는 역할을 한다. 이 때, forward 상에서의 결과 값으로 곱해주므로, scale 역할까지 함께 하게 된다. 2. Neural Networks강의에서는 Neural Network 에 대한 intuition 을 위해, biological neuron 과 비교하였다. 모델 architecture 로서의 neuron 과 biological neuron 의 공통점은 다음과 같다. input impulse input axon → dendrite (cell body)activation &amp; activation function output axon 이러한 비교는, 나의 개인적인 Neural Network에 대한 공부와 이해에 도움이 되지 않기에 큰 감동은 없다. 4강에 대한 핵심 사항은, Backpropagation 에 대한 수식적 이해와 그 이해를 통해 Backpropagation 이 gradient를 구함에 있어, 얼마나 편한 representation 인지이다. 공부하며 어려웠던 것은, backprop in vectorized 에서, Jacobian Matrix 의 표현이 scalar backprop 때와는 달리 한번에 머릿속으로 상상되지 않았기에, 손으로 써가며 따라 갔어야만 했다. 3. ReferenceLecture 4 | Introduction to Neural Networks Syllabus | CS 231N","link":"/2019/05/18/CS231n-Lecture04-Summary/"},{"title":"Classification Metrics","text":"Classification Metrics: 분류 성능 지표Kaggle Classification 의 Evaluation 에 자주 사용되는 ROC-AUC를 정리해보면서, 이번 기회에 분류모델에서 자주 사용되는 성능지표(Metric)을 간단하게 정리해봅니다. Confusion Matrix 에서 비롯되는 Metric 들은 이를 이미지로 기억하는 것이 효율적입니다. Confusion Matrixconfusion matrix 는 해당 데이터의 정답 클래스(y_true) 와 모델의 예측 클래스(y_pred)의 일치 여부를 갯수로 센 표입니다. 주로, 정답 클래스는 행으로(row), 예측 클래스는 열로(columns) 표현합니다. (정의하기에 따라 다르지만) 일반적으로, class 1 을 positive로, class 0 을 negative 로 표기합니다. 우리가 예측한 클래스를 기준으로 Positive 와 Negative 를 구분합니다. 그리고 정답과 맞았는지, 틀렸는지를 알려주기 위해 True 와 False 를 각각 붙여줍니다. Accuracy: 정확도 전체 데이터에 대해 맞게 예측한 비율 $$\\frac{TP+TN}{TP+FN+FP+TN}$$ Precision: 정밀도 class 1 이라고 예측한 데이터 중, 실제로 class 1 인 데이터의 비율 $$\\frac{TP}{TP+FP}$$ 우리의 모델이 기본적인 decision이 class 0 이라고 생각할 때, class 1 은 특별한 경우를 detect 한 경우 일 것입니다. 이 때 class 1 이라고 알람을 울리는 우리의 모델이 얼마나 세밀하게 class를 구분 할 수 있는지의 정도를 수치화 한 것입니다. Recall: 재현율 실제로 class 1 인 데이터 중에 class 1 이라고 예측한 비율 = Sensivity = TPR $$\\frac{TP}{TP+FN}$$ 제가 기억하는 방식은, 자동차에 결함이 발견되서 recall 이 되어야 하는데 (실제 고장 데이터중) 얼마나 recall 됐는지로 생각합니다. Fall-out: 위양성율 실제로 class 1 이 아닌 데이터 중에 class 1이라고 예측한 비율 낮을 수록 좋음 = FPR = 1 - Specificity Specificity = 1 - Fall out $$\\frac{FP}{FP+TN}$$ 실제로 양성데이터가 아닌 데이터에 대해서 우리의 모델이 양성이라고 잘못 예측한 비율을 말합니다. 억울한 데이터의 정도를 측정했다고 생각 할 수 있습니다. 각 Metric 간의 상관관계우리 모델의 decision function 을 f(x) 라 할 때, 우리는 f(x)의 결과와 threshold (decision point)를 기준으로 class를 구분합니다. Recall vs Fall-out : 양의 상관관계 Recall은 위의 정의에 의하듯이, 실제로 positive 인 클래스에 대해 얼마나 positve 라고 예측했는지의 비율입니다. 우리가 Recall 을 높이기 위해서는 고정되어있는 실제 positive 데이터 수에 대해 예측하는 positive 데이터의 갯수 threshold 를 낮춰 늘리면 됩니다. 이에 반해 threshold 를 낮추게 되면, 실제로 positive 가 아닌 데이터에 대해 positive 라고 예측하는 억울한 데이터가 많아지므로 Fall-out 은 커지게 되고 둘은 양의 상관관계를 갖게 됩니다. Recall vs Precision : 대략적인 음의 상관관계 위에 설명한 것처럼 threshold 를 낮춰 우리가 예측하는 positive 클래스의 숫자를 늘리게 되면, recall 은 높아지는 반면, 예측한 positive 데이터 중 실제 positive 데이터의 비율은 작아 질 수 있습니다. F-beta score precision 과 recall의 가중 조화평균 $$(\\frac{1}{1+\\beta^2}\\frac{1}{precision} + \\frac{\\beta^2}{1+\\beta^2}\\frac{1}{recall})^{-1}$$ 이처럼, 다양한 Metric 에 대해 우리가 초점을 맞추는 것에 따라 모델의 성능은 다르게 바라 볼 수 있습니다. 따라서 모델에 대해 성능을 평가하고 최종 모델을 선택함에 있어, 서로 다른 Metric 을 동시에 비교해야합니다. 이를 위해 precision 과 recall 을 precision 에 beta^2 만큼 가중하여 바라보는 스코어가 F beta score 입니다. 이 중 beta=1 일 때, score 가 우리가 자주 보는 f1 score 입니다. $$F_1=\\frac{2precisionrecall}{precision+recall}$$ ROC Curve: Receiver Operator Characteristic Curve Recall vs Fallout 의 plot (TPR vs FPR) 위의 예시 처럼, 우리가 클래스를 판별하는 기준이 되는 threshold (decision point) 를 올리거나 내리면서, recall 과 fall out 은 바뀌게 됩니다. 이렇게 threshhold 를 변화 해 가면서, recall 과 fall out 을 plotting 한 것이 ROC curve 입니다. sklearn.metrics.roc_curve() 의 documentation (ROC-)AUC: Area Under Curve 위에서 그린 ROC Curve 의 넓이를 점수로써 사용하는 것이 AUC 입니다. AUC 의 유의미한 범위는 class 를 50%의 확률로 random 하게 예측한 넓이인 0.5 보다는 클 것이고, 가장 최대의 AUC 의 넓이는 1 일 것이므로 0.5≤AUC≤1 의 범위를 갖는 score 입니다. ROC 커브와 AUC score 를 보고 모델에 대한 성능을 평가 하기 위해서, ROC 는 같은 Fall out 에 대해 Recall 은 더 높길 바라고, 같은 Recall 에 대해서는, Recall 이 더 작길 바랍니다. 결국, 그래프가 왼쪽 위로 그려지고, AUC 즉 curve 의 넓이는 커지는 것이 더 좋은 성능의 모델이라고 볼 수 있습니다.","link":"/2019/06/03/Classification-Metrics/"},{"title":"Generative Model","text":"확률론적 생성모형 (Generative Model)확률론적 생성모형의 기본 개념 우리가 궁금한 것은 $ x $ (features) 들이 있을 떄, $ y $가 어떤 Class 인지 맞추는 것이다. 맞추기 위해서는 training set 으로 부터 각 Class 마다 $ P(y \\mid x) $의 모형을 알아내야 한다. 베이즈 정리를 이용하여, $ y=C_k $일 때의 조건부 확률을 구할 수 있다.$$P(y = C_k \\mid x) = \\dfrac{P(x \\mid y = C_k); P(y = C_k)}{P(x)}$$ 이 때, $ P(x \\mid y = C_k) $== Likelihood== y가 k라는 클래스 일 때, x의 확률을 구하는 것이 관건 Likelihood 추정의 알고리즘 $ P(x \\mid y = C_k) $ 가 ** 어떤 ** 확률분포를 따를 것이다라고 가정 x의 특성에 따라 우리가 알고 있는 특정 확률분포를 따른다! 라고 가정 Class k 를 만드는 data들을 통해 이 확률분포의 모수값을 구한다. 모수값을 안다 == $ P(x \\mid y = C_k) $ 의 pdf 를 알고 있다. test set x 가 들어오면 $ P(x \\mid y = C_k) $ 를 구할 수 있다.","link":"/2018/12/05/Generative-Model/"},{"title":"[Lecture] 딥러닝을 이용한 자연어 처리 Section A, B","text":"Section A, B - Summary이 글은 edwith(https://www.edwith.org/)의 조경현 교수님의 딥러닝을 이용한 자연어 처리 (https://www.edwith.org/deepnlp/joinLectures/17363)강의를 듣고 정리한 글입니다. Section A. Introduction 알고리즘의 정의 : 문제를 해결하기 위한 instruction 의 sequence Machine Learning Algorithm 문제 정의 (optional) : 문제를 specific 하게 정의 하는 것 자체가 쉽지 않다. ex) 얼굴을 detection 할 때, 어떤 범위까지 얼굴이라고 정의할 것인지 Example들이 주어진다 → 데이터들 문제를 해결 할 수 있는 Train된 Machine Learning Model Section B. Basic Machine Learning: Supervised Learning0. Supervised Learning 제공되는 것들: N 개의 pair 로 된 training set $$D = {(x_1, y_1), …, (x_N, y_N)}$$ Data 별 loss function Loss function 은 필요에 따라서 우리가 디자인 해야 할 때도 있다. $$l(M(x), y) \\geq 0$$ Evaluation sets: Validation set과 test set 기존에 보지 못한 dataset 에도 trained model 이 잘 작동 하는지 확인하는 것이 필수 우리가 결정해야 하는 것들: Hypothesis sets: H1, H2 … : 모델, hyper parameter들이 다른 모델, 여러가지 실험 해보고 싶은 것들이 될 수 있다. 가설을 잘 설정하는 것이 최종의 모델을 결정하는데 중요한 기초 작업 Optimization Algorithm 어떤 방법론으로 최적화를 진행 할지 역시 매우 중요한 문제 결국, 우리가 해야 하는 것은 주어진 Training set 에서, hypothesis set 안의 각 Hm 마다 가장 좋은 모델을 찾는다. Trained Hm 중에서, Validation set 에서 가장 좋은 한 가지 모델을 결정한다. Reporting 을 위해 test set 에서 얼마나 좋은 성능을 나타내는지 확인한다. [Three points to Consider both in research and in practice]1. 어떻게 Hypothesis set 을 설정하는가? Hypothesis Set 자체가 infinite 하다는 문제 Neural Network 에서 국한되서 보자면, 어떤 Network Architecture 를 사용하여 모델을 구성할 것인지, 각 모델마다 hyper parameter 를 어떻게 설정할 것인지 등 hypothesis set 이 매우 다양하다. 이 중, 좋은 한가지 모델을 한가지 찾는 방법이 어렵다. 강의 표현 중 이를 찾는 것은 Science ——— Magic 사이에 어느 한 점인, 거의 Art 에 가깝다고 하셔서 매우 웃겼다. 개인적으로 이 부분이 가장 공부하면서도 어렵고, 그 모델을 찾는 것이 매우 추상적인 느낌이다. 그리고 개인적으로 진행하는 프로젝트에서 과연 내가 찾은 모델보다 더 좋은 성능을 가지는 모델혹은 파라미터는 없을까(무조건 있을 것인데..라고 생각하는 경우가 훨씬 많지만..)라고 생각하며 분석과 모델링의 열정을 높이곤한다. Network Architectures Neural Network 는 Directed Acyclic Graph이다!! Inference : Forward Computaion 만으로, 쉽게 trained neural network를 사용할 수 있다. 이를 구성하는데 있어, high-level 로 abstraction 된 라이브러리를 oop , functional programming 을 활용해 쉽게 구현할 수 있다. (pytorch, tensorflow…) 2. Loss Function 관점의 이동: 어떤 주어진 data x 에 대하여 y 는 무엇일까? 를 생각하는 모델이 아니라, $$f_\\theta(x) = ? $$ 주어진 x 에 대해 y가 어떤 case 혹은 값일 확률로 생각한다. $$p(y=y’|x) = ?$$ Distribution based loss functions Binary Classification : Bernoulli distribution → Sigmoid Multiclass Classification : Categorical distribution → Softmax Linear Regression : Gaussian distribution Multimodal linear Regression : Mixture of Gaussians 결국 Loss Function 은 다음과 같이 표현될 수 있다. Maximize logp $$argmax_\\theta \\sum_{n=1}^{N} logp_\\theta(y_n|x_n)$$ = minimize L(theta): $$L(\\theta) = \\sum_{n=1}^{N} l(M_\\theta(x_n), y_n)= -\\sum_{n=1}^{N} logp_\\theta(y_n|x_n)$$ 3. Optimization Optimization 방법 Optimization 방법에는 GD, SGD, Newton Method 등 다양한 방법이 있지만, Nerual Network 에서는 Gradient Descent 방법을 위주로 사용한다. 이 강의에서는 다루지 않았지만, Gradient Descent 방법 외의 다른 알고리즘들은 전제되는 가정들이 많고, Nerual Network 의 고차원에서는 그 가정을 만족하기가 쉽지 않다. (예를 들어, Newton Method 에서의 Hessian 행렬이 구해지기 위한 가정, computation 양 또한 매우 많다.) 이런 이유에서 Gradient Descent 방법을 사용한다. Backward Computation : Backpropagation Loss function 의 gradient 를 구하는 방법은 쉽지 않다. Neural Network는 Automatic differentiation (Autograd) 를 사용하여, weight 과 bias term 에 대해 쉽게(?) gradient 값을 구할 수 있다. library 덕분에 우리가 loss function 의 gradient 를 직접 구하지 않아도 된다.","link":"/2019/05/03/Lecture-딥러닝을-이용한-자연어-처리-Section-A-B/"},{"title":"Linear Model with Pytorch","text":"Linear Model with Pytorch 이 글의 목적은, 지난 Linear Regression 에서 좀더 나아가서, 다양한 Regression 예제들을 Linear Model (WX) 형태로 pytorch 를 이용해 풀어 보는 것입니다. Pytorch 를 사용하여 Modeling 과 loss function 등을 class 형태, 내장 loss 함수등을 사용해보겠습니다. 12345678910import torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Fimport numpy as npimport warningswarnings.filterwarnings(\"ignore\")%config InlineBackend.figure_format = 'retina'%matplotlib inline 1. Quadratic Regression Model$$f(x) = w_0 + w_1x + w_2x^2$$ 12345678x = np.linspace(-10, 10, 100)y = x**2 + 0.7 * x + 3.0 + 20 * np.random.rand(len(x))plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 12345x_train = torch.FloatTensor([[each_x**2, each_x, 1] for each_x in x])y_train = torch.FloatTensor(y)print(\"x_train shape: \", x_train.shape)print(\"y_train shape: \", y_train.shape) x_train shape: torch.Size([100, 3]) y_train shape: torch.Size([100])1234567891011121314151617W = torch.zeros(3, requires_grad=True)optimizer = optim.SGD([W], lr=0.0001)epochs = 10000for epoch in range(1, epochs + 1): hypothesis = x_train.matmul(W) loss = torch.mean((hypothesis - y_train) ** 2) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch: {} -- Parameters: W: {} -- loss {}\".format(epoch, W.data, loss.data)) epoch: 1000 -- Parameters: W: tensor([1.1738, 0.4943, 1.0699]) -- loss 85.30189514160156 epoch: 2000 -- Parameters: W: tensor([1.1581, 0.4949, 2.0311]) -- loss 76.05414581298828 epoch: 3000 -- Parameters: W: tensor([1.1437, 0.4949, 2.9105]) -- loss 68.31205749511719 epoch: 4000 -- Parameters: W: tensor([1.1305, 0.4949, 3.7151]) -- loss 61.83049011230469 epoch: 5000 -- Parameters: W: tensor([1.1185, 0.4949, 4.4514]) -- loss 56.4041862487793 epoch: 6000 -- Parameters: W: tensor([1.1075, 0.4949, 5.1250]) -- loss 51.86140060424805 epoch: 7000 -- Parameters: W: tensor([1.0974, 0.4949, 5.7414]) -- loss 48.058231353759766 epoch: 8000 -- Parameters: W: tensor([1.0882, 0.4949, 6.3054]) -- loss 44.8742790222168 epoch: 9000 -- Parameters: W: tensor([1.0798, 0.4949, 6.8214]) -- loss 42.20869445800781 epoch: 10000 -- Parameters: W: tensor([1.0721, 0.4949, 7.2935]) -- loss 39.9770965576171912345plt.plot(x, y, 'o', label='train data')plt.plot(x, (x_train.data.matmul(W.data).numpy()), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show() 2. Cubic Regression Model$$f(x) = w_0 + w_1x + w_2x^2 + w_3x^3$$ 2.1 Generate Toy data 100개의 data 를 생성합니다. 12x = np.linspace(-1, 1, 100)y = 3*x**3 - 0.2 * x ** 2 + 0.7 * x + 3 + 0.5 * np.random.rand(len(x)) 12345plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 2.2 Define Model x_train과 y_train 을 만들어줍니다. 123x_train = torch.FloatTensor([[xval**3, xval**2, xval, 1]for xval in x])y_train = torch.FloatTensor([y]).view(100, -1)y_train.shape torch.Size([100, 1]) 이번에 Model을 nn.Module 추상 클래스를 상속 받아, class 형태로 모델링 해보겠습니다. 1234567class CubicModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4, 1) def forward(self, x): return self.linear(x) 1model = CubicModel() train 시킬 때, loss 역시 nn.functional 에 있는 내장 mse loss 를 사용하여 보겠습니다. 1234567891011121314151617optimizer = optim.SGD(model.parameters(), lr=0.001)epochs = 15000for epoch in range(1, epochs + 1): hypothesis = model(x_train) # define loss loss = F.mse_loss(hypothesis, y_train) # Backprop &amp; update parameters optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1500 == 0: print(\"epoch: {} -- loss {}\".format(epoch, loss.data)) epoch: 1500 -- loss 0.22306101024150848 epoch: 3000 -- loss 0.11560291796922684 epoch: 4500 -- loss 0.09848319739103317 epoch: 6000 -- loss 0.08879078179597855 epoch: 7500 -- loss 0.08104882389307022 epoch: 9000 -- loss 0.07452096790075302 epoch: 10500 -- loss 0.06889640539884567 epoch: 12000 -- loss 0.06398065388202667 epoch: 13500 -- loss 0.05964164435863495 epoch: 15000 -- loss 0.05578556656837463412345plt.plot(x, y, 'o', label='train data')plt.plot(x, model(x_train).data.numpy(), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show() 3. Exponential Regression Model$$f(x) = e^{w_0x}$$ $$g(x) = \\ln f(x) = w_0x$$ Exponential 의 경우, Linear Model 형태를 만들어 주기 위해, log 를 씌워 주워 train 을 시킨후, 다시 exponential 을 양변에 취해주는 형태로 modeling 을 하여야 한다. 3.1 Generate Toy data123np.random.seed(20190505)x = np.linspace(-1, 1, 50)y = np.exp(2 * x) + 0.2 * (2 * np.random.rand(len(x)) - 1) 12345plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 3.2 Define Model123x_train = torch.FloatTensor([[xval, 1] for xval in x])y_train = torch.FloatTensor([np.log(y)]).view(50, -1)y_train.shape torch.Size([50, 1])1234567class ExpModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(2, 1) def forward(self, x): return self.linear(x) 이번에는 optimize algorithm 중 Adam 을 사용해 보겠습니다. Adam 은 adaptive 하게 learning rate 를 조정해 주는 algorithm 입니다. 1234567891011121314151617model = ExpModel()optimizer = optim.Adam(model.parameters())epochs = 15000for epoch in range(1, epochs + 1): hypothesis = model(x_train) loss = F.mse_loss(hypothesis, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch: {} -- loss {}\".format(epoch, loss.data)) epoch: 1000 -- loss 1.4807509183883667 epoch: 2000 -- loss 0.6568859815597534 epoch: 3000 -- loss 0.2930431365966797 epoch: 4000 -- loss 0.1839657723903656 epoch: 5000 -- loss 0.1683545857667923 epoch: 6000 -- loss 0.16775406897068024 epoch: 7000 -- loss 0.16775156557559967 epoch: 8000 -- loss 0.16775153577327728 epoch: 9000 -- loss 0.16775155067443848 epoch: 10000 -- loss 0.16775155067443848 epoch: 11000 -- loss 0.16775155067443848 epoch: 12000 -- loss 0.16775155067443848 epoch: 13000 -- loss 0.16775153577327728 epoch: 14000 -- loss 0.1677515208721161 epoch: 15000 -- loss 0.167751520872116112345plt.plot(x, y, 'o', label='train data')plt.plot(x, np.exp(model(x_train).data.numpy()), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show() 4. Sine &amp; Cosine Regression$$f(x) = w_0\\cos(\\pi x) + w_1\\sin(\\pi x)$$ 4.1 Generate Toy data12x = np.linspace(-2, 2, 100)y = 2 * np.cos(np.pi * x) + 1.5 * np.sin(np.pi * x) + 2 * np.random.rand(len(x)) - 1 12345plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 4.2 Modeling123x_train = torch.FloatTensor([[np.cos(np.pi*xval), np.sin(np.pi*xval), 1] for xval in x])y_train = torch.FloatTensor(y).view(100, -1)y_train.shape torch.Size([100, 1])1234567class SinCosModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3, 1) def forward(self, x): return self.linear(x) 1234567891011121314151617model = SinCosModel()optimizer = optim.Adam(model.parameters())epochs = 10000for epoch in range(1, epochs + 1): hypothesis = model(x_train) loss = F.mse_loss(hypothesis, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch: {} -- loss {}\".format(epoch, loss.data)) epoch: 1000 -- loss 1.1333037614822388 epoch: 2000 -- loss 0.45972707867622375 epoch: 3000 -- loss 0.36056602001190186 epoch: 4000 -- loss 0.3566252291202545 epoch: 5000 -- loss 0.3566077649593353 epoch: 6000 -- loss 0.3566077947616577 epoch: 7000 -- loss 0.3566077649593353 epoch: 8000 -- loss 0.3566077947616577 epoch: 9000 -- loss 0.3566077947616577 epoch: 10000 -- loss 0.356607764959335312345plt.plot(x, y, 'o', label='train data')plt.plot(x, model(x_train).data.numpy(), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show()","link":"/2019/05/04/Linear-Model-with-Pytorch/"},{"title":"Linear Regression with Pytorch","text":"Linear Regression through Pytorch 이번 포스트의 목적은 Linear Model을 Pytorch을 통해 구현해보며, 개인적으로 Pytorch의 사용을 연습하며 적응력을 높여보는 것입니다. Import Library12345678910import torchimport torch.optim as optimimport matplotlib.pyplot as pltimport numpy as npimport warningswarnings.filterwarnings(\"ignore\")%config InlineBackend.figure_format = 'retina'%matplotlib inline Generate Toy Data$ y = \\frac{1}{3} x + 5 $ 와 약간의 noise 를 합쳐 100 개의 toy data를 만들겠습니다. 12345# Target Functionf = lambda x: 1.0/3.0 * x + 5.0x = np.linspace(-40, 60, 100)fx = f(x) 123plt.plot(x, fx)plt.grid()plt.show() 123456# y_train data with little noisey = fx + 10 * np.random.rand(len(x))plt.plot(x, y, 'o')plt.grid()plt.show() 1. Gradient Descent Model (hypothesis) 를 설정합니다.(여기선, Linear Regression 이므로, $y = Wx + b$ 형태를 사용합니다.) Loss Function 을 정의합니다. (여기선, MSE loss 를 사용하겠습니다.) gradient 를 계산합니다.(여기선, Gradient Descent 방법으로 optimize 를 할 것이므로, optim.SGD() 를 사용합니다.) parameter 를 update 합니다. 1234x_train = torch.FloatTensor(x)y_train = torch.FloatTensor(y)print(\"x_train Tensor shape: \", x_train.shape)print(\"y_train Tensor shape: \", y_train.shape) x_train Tensor shape: torch.Size([100]) y_train Tensor shape: torch.Size([100])1234567891011121314151617181920212223242526# train code# parameter setting &amp; initializeW = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)# optimizer settingoptimizer = optim.SGD([W, b], lr=0.001)# total epochsepochs = 3000for epoch in range(1, epochs + 1): # decide model(hypothesis) model = W * x_train + b # loss function -&gt; MSE loss = torch.mean((model - y_train)**2) optimizer.zero_grad() loss.backward() optimizer.step() # 10 epoch 마다 train loss 를 출력합니다. if epoch % 500 == 0: print(\"epoch: {} -- Parameters: W: {} b: {} -- loss {}\".format(epoch, W.data, b.data, loss.data)) epoch: 500 -- Parameters: W: tensor([0.3709]) b: tensor([5.6408]) -- loss 22.728315353393555 epoch: 1000 -- Parameters: W: tensor([0.3467]) b: tensor([7.9427]) -- loss 11.399767875671387 epoch: 1500 -- Parameters: W: tensor([0.3368]) b: tensor([8.8829]) -- loss 9.51008415222168 epoch: 2000 -- Parameters: W: tensor([0.3327]) b: tensor([9.2669]) -- loss 9.194862365722656 epoch: 2500 -- Parameters: W: tensor([0.3311]) b: tensor([9.4237]) -- loss 9.142287254333496 epoch: 3000 -- Parameters: W: tensor([0.3304]) b: tensor([9.4878]) -- loss 9.13351631164550812345plt.plot(x, y, 'o', label=\"train data\")plt.plot(x_train.data.numpy(), W.data.numpy()*x + b.data.numpy(), label='fitted')plt.grid()plt.legend()plt.show() 2. Stochastic Gradient Descent Model (hypothesis) Setting Loss Function Setting 최적화 알고리즘 선택 shuffle train data mini-batch 마다 W, b 업데이트 12345678910111213141516# batch 를 generate 해주는 함수def generate_batch(batch_size, x_train, y_train): assert len(x_train) == len(y_train) result_batches = [] x_size = len(x_train) shuffled_id = np.arange(x_size) np.random.shuffle(shuffled_id) shuffled_x_train = x_train[shuffled_id] shuffled_y_train = y_train[shuffled_id] for start_idx in range(0, x_size, batch_size): end_idx = start_idx + batch_size batch = [shuffled_x_train[start_idx:end_idx], shuffled_y_train[start_idx:end_idx]] result_batches.append(batch) return result_batches 12345678910111213141516171819# trainW = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)optimizer = optim.SGD([W, b], lr=0.001)epochs = 10000for epoch in range(1, epochs + 1): for x_batch, y_batch in generate_batch(10, x_train, y_train): model = W * x_batch + b loss = torch.mean((model - y_batch)**2) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(\"epoch: {} -- Parameters: W: {} b: {} -- loss {}\".format(epoch, W.data, b.data, loss.data)) epoch: 500 -- Parameters: W: tensor([0.0890]) b: tensor([9.5399]) -- loss 162.1055450439453 epoch: 1000 -- Parameters: W: tensor([0.3672]) b: tensor([9.5366]) -- loss 12.424881935119629 epoch: 1500 -- Parameters: W: tensor([0.3560]) b: tensor([9.5097]) -- loss 7.826609134674072 epoch: 2000 -- Parameters: W: tensor([0.3375]) b: tensor([9.5556]) -- loss 13.15934944152832 epoch: 2500 -- Parameters: W: tensor([0.2462]) b: tensor([9.5157]) -- loss 11.582895278930664 epoch: 3000 -- Parameters: W: tensor([0.3097]) b: tensor([9.5111]) -- loss 9.991677284240723 epoch: 3500 -- Parameters: W: tensor([0.2497]) b: tensor([9.5532]) -- loss 20.481367111206055 epoch: 4000 -- Parameters: W: tensor([0.4388]) b: tensor([9.5390]) -- loss 20.827198028564453 epoch: 4500 -- Parameters: W: tensor([0.1080]) b: tensor([9.4959]) -- loss 140.0277862548828 epoch: 5000 -- Parameters: W: tensor([0.3188]) b: tensor([9.4829]) -- loss 6.635367393493652 epoch: 5500 -- Parameters: W: tensor([0.2553]) b: tensor([9.5017]) -- loss 25.45773696899414 epoch: 6000 -- Parameters: W: tensor([0.2490]) b: tensor([9.5489]) -- loss 9.580666542053223 epoch: 6500 -- Parameters: W: tensor([0.3189]) b: tensor([9.5347]) -- loss 12.585128784179688 epoch: 7000 -- Parameters: W: tensor([0.3026]) b: tensor([9.4874]) -- loss 8.298829078674316 epoch: 7500 -- Parameters: W: tensor([0.3507]) b: tensor([9.6815]) -- loss 13.348054885864258 epoch: 8000 -- Parameters: W: tensor([0.1423]) b: tensor([9.5220]) -- loss 32.567440032958984 epoch: 8500 -- Parameters: W: tensor([0.7147]) b: tensor([9.5182]) -- loss 75.97190856933594 epoch: 9000 -- Parameters: W: tensor([0.5170]) b: tensor([9.5289]) -- loss 39.07848358154297 epoch: 9500 -- Parameters: W: tensor([0.3748]) b: tensor([9.5590]) -- loss 10.358983993530273 epoch: 10000 -- Parameters: W: tensor([0.2958]) b: tensor([9.6088]) -- loss 7.410649299621582 Stochasitic 하게 loss의 gradient 를 계산하여, parameter update를 하므로, loss 가 굉장히 oscilation 이 나타나며 감소하는 것을 볼 수 있다. 12345plt.plot(x, y, 'o', label=\"train data\")plt.plot(x_train.data.numpy(), W.data.numpy()*x + b.data.numpy(), label='fitted')plt.grid()plt.legend()plt.show() 12","link":"/2019/05/03/Linear-Regression-with-Pytorch/"},{"title":"Mysql","text":"MySQL1. Install MySQL https://dev.mysql.com/downloads/mysql/5.7.html#downloads 에서DMG 파일 다운로드 시스템 환경설정에서 MySQL -&gt; Start MySQL Server : server 시작 1$ cd usr/local/mysql/bin 위 경로에서 MySQL 서버에 접속1$ sudo ./mysql -p 1mysql &gt; 패스워드 변경123mysql&gt;ALTER USER 'root'@'localhost' IDENTIFIED BY '바꾸고싶은 비밀번호';mysql&gt;FLUSH PRIVILEGES;mysql&gt;quit; 2. MySQL Shell Command(1) DATABASE 생성, 접속, 삭제 현재 상태 보기 1mysql&gt; STATUS DB 목록 보기 1mysql&gt; SHOW DATABASES; DB 만들기 1mysql&gt; CREATE DATABASE DBNAME DB 접속하기 1mysql&gt; USE DBNAME; 현재 접속중인 DB 확인하기 1mysql&gt; SELECT DATABASE(); DB 지우기 1mysql&gt; DROP DATABASE DBNAME; (2) TABLE 생성, 추가, 삭제 table 만들기 1234567CREATE TABLE table_name( column_name_1 column_data_type_1 column_constraint_1, column_name_2 column_data_type_2 column_constraint_2, . . .) column_constraint 는 Optional 이다. (unique 와 같은 제약조건) user 라는 table에 name, email, age 컬럼 생성 example1 : constraint 가 없을 떄, 12345mysql&gt; CREATE TABLE user( name CHAR(20), email CHAR(40), age INT(3)) example2 : constraint가 있을 때, 1234567mysql&gt; CREATE TABLE user2( user_id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20) NOT NULL, email VARCHAR(30) UNIQUE NOT NULL, age INT(3) DEFAULT'30', rdate TIMESTAMP) ## (3) 수정 ### (3)-1. DATABASE 수정 - DATABASE 의 **encoding** 수정 - 현재 문자열 encoding 확인 1mysql&gt; SHOW VARIABLES LIKE \"CHARACTER_SET_DATABASE\"; mydb 데이터베이스의 문자열 인코딩을 utf8 로 변경 1mysql&gt; ALTER DATABASE mydb CHARACTER_SET = utf8; user 데이터베이스의 문자열 인코딩을 ascii 로 변경 1mysql&gt; ALTER DATABASE user CHARACTER_SET=ascii (3)-2. TABLE 수정 user 테이블에 tmp라는 컬럼명, TEXT 데이터 타입 컬럼을 추가1mysql&gt; ALTER TABLE user ADD tmp TEXT;","link":"/2018/11/01/MySQL/"},{"title":"NGINX","text":"Nginx0. What is nginx? client 가 외부 IP와 그 포트를 통해 server 에 접근하면, 그 때부터는 request 를 내부 IP 와 port 로 연결을 해주어야 한다. 일종의 proxy server 역할을 해주는 것이 nginx 1. Install Nginx12$ sudo apt-get update$ sudo apt-get install nginx 2. Manage the Nginx Process nginx Process1234567891011121314#start Nginx$ sudo systemctl start nginx#stop nginx$ sudo systemctl stop nginx#restart nginx$ sudo systemctl restart nginx#check status nginx$ sudo systemctl status nginx# reload nginx$ sudo systemctl reload nginx 3. Structure /etc/nginx Nginx의 설정에 관련된 directory /etc/nginx/nginx.conf nginx의 기본설정 파일, global설정은 이 파일에서 /etc/nginx/sites-available/ 포트 접속시 개별 설정하는 directory 여기 안에 default 파일 변경 4. Nginx Configuration4.1 static file serving/etc/nginx/sites-available/default 파일 수정 123456789server { location / { root /path/to/html ; } location /images/ { root /path/to/image; }} 4.2 proxy server port 별로 설정이 가능하다 12345678server { # default 는 80 port (http default) # 8080 port 에 대해서 listen 8080; location / { proxy_pass http://localhost:8080; }} 여러개의 포트를 연결 server { listen 8080; listen 80; location / { proxy_pass http://localhost:8080; } }","link":"/2018/11/29/NGINX/"},{"title":"Pillow","text":"Pillow 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Pillow 는 Python에서 이미지를 핸들링 하기위한 라이브러리이다. 스크린 샷, 이미지 크롭, 썸네일등을 만들 수 있다. 또한, 다양한 이미지 확장자를 다룰수 있다. (예, jpg, png) Pillow install1pip install Pillow 1. Import Convention Pillow 를 사용하기에 앞서, Import 해야한다.1from PIL import Image 2. Pillow 메소드(1) open() 이름 그대로, 경로와 파일 이름을 적어주면, 해당 이미지리턴하여 Variable 에 할당한다. 1image = Image.open(\"imagefile_name\") (2) crop() 이름 그대로, image 를 잘라준다. 이 때 Parameter 는 box형태로 들어가게 된다. box 는 (left, upper, right, lower) 로 들어가게 된든데, pixel 단위로 들어가면 된다 이미지의 가장 왼쪽과 위쪽 라인을 기준으로, left px 만큼 upper px 만큼 (왼쪽을 기준으로) right px 만큼 (위쪽을 기준으로) bottom px 만큼 잘라, box 로 만들어 준다.123box = (10, 10, 110, 110)image.crop(box)### 이렇게 되면 가로세로 100px 만큼의 이미지가 만들어진다. (3) thumbnail() 썸네일을 만들어준다. 썸네일의 활용 방안은 한 가지 사진을 어플리케이션 곳곳에서 사용한다고 할 때, 데이터 사이즈가 큰 원본 사진을 계속 해서 들고 다니며 사용하면 어플리케이션에 따라 메모리 낭비, 서버용량 낭비, 트래픽 낭비 등으로 이어질 수 있다. 따라서, 이미지의 데이터 크기와 해상도를 낮춰 사용할 수 있다. 12image.thumbnail((pixel, pixel))#thumbnail 메소드를 사용하여, 원하는 pixel 수로 줄일 수 있다.","link":"/2018/10/25/Pillow/"},{"title":"Provision","text":"Server Provisioning &amp; Terraform Provisioning 이란, 한정된 자원을 최적의 효율을 위해 제공하는 기술적 개념을 말한다. 유저의 요청에 맞게 자원을 미리 세팅해두고, 유저의 요청에 따라 준비된 자원들을 목적과 효율에 맞게 제공하는 개념이다. 특정 분야에서 한정되어 사용하는 개념이 아니라 다양한 분야에서 응용되어지는 주제이다. (IT 분야만으로 한정되지도 않는다) IT 분야의 Provisioning의 예시로는, Server Provisioning, Storage Provisioning, Telecommunication Provisioning 등이 있다. 여기서는 Terraform 을 활용한 AWS 서버 프로비져닝에 관해 다룬다. ## AWS EC2 AWS EC2를 활용하기 위해서는 3가지의 기본적인 세팅이 필요하다. 키페어 (Key pair) 보안그룹 (Security Group) 인스턴스 (Instance) 이 세가지를 Terraform 을 활용해 생성하는 코드를 정리한다. 1. 키페어 생성 (Key pair)1-1. Key 만들기- 자신의 email로 ssh key 를 생성하여, key_name 이름으로 .ssh 폴더에 저장ssh-keygen -t rsa -b 4096 -C “email” -f “$HOME/.ssh/key_name” -N “” - 이렇게 생성된 key 는 /key_name/ 과 /key_name.pub/로 private key 와 public key가 생성된다. 1-2. Key pair 생성- `main.tf` 파일 생성123456789`provider “aws” { # 이 region 은 seoul 을 의미한다 region = “ap-northeast-2”}Resource “aws_key_pair” “resource_name” { # keygen 으로 생성한 key_name key_name = &quot;key_name&quot; public_key = &quot;${file(&quot;~/.ssh/key_name.pub&quot;)}&quot;} - apply 를 실행해, aws 키페어를 생성123$ terraform init$ terraform plan$ terraform apply - destroy 를 실행해, aws 키페어를 삭제$ terraform destroy 2. 보안그룹 생성 (Security Group)- 보안그룹은 생성될 인스턴스의 정책을 설정하는 부분이다. 가장 대표적인 기능은 인바운드와 아웃바운드 port 를 설정할 수 있다. - 필요에 따라 port number 를 열어주면 된다. - `ingress` 는 인바운드, `egress` 는 아웃바운드 태그이다. - `from_port` 와 `to_port` 는 말 그대로 from 부터 to 까지의 번호를 지정한다. - 대표적인 포트번호에 관한 설명 - 22 : ssh 접속을 위한 포트 - 80 : http의 기본포트 - 8888 : Jupiter notebook 사용을 위한 포트 - 27017 : MongoDB 사용을 위한 포트 - 3306 : MySQL 사용을 위한 포트 ( /여기서는 DB의 경우, 다른 서버를 두고 사용하고 있으므로, 열어주지 않는다/ ) - `main.tf` 파일 생성1234567891011121314151617181920212223242526provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot;}resource &quot;aws_security_group&quot; &quot;resource_name&quot; { name = &quot;보안그룹 이름&quot; description = &quot;보안그룹의 설명&quot; ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { from_port = 80 to_port = 80 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { from_port = 8888 to_port = 8888 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] }} 마찬가지로, init, plan, apply 를 활용하여 보안그룹을 생성하고, destroy 로 제거한다. 3. 인스턴스 생성- 대망의 인스턴스 생성! - 여기선, EC2 중, linux ubuntu 18.04 버젼 을 활용한다. AWS 내에서 다른 종류의 인스턴스를 사용할 경우, ami 를 다른 값으로 사용하면 된다. - 또한, 다양한 인스턴스 유형중, t2.nano를 사용한다. 인스턴스 유형도 필요에 따라 다르게 설정하면 된다. - `main.tf` 파일 생성12345678910111213141516171819provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot;}data &quot;aws_security_group&quot; &quot;resource1&quot; { name = &quot;security_group_name&quot;}resource &quot;aws_instance&quot; &quot;resource2&quot; { ami = &quot;ami-06e7b9c5e0c4dd014&quot; instance_type = &quot;t2.nano&quot; key_name = &quot;key_name&quot; vpc_security_group_ids = [ &quot;${data.aws_security_group.resource1.id}&quot; ] tags { Name = &quot;dss_instance&quot; }} - `init`, `plan`, `apply` 를 활용하여 인스턴스를 생성하고, `destroy` 로 제거한다.4. 생성된 인스턴스 확인ssh -I ~/.ssh/key_name ubuntu@외부ip 로 생성된 인스턴스를 확인하고, 활용할 수 있다.","link":"/2019/01/10/Provision/"},{"title":"Queue","text":"Queue 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Queue의 특징 내가 버스정류장에 서있다고 생각해보면, 버스 전용차선으로 버스가 줄지어 들어온다. 아무리 뒷차가 손님을 다 태웠다고해서, 앞에 버스가 아직 손님을 태우고 있으면 뒷 버스는 출발 하지 못한다. 이것이 바로 QUEUE FIFO : First In First Out == 선입선출 or 후입후출 front와 rear 라는 index가 각각 구조의 맨 앞과 뒤를 가리키고 있다. Data는 rear로 들어가고, front 에서 나온다. ADT empty() 라는 메소드로 Queue 에 data가 있는지 없는지 확인하게 한다. empty 이면 True, not empty 이면, False 를 return 한다. Queue.empty() returns Boolean enaueue(data) 메소드로 Queue의 rear 가 가리키고 있는 data 뒤에 data를 넣는다. Queue.enqueue (data) returns None dequeue() 메소드로 Queue의 front가 가리키고 있는 data를 반환하면서 삭제 된다. Queue.dequeue() returns data peek() 메소드로 Queue의 front가 가리키고 있는 data를 반환한다. peek은 어디까지나 확인하는 메소드 이므로, data가 삭제되지 않는다. Queue.peek() returns data 구현 1 : by python list Queue 구조를 Python에 내장 되어 있는 list를 container 로 구현한다. 123456789101112131415161718192021class Queue: def __init__(self): self.container=list() def empty(self): # ADT 를 따라서, 비어 있으면 return True 비어있지 않으면 return False if not self.container: return True return False def enqueue(self, data): # list 를 활용했으므로, list.append(data)를 활용할 수 있다. self.container.append(data) def dequeue(self): # pop 역시 list.pop()을 활용할 수 있다. return self.container.pop(0) def peek(self): # peek은 단지 front 에 어떤 데이터가 있는지 확인 하는 것 뿐이므로, 현재 리스트의 가장 앞 부분에 있는 data 를 확인하면 된다. return self.container[0] 구현 2 :","link":"/2018/10/22/Queue/"},{"title":"Requests","text":"WEB CRAWLING 1 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Requests Requests 패키지는 크롤링 하고자 하는 페이지를 url 값을 통해 가져와 객체로 변환 해주는 기능을 가지고 있다. 1. Installation Requests 를 이용하기 위해 package 를 설치해준다.1$ pip3 install requests - request를 이용하면서, json 형식으로의 크롤링과, html 형식으로 css selecting 을 위한 크롤링을 실습해 볼 것이므로, BeautifulSoup Package 역시 같이 설치해준다. (BeautifulSoup은 html parser 를 이용해보기 위함이다.) - python-forecastio 는 dark sky 의 api를 활용해 날씨 데이터를 받아올때 사용해보기 위해 설치한다. 12$ pip3 install bs4$ pip3 install python-forecastio import 할 것들1234import requestsimport forecastiofrom bs4 import BeautifulSoupfrom pandas.io.json import json_normalize 2. [ jSON ] Dark Sky api 활용 날씨정보 가져오기 DarkSky api 는 위도와 경도를 입력하면, 날씨 정보를 주는 api 이다. https://darsky.net/dev/ 에 가입 후, TOKEN 을 받는다. 위에서 받은 개인의 TOKEN 을 활용해 url 을 먼저, formating을 해준다. requests의 get 메소드를 활용해 url 의 정보를 받아온다. 받아온 정보를 requests 의 json 메소드로 json 형식으로 변환해준다. json 을 확인하고 원하는 정보를 return 해준다. 12345def forecast(lat, lng): url = \"https://api.darksky.net/forecast/{}/{},{}\".format(TOKEN, lat, lng) response = requests.get(url) json_obj = response.json() return json_obj[\"hourly\"][\"summary\"] 3. [ html ] BS4 활용 html selecting 해서 가져오기 네이버의 실시간 검색순위 부분의 text 를 크롤링 해보자 html 파일을 BS4 를 활용해 받아온뒤, CSS selecting 으로 원하는 text data 를 가져온다. 123456789101112131415from bs4 import BeautifulSoupdef naver(): url = \"https://www.naver.com\" df = pd.DataFrame(columns=[\"rank\", \"keyword\"]) response = requests.get(url) dom = BeautifulSoup(response.content, \"html.parser\") # BeautifulSoup(markup, features, builder, parse_only, from_encoding, exclude_encodings) for keyword in keywords: df.loc[len(df)] = { \"rank\":keyword.select_one('.ah_r').text, \"keyword\":keyword.select_one('.ah_k').text } return df print(response.content)의 모양을 보자. 따라서, BeautifulSoup 으로 html 형식으로 parsing 해주고, css 를 활용해 selecting 해준다. .select 는 여러개의 엘리먼트를 선택 -&gt; 결과가 리스트 .select_one은 하나의 엘리먼트를 선택 -&gt; 결과가 하나의 객체","link":"/2018/10/24/Requests/"},{"title":"SPARK BASIC 1","text":"Apache Spark Basic 1 SPARK 를 공부하면서 실습 과정을 정리해서 남깁니다. 실습환경 CentOS Spark 2.4.3 Hadoop 2.7 1. Spark-shell1-1. Introductionshell의 spark home directory 에서 다음 명령어를 통해 spark shell 을 진입할 수 있습니다. $ cd /spark_home_directory/ $ ./bin/spark-shell sc : spark context spark : spark session spark context 와 spark session 의 경우, spark shell에 띄우면서 내부적으로 선언된 변수명이다. $ jps // jps 명령어를 통해 현재 돌고 있는 spark process 를 확인할 수 있다. // spark processor 가 jvm 을 바탕으로 돌기 때문에, jvm 프로세스가 도는 것을 확인하므로써 // 확인 할 수 있는 것이다.http://localhost:4040, 즉 해당 서버의 ip:4040 포트를 통해서 드라이버에서 제공되는 웹 UI 를 확인할 수 있다. 이 웹 UI 를 통해 현재 작동하는 프로세서와 클러스터들을 관리 할 수 있다. 1-2. RDDspark는 data를 처리할 때, RDD 와 Spark SQL 을 통해서 data object 를 생성하고 이를 바탕으로 다양한 pipeline 으로 동작 할 수 있다. RDD 를 처음 접해보는 실습. //scala shell val data = 1 to 10000 val distData = sc.parallelize(data) distData.filter(_ &lt; 10).collect() data 가 RDD sc.parallelize의 return 형 역시 parallelize 된 RDD, 즉 distData 도 RDD 마지막 command line 은 10보다 작은 data 에 대해 filtering 하고 각 executor 에서 실행된 자료를 collect() spark 의 특징은 .collect() 와 같은 action api 가 실행될 때 모든 것이 실행되는 Lazy Evaluation (RDD)으로 동작한다. 드라이버 웹 UI 를 통해 이를 확인 할 수 있다. 이전 command line 에서는 아무 동작도 일어나지 않다가 collect() action api 수행을 통해 실제로 command들이 수행되는 것을 확인 할 수 있다. local 에서 default 로 동작하기 때문에 2개의 partition 으로 동작하며, 어떤 shuffling 도 일어나지 않았기 때문에 1개의 stage 임을 확인 할 수 있다. // scala shell // sc.textFile 을 통해 textfile, md 파일등을 읽어드릴 수 있다. val data = sc.textFile(&quot;file_name&quot;) // rdd 의 .map api 를 통해서 rdd 의 element 마다 val distData = data.map(r =&gt; r + &quot;_experiment!!&quot;) // 앞선 map 이 수행되고, 각 element(data) 갯수를 세개 된다. distData.count여기서는 .count 가 action api 이므로, .count 가 수행될 때, 앞선 command 들이 수행되게 된다. sc.textFile() 의 경우 ‘\\n’, newline 을 기준으로 element 를 RDD 에 담게 된다. RDD.toDebugString 를 통해 해당 RDD 의 Lineage 를 확인 할 수 있다. 가장 왼쪽에 있는 | 를 통해 stage 정보 역시 확인 할 수 있다. shuffle 이 일어나게 되면, stage가 바뀌므로, 서로 다른 stage 에 있는 command 의 경우, 다른 indent에 있게 된다. RDD.getNumPartitions 를 통해 해당 RDD의 Partition 갯수 (=Task 의 갯수), 즉 병렬화 수준을 확인 할 수 있다. Shuffle!!suffle 이 일어나는 경우는 api 마다 다양할 수 있다. 가장 기본적으로, 우리가 default partition 갯수를 변경하므로써 shuffle 이 일어나는 것을 확인 할 수 있다. val data = sc.textFile(&quot;file_name&quot;) data.getNumPartitions // Partition 의 숫자를 확인해보면, default 이므로 2 인 것을 확인 할 수 있다. val newData = data.repartition(10) newData.getNumPartitions // Partition 갯수가 10로 변경된 것을 확인 할 수 있다. newData.toDebugString // newData 의 Lineage 를 확인하면, repartition 이 일어나면서 shuffle 이 되고, // shuffle 로 인해 stage 가 2개가 되는 것을 확인 할 수 있다.(indentation) newData.count // action api 를 수행하여 앞선 command 를 모두 수행 위 command line 에 대한 DAG 를 웹 UI 를 통해 확인하면, 다음과 같이 stage 가 repartition을 기점으로 나누어 지는 것을 확인 할 수 있다. 총 Partition의 갯수 (Task의 갯수)를 확인해 보면, default 로 수행된 partition 2 개와, 우리가 설정해준 Partition 의 갯수인 10개를 합하여 12개인 것을 확인 할 수 있다. 여기서 한 스텝을 더 들어가보면, spark 만의 특이한 특징을 확인 할 수 있다. // 위 코드에 이어서, newData 에 대해 // newData RDD를 collect 해서 cli에 찍는 command 를 수행해보자. newData.collect.foreach(println)collect api 와 RDD의 element를 print 를 하는 action api 를 수행할 때, 지금까지 공부한 것으로 생각해 보면, text를 읽어서, 2개의 Partition 을 나누고, 다시 10개의 Partition 을 나누는 작업으로 이전의 12 개의 Task 와 다를게 없을 것 같은 느낌이다. 하지만 UI 를 통해 확인해보면, 10개의 Partition 으로 2개가 skipped 되었다고 확인할 수 있다. DAG 에서도, skipped 된 stage에 대해서 회색으로 확인된다. 이는 spark 에서 이 커맨드라인을 수행할 때, process 간 통신이 file 을 기반으로한 통신을 했기 때문이다. 제일 처음 newData 에 대해서 수행 될 때, 첫 stage 에서 shuffle 이 수행 될 때, 해당 파일을 각 executor 에서 shuffle write 을 하고 저장해두었다가, 두번째 stage 에서 shuffle 이 수행 될때, shuffle read 를 하는 방식으로 file을 기반으로 processor 가 통신하게 된다. 따라서, spark 가 같은 command line 을 수행하게 되면 미리 shuffle write 된 file 을 읽기만 함으로써 앞선 stage 의 동일한 반복 작업을 수행하지 않게 되는 것이다. UI 를 확인 해보아도, shuffle read 만 수행 되었다. SaveFile!!!RDD.saveAsTextFile(&quot;directory_name&quot;) api 를 활용하여, 어떤 처리가 끝난 RDD 를 저장할 수 있다. 이 때 주의 할 점은 parameter 에 들어 가는 것이 directory_name 이라는 것이다. 또한 partition 별로 파일이 저장된다. (e.g. 10개의 partition 이라면, 10개의 file이 저장된다.) Cache!!!spark가 자랑하는 가장 큰 특징은, data(RDD) 를 memory에 cache 함으로써 처리의 속도가 매우 빠르다는 점이다. RDD.cache api 를 통해 memory 에 캐시할 수 있다. // distData RDD 에 이름을 부여 distData.name = &quot;myData&quot; // cache! distData.cache // action : 5 개의 data 를 가져옴 distData.take(5) // action : collect distData.collectdistData.take(5) 까지 한 결과를 UI 에서 cache 를 살펴보면, 다음과 같다. 우리가 설정 한 것 처럼, RDD 의 이름이 myData 로 들어간것을 확인 할 수 있고 cache 역시 확인 할 수 있다. 하지만, Cached 된 비율을 확인하면 전체 RDD 에서 50% 만 된 것을 확인 할 수 있다. 반면에, distData.collect action 을 취하게 되면, Fraction Cached 가 100% 가 된 것을 확인 할 수 있다. 이는 우리의 action 에 따라 cache 할 용량이 달라 질 수 있기 때문이다. spark 입장에서 take(5) api 는 전체 RDD 중 5개의 element data 만 가져오면되고, 이 때 2개의 Partition 중 하나의 Partition 만 cache 해도 충분하기 때문에 Fraction Cached가 50%라고 나오는 것이다. 반면 collect api 는 collect 자체가 각 executor 에 있는 data 를 driver 로 모두 가져오는 것이므로 100% cache 하게 된다. Cache 에서 중요한 것은, 각 executor 의 cache 를 위한 가용 메모리 공간이 해당 Partition의 용량보다 작을 경우, 저장 할 수 있는 용량만큼 저장되는 것이 아니라, 해당 Partition 은 아예 저장이 안되게 된다. 이 점은 Cache를 할 때, Partition 의 용량과 해당 Executor 의 가용 메모리 공간을 미리 파악하여, 설계해야 한다. Word Count 예제!!!우리가 데이터 분석을 할 때, 가장 basic 한 방법은 해당 데이터의 갯수를 세어 보는 것이다. 본 예제에서는 텍스트 파일을 읽어, 띄어쓰기를 바탕으로 word token을 나누고, 이를 세어보자. WordCount 예제는 매우 basic 한 코드이므로, 어떤 로직으로 돌아가는지 완벽한 이해와 코드작성이 필수라고 생각한다. val originalDataRDD = sc.textFile(&quot;text-file&quot;) val wordcountRDD = originalDataRDD.flatMap(line =&gt; line.split(&quot; &quot;)) .map(word =&gt; (word, 1)).reduceByKey(_ + _) wordcountRDD.collect.foreach(println) originalDataRDD 에서 text-file을 읽고, line 마다 띄어쓰기를 기준으로 split 하고 이를 .flatMap 을 통해, flatten 하게 됩니다. 그리고 .map 을 통해 (word, 1) tuple 형태로 mapping 합니다. .reduceByKey 를 통해 같은 word 에 대해 그 counting 갯수를 더하게 된 것을 RDD 로 return 하게 됩니다.","link":"/2019/05/31/SPARK-BASIC-1/"},{"title":"Scrapy","text":"Scrapy 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Scrapy 라이브러리는 파이썬에서 제공하는 라이브러리로써, 대량의 페이지들의 Crawling을 손쉽게 해주는 라이브러리이다. 1. Install 파이썬의 라이브러리 이므로 pip 으로 설치 할 수 있다.1pip3 install scrapy 2. 실습 실습을 위해 import 할 것들 123import scrapyimport requestsfrom scrapy.http import TextResponse requests 를 통해 url 정보를 받아온다. TextResponse 를 통해 받아온 html 파일을 encoding 과 text형식으로 return 12req = requests.get(\"url_name\")response = TextResponse(req.url, body=req.text, encoding=\"utf-8\") 123456a = response.xpath('xpath')# xpath 로 지정한 엘리먼트를 가져온다.a_text = reponse.xpath('xpath/text()')# 엘리먼트의 text data 를 가져온다.a_text.extract()# 엘리먼트의 text data들을 말그대로 extract 하여, list 형태로 return 해준다 3. Scrapy 사용하기(1) scrapy 프로젝트 생성 shell command1scrapy startproject crawler 1!scrapy startproject crawler New Scrapy project &apos;crawler&apos;, using template directory &apos;/Users/emjayahn/.pyenv/versions/3.7.0/envs/dss/lib/python3.7/site-packages/scrapy/templates/project&apos;, created in: /Users/emjayahn/Dev/DSS/TIL(markdown)/crawler You can start your first spider with: cd crawler scrapy genspider example example.com1!tree crawler crawler ├── crawler │ ├── __init__.py │ ├── __pycache__ │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── __pycache__ └── scrapy.cfg 4 directories, 7 files(2) Scrapy 기본 구조 Spider 크롤링 절차 정하기 어떤 웹사이트들을 어떻게 크롤링 할 것인지 선언 각각의 웹페이지의 어떤 부분을 스크래핑 할 것 인지 명시하는 클래스 items.py spider 가 크롤링한 data 들을 저장할 때, 사용자 정의 자료구조 클래스 MVC : 중 Model 부분에 해당 Feature 라고 생각 pipeline.py 스크래핑한 데이터를 어떻게 처리할지 정의 데이터에 한글이 포함되어 있을 때는 encoding=’utf-8’ utf-8인코딩이 필요 settings.py Spider, item, pipeline 의 세부 사항을 설정 (예) 크롤링 빈도 등 (예) robots.txt - ROBOTSTXT_OBEY=True","link":"/2018/10/26/Scrapy/"},{"title":"Selenium","text":"Selenium 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 자동화 할 수 있는 프로그램 [ Install Selenium ] chrome driver 다운로드 1$ mv ~/Download/chromedriver /usr/local/bin Selenium Python Package 설치 1$ sudo pip3 install selenium 1. 셀레니움 사용해보기 import 1from selenium import webdriver Open Browser (Chrome Driver) 1driver = webdriver.Chrome() 페이지 이동 1driver.get(url) 브라우져 상에서 자바 스크립트 실행 1driver.execute_script(\"window.scrollTo(300, 400)\") 지금 control하고 있는 window 객체 확인 123main_window = driver.current_window_handlemain_window#현재 control 하고 있는 window 의 객체를 리턴한다. 열려 있는 전체 window 탭 모두 확인 123windows = driver.window_handleswindows#현재 열려있는 모든 탭의 객체를 리스트 형태로 리턴한다. 열려있는 window 탭 중, control 대상 window 로 바꾸기 123driver.switch_to_window(windows[0])#다시 원래 열었던 창으로 돌아가기driver.switch_to_window(main_window) 2. Alert 와 Confirm 다루기 웹을 자동화해서 다니다보면, Alert 창이나 Confirm 창이 의도치 않게 나올 수 있다. 이를 다루는 방법은 다음과 같다. Alert와 Confirm 창의 차이는, Alert 는 확인 창 하나만 있고, Confirm은 확인과 취소가 같이 있는 창이다. (1) Alert Alert 띄우기 12drive.switch_to_window(main_window)drive.execute_script(\"alert('이게 alert 로 뜰겁니다.');\") Alert 확인 누르기 12alert = driver.switch_to.alertalert.accept() Alert 창이 있으면 확인을 누르고, 없으면 없다고 리턴하기 (예외처리) 12345try: alert = driver.switch_to.alert alert.accept()except: print(\"alert 가 없습니다.\") (2) Confirm Confirm 띄우기 1driver.execute_script(\"confirm('이게 Confirm 창입니다.');\") Confirm 창 확인 누르기 or 취소 누르기 12345confirm = driver.switch_to.alert# 확인# confirm.accept()# 취소confirm.dismiss() 3. 입력창에 글씨 입력하기 이제부터 Selenium을 통해 특정 html 의 element 에 액션을 주려면,각종 Selector 를 사용하여 html 상의 element을 셀렉팅 하고, 해당 element 에게 액션을 주어야한다. 1driver.find_element_by_css_selector(nameof cssselector).send_keys(\"입력할 내용\") .find_element_by_css_selector 와 .find_elements_ by_css_selector 는 다르다.element 는 하나의 selector 만 선택하는 반면, elements 는 여러가지 element 를 셀렉팅 해서, 리스트 형식으로 Return 한다. 따라서 이렇게 리스트 형식으로 Return 이 된 경우, List[0] 등과 같이 그 엘리먼트를 뒤에 지정해줘야된다. 즉,1234driver.find_element_by_css_selector(nameof cssselector)#위는 바로 selecting 이 된것이지만,driver.find_elements_by_css_selector(nameof cssselector)[0]#위는 뒤에 selecting 위해 list 의 요소를 선택 해주어야한다. 4. 버튼 클릭하기12# 위의 방법처럼, element 를 선택해준다.driver.find_element_by_css_selector(\"name of css selector\").click() 5. id, class, 그 외의 attribute 값을 selecting 하는 법1234driver.find_element_by_css_selector(\"name of css selector.\").click()# id 의 경우 : #idname# class 의 경우 : .classname# 다른 attribute 의 경우 : [attribute = 'value'] 6. selecting 한 element 의 attribute value 를 얻는 방법 CSS로 선택한 element에 html 태그에는 다양한 attribute 들이 있을 수 있다. 이 중attribute의 value에 접근 하고 싶을 때는 .get_attribute()메소드를 사용할 수 있다.1driver.find_element_by_css_selector(\"name of css selector\").get_attribute(\"attribute_name\") 7. element 의 위치와, 사이즈 구하기 스크롤을 내리거나, 엘리먼트의 위치와 크기를 알고 싶을 때 사용할 수 있다.12345element = driver.find_element_by_css_selector(\"name of css selector\")element.location#element 의 좌측 상단의 위치가 pixel 단위로 x, y 값의 dictionary로 보여준다.element.size#element 의 size 를 height, width를 pixel 단위로 dictionary로 보여준다.","link":"/2018/10/24/Selenium/"},{"title":"Stack","text":"Stack 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Stack 의 특징 접시를 쌓듯이 데이터를 쌓아 올리는 모양의 데이터 구조 LIFO : Last In First Out == 후입선출 or 선입후출 top index 가 항상 Stack의 가장 윗부분을 가리키고 있어, 우리는 top 위치만 볼 수 있다. 단점 : stack 의 top 이 외에 밑에 쌓여져있는 데이터의 Search 가 안된다. ADT empty() 라는 메소드로 Stack 에 data가 있는지 없는지 확인하게 한다. empty 이면 True, not empty 이면, False 를 return 한다. Stack.empty() returns Boolean Stack의 top 위치에 데이터를 쌓는다. Stack.push(data) returns None Stack의 top 위치에 있는 데이터를 삭제하면서 반환한다. Stack.pop() returns data Stack의 top 위치에 있는 데이터를 반환하지만, 삭제하지 않는다. 어떤 데이터가 있는지 just 확인. Stack.peek() returns data 구현 1 : by python list Stack 구조를 Python에 내장 되어 있는 list를 container 로 삼아 구현하게 되면, 그 구현은 매우매우 쉽다. Python의 List 자료형의 대단함을 그만큼 느낀다. 이 때 List 의 index가 큰 쪽이 top 방향이다.12345678910111213141516171819202122class Stack: def __init__(self): self.container=list() def empty(self): # ADT 를 따라서, 비어 있으면 return True 비어있지 않으면 return False if not self.container: return True return False def push(self, data): # list 를 활용했으므로, list.append(data)를 활용할 수 있다. self.container.append(data) def pop(self): # pop 역시 list.pop()을 활용할 수 있다. return self.container.pop() def peek(self): # peek은 단지 top 에 어떤 데이터가 있는지 확인 하는 것 뿐이므로, 현재 리스트의 가장 마지막에 append 되어 있는 data를 확인하면 된다. return self.container[-1] 구현 2 :","link":"/2018/10/22/Stack/"},{"title":"Thread","text":"Thread 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 파이썬 기본 : Single Thread (Main Thread) threading 모듈 : main thread에서 subthread 를 생성하여 진행하는 방식 multiprocessing 모듈 : double cpu ThreadPoolExecutor : API - 멀티스레드와 멀티프로세스 동일한 형태로 디자인(Pool 클래스만 변경하면됨) threading.Thread() arguement12345Thread(group=, target=, args= , kwargs=, *, daemon=None)#target= : 실제 스레드에서 돌아가게 될 함수#args= : tuple 로 target 함수에 들어가게될 argument#kwargs= : dictionary로 target 함수에 들어가게될 argument#daemon : 데몬 스레드로 돌아갈지 여부 12345#Thread 의 메소드start(): #스레드의 실행, self 의 run() 메소드를 호출run(): #스레드가 실제로 수행하게될 작업name : #스레드의 이름threading.locals() : #해당 스레드 내부에서 사용할 로컬 변수 지정","link":"/2018/10/24/Thread/"},{"title":"Xpath","text":"xpath 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 1. xpath란? xpath = XML Path Language XML 문서의 element나 attribute의 value에 접근하기 위한 언어 XML 문서를 해석하기 위한 언어이므로, 기본적으로 path expression 이 바탕이 되어있다. 연산, 문자열 처리를 위한 라이브러리를 내장하고 있다. 2. Location Path element 를 찾으러 가는 것이므로, location을 나타내기 위한 연산자는 다음과 같다. element_name : element_name 과 일치하는 모든 element를 선택한다. / : 가장 처음 쓰는 /는 절대경로를 의미한다. Path 중간에 쓰는 /는 조건에 맞는 바로 다음 하위 엘리먼트를 검색한다. (css selector에서 &gt;와 같다) // : 가장 상위 엘리먼트 . : 현재 엘리먼트 * : 조건에 맞는 전체 하위 엘리먼트를 검색한다(css selector 에서 한칸 띄우는 것 과 같다) element[조건] : 엘리먼트에 조건에 해당되는 것을 검색한다 (예) p[2] : p 엘리먼트 중 두번째 엘리먼트를 선택 *주의:1부터 시작, 0이 아님 * (예) [@(attribute_key=&quot;attribute_value&quot;)] : 속성값으로 엘리먼트를 선택[@id=&quot;main&quot;] : “main” 이라는 id 를 선택[@class=&quot;pp&quot;] : “pp” 라는 class 를 선택 not(조건) : 조건이 아닌 엘리먼트를 찾는다. /text() : 해당 엘리먼트의 text 데이터를 가져온다 /@attribute_name : 해당 엘리먼트의 attribute_name 에 할당된 value 값을 가져옴 /@href : 해당 엘리먼트의 href의 value 값을 가져옴","link":"/2018/10/26/Xpath/"},{"title":"[GCP] Computing Engine 환경설정","text":"Computing Engine 환경설정 이번 글은, Google Cloud Platform의 Computing Engine 인스턴스의 기본적인 환경설정 방법을 소개하는 글입니다. 직접 작업환경을 세팅하면서, 정리 하는 목적으로 작성하는 글입니다. 다음과 같은 환경을 설정합니다. 인스턴스 생성 및 네트워크 설정 접속을 위한 ssh 생성 및 접속 python3, pip3 설치 CUDA 설치 cuDNN 설치 Pytorch 설치 Jupyter 설치 및 환경설정 1. 인스턴스 생성 및 네트워크 설정Compute Engine에서 자신의 목적에 맞는 리소스를 정해, 인스턴스를 생성해줍니다. Jupyter notebook 을 사용하기 위해, VPC 네트워크 → 방화벽 규칙 탭에서 방화벽 규칙을 만들어 줍니다. 후에 Tensorboard 와 다른 기타 환경들을 사용할 때 그에 맞는 포트 규칙을 동일한 방법으로 열어주면 됩니다. 아래의 4가지를 설정해주고 ‘만들기’ 클릭 이름 : jupyter 소스 IP 범위: 0.0.0.0/0 대상 태그: jupyter tcp: 8888 http, https: 체크 2. 접속을 위한 RSA key pair 생성 및 ssh 접속 gcp 에서 제공하는 gcloud로도 접속 할 수 있습니다. 위에서 생성한 인스턴스에 접속하기 위해, RSA key pair 를 이를 통해 접속 해 봅니다. 아래의 명령어를 이용해 키페어를 생성해 줍니다. 이 때, USERNAME 은 gcp에 등록한 이메일로 설정합니다. $ ssh-keygen -t rsa -f ~/.ssh/[KEYFILE_NAME] -C “[USERNAME]” example) $ ssh-keygen -t rsa -f ~/.ssh/gcp-key -C “myemail@mail.com“ 앞으로 사용할 Password 를 입력하고, 생성된 키페어는 .ssh 폴더 안에서 확인 할 수 있습니다. .ssh/[KEYFILE_NAME].pub 를 확인해 볼 수 있습니다. 생성한 키페어를 gcp 의 메타데이터 탭 → SSH 키에 등록합니다. ssh 접속 $ ssh -i ~/.ssh/[KEYFILE_NAME] [USERNAME]@[GCP외부IP] 3. Python3, PIP 설치위에서 서버에 접속했다면, 서버의 개발환경을 설정해 주기만 하면 됩니다. python3 와 pip 부터 설치해 봅니다. locale 설정123456$ sudo apt install language-pack-ko$ sudo locale-gen ko_KR.UTF-8$ export LC_ALL=\"en_US.UTF-8\"$ export LC_CTYPE=\"en_US.UTF-8\"$ sudo dpkg-reconfigure locales en_US.UTF-8이 [*] 로 체크 되어 있는지까지 확인합니다. Python3, PIP 설치12$ sudo apt update$ sudo apt install python3-pip 설치 후, python3 --version 으로 python 이 잘 설치 되었는지 확인합니다. 4. CUDA 설치우리가 가장 원하는 리소스인 gpu를 활용한 연산을 위해 CUDA 를 설치 해 줍니다. 먼저, 설치 파일을 다운로드 해줍니다. 12// 루트에서$ wget [http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/](https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64) ls 로 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb 이 잘 다운로드 되었는지 확인 해 줍니다. 다운로드 결과 .deb 확장자가 되어있지 않다면, 파일명에 .deb 를 뒤에 붙여 줍니다. mv cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb 설치1234$ sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb$ sudo apt add /var/cuda-repo-&lt;version&gt;/7fa2af80.pub$ sudo apt update$ sudo apt install cuda CUDA 가 정상적으로 설치되었다면, $ nvidia-smi 를 통해 자신의 gpu 상태를 확인 할 수 있습니다. 또한, /usr/local/cuda/version.txt 에 설치한 CUDA, 현재는 10.0의 version 을 확인 할 수 있습니다. 5. cuDNN 설치다음의 명령어를 통해 cuDNN 을 설치 할 수 있습니다. 123$ sudo sh -c 'echo \"deb [http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64](http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/) /\" &gt;&gt; /etc/apt/sources.list.d/cuda.list'$ sudo apt update$ sudo apt install libcudnn7-dev 6. Pytorch 설치 우리는 서버가 https 프로토콜을 사용한다고 체크했으므로 -H flag 를 주어 sudo pip3 를 활용해야합니다.12$ sudo pip3 -H install [https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl](https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl)$ sudo pip3 -H install [https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl](https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl) 7. Jupyter Notebook 설치 및 환경설정 Jupyter를 설치합니다.1$ sudo pip3 -H install jupyter 설치 후, $ jupyter notebook 으로 주피터 커널이 켜지는지 확인합니다. 주피터 config 파일을 생성합니다. 1$ jupyter notebook --generate-config 비밀번호를 생성합니다. 이 비밀번호는 지금 설치한 주피터 환경에 들어가기 위한 비밀번호 입니다. 12345$ ipythonfrom notebook.auth import passwdpasswd()Enter Password: 사용할 비밀번호 입력Verify Password: 사용할 비밀번호 입력 출력된 비밀번호 해쉬 sha1: ~~~ 를 복사해 둡니다. 우리의 인스턴스는 https 프로토콜을 사용하므로, SSL 키파일을 생성해야 합니다.1$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout cert.pem -out cert.pem 위 명령어를 입력하고, 뒤따라 나오는 정보들을 입력하여, .pem 파일을 생성합니다. 에서 생성한 config 파일 수정1$ vi /home/[USERNAME]/.jupyter/jupyter_notebook_config.py 123456// config 파일에 다음의 내용을 추가합니다.c = get_config()c.NotebookApp.ip = &apos;내부아이피주소&apos;c.NotebookApp.open_browser=Falsec.NotebookApp.password=&apos;3에서 생성한 비밀번호 해쉬 sha1:~&apos;c.Notebook.certfile=&apos;4에서 생성한 .pem 파일 경로 /home/USERNAME/cert.pem&apos; 위까지 완료하게 되었으면, 주피터 서버를 열고, https://외부아이피:8888 로 주피터를 접속 하는 것을 확인하시면 되겠습니다.","link":"/2019/06/17/gcp-setting/"},{"title":"math_cheatsheet","text":"이산확률분포베르누이 분포 (bernoulli distribution) 동전 생각하기 결과가 두가지 중 하나 확률변수를 1 or 0 으루 주었을 때,$$\\text{Bern}(x;\\mu) = \\mu^x(1-\\mu)^{(1-x)}$$ 확률변수를 1 or -1 로 주었을 때,$$\\text{Bern}(x; \\mu) = \\mu^{(1+x)/2} (1-\\mu)^{(1-x)/2}$$ 1234#예제코드#베르누이 분포 SciPy#100개 samplingsp.stats.bernoulli(mu).rvs(100, random_state=0) 이항분포 (binomial distribution) 동전을 N번 던졌다 성공확률이 $\\mu$ 인 베르누이 시도를 $N$ 번 반복 할 때, 성공 횟수를 $X$ 라 하면, $$ X \\sim \\text{Bin}(x;N,\\mu) $$ $$ \\text{Bin}(x;N,\\mu) = \\binom N x \\mu^x(1-\\mu)^{N-x} $$ 1sp.stats.binom(N, mu).rvs(100, random_state=0) 카테고리 분포 (Categorical distribution) 주사위 생각해! $K$ 개의 카테고리가 있을 때,$$ x = (x_1, x_2, x_3, x_4, … , x_k) $$ 각 원소 x_i 가 1이 나올 수 있는 확률 ($i$번쨰 원소의 성공확률)을 $\\mu_i$ 라 하면, $$ \\text{Cat}(x;\\mu) = \\mu_1^{x_1} \\mu_2^{x_2} \\cdots \\mu_K^{x_K} = \\prod_{k=1}^K \\mu_k^{x_k} $$ 이 표현은 One-Hot-Encoding 으로 카테고리 분포를 표현 했기에 가능하다. 단, $$ 0 \\leq \\mu_i \\leq 1 $$ $$ \\sum_{k=1}^K \\mu_k = 1 $$ 123#카테고리 분포는 SciPy 의 메소드가 없으므로, 다항분포의 N을 1로 준다.#mu 는 각 term 이 나올 수 있는 확률 vectorsp.stats.multinomial(1, mu) 다항 분포 (Multinomial distribution) 주사위를 여러번 던졌다. 카테고리 시도를 $N$ 번 반복하여 $k$ $(k=1,…,K)$ 가 $x_k$ 번 나올 확률분포. 123#N : number of trial#mu 는 각 term 이 나올 수 있는 확률 vectorsp.stats.multinomial(N, mu)","link":"/2018/11/23/math-cheatsheet/"},{"title":"numpy_cheatsheet","text":"np.arange(start, end) start 에서 end-1 까지의 행렬(array) return np.linspace(start, end, number_of_dots) number_of_dots : 범위 안의 점 갯수 np.zeros_like(x) 0으로 채운 x 와 같은 크기의 행렬","link":"/2018/11/23/numpy-cheatsheet/"}],"tags":[{"name":"TWIL","slug":"TWIL","link":"/tags/TWIL/"},{"name":"CS231n","slug":"CS231n","link":"/tags/CS231n/"},{"name":"summary","slug":"summary","link":"/tags/summary/"},{"name":"Classification, 확률론적생성모형, Generative, 생성모형, 분류모델","slug":"Classification-확률론적생성모형-Generative-생성모형-분류모델","link":"/tags/Classification-확률론적생성모형-Generative-생성모형-분류모델/"},{"name":"mysql, MYSQL, database","slug":"mysql-MYSQL-database","link":"/tags/mysql-MYSQL-database/"},{"name":"nginx, proxy","slug":"nginx-proxy","link":"/tags/nginx-proxy/"},{"name":"pillow, Pillow, image","slug":"pillow-Pillow-image","link":"/tags/pillow-Pillow-image/"},{"name":"datastructure","slug":"datastructure","link":"/tags/datastructure/"},{"name":"Crawling, Web, Requests","slug":"Crawling-Web-Requests","link":"/tags/Crawling-Web-Requests/"},{"name":"scrapy, crawling, web","slug":"scrapy-crawling-web","link":"/tags/scrapy-crawling-web/"},{"name":"자동화, selenium, webdriver","slug":"자동화-selenium-webdriver","link":"/tags/자동화-selenium-webdriver/"},{"name":"datastructure, stack","slug":"datastructure-stack","link":"/tags/datastructure-stack/"},{"name":"Thread","slug":"Thread","link":"/tags/Thread/"},{"name":"Google Cloud Platform","slug":"Google-Cloud-Platform","link":"/tags/Google-Cloud-Platform/"}],"categories":[{"name":"MachineLearning","slug":"MachineLearning","link":"/categories/MachineLearning/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"},{"name":"Pytorch","slug":"MachineLearning/Pytorch","link":"/categories/MachineLearning/Pytorch/"},{"name":"Lecture","slug":"Lecture","link":"/categories/Lecture/"},{"name":"CS231n","slug":"Lecture/CS231n","link":"/categories/Lecture/CS231n/"},{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"딥러닝을 이용한 자연어 처리","slug":"Lecture/딥러닝을-이용한-자연어-처리","link":"/categories/Lecture/딥러닝을-이용한-자연어-처리/"},{"name":"wiki","slug":"wiki","link":"/categories/wiki/"},{"name":"DataStructure","slug":"DataStructure","link":"/categories/DataStructure/"},{"name":"Apache Spark","slug":"MachineLearning/Apache-Spark","link":"/categories/MachineLearning/Apache-Spark/"},{"name":"Google Cloud Platform","slug":"Google-Cloud-Platform","link":"/categories/Google-Cloud-Platform/"},{"name":"MySQL","slug":"wiki/MySQL","link":"/categories/wiki/MySQL/"},{"name":"NGINX","slug":"wiki/NGINX","link":"/categories/wiki/NGINX/"},{"name":"Pillow","slug":"wiki/Pillow","link":"/categories/wiki/Pillow/"},{"name":"Provision","slug":"wiki/Provision","link":"/categories/wiki/Provision/"},{"name":"Requests","slug":"wiki/Requests","link":"/categories/wiki/Requests/"},{"name":"Scrapy","slug":"wiki/Scrapy","link":"/categories/wiki/Scrapy/"},{"name":"Selenium","slug":"wiki/Selenium","link":"/categories/wiki/Selenium/"},{"name":"Python","slug":"wiki/Python","link":"/categories/wiki/Python/"},{"name":"Xpath","slug":"wiki/Xpath","link":"/categories/wiki/Xpath/"}]}