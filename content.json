{"pages":[{"title":"Categories","text":"","link":"/Categories/index.html"},{"title":"about","text":"since 2018년 10월,데이터 사이언스를 공부하고 있습니다.","link":"/about/index.html"},{"title":"all-archives","text":"","link":"/all-archives/index.html"},{"title":"all-categories","text":"","link":"/all-categories/index.html"},{"title":"all-tags","text":"","link":"/all-tags/index.html"}],"posts":[{"title":"181018_DailyScrum","text":"181018오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다.","link":"/2018/10/18/181018-TodayWhatILearned/"},{"title":"181022 TodayWhatILearned","text":"181022 TWIL오늘 할 일은 무엇인가 블로그 테마 바꾸기 Selenium 과제 두번째 것 (17:47 - 19:00) 확률 분포 다시 한번 정리하기 데이터구조 (PseudoLinkedList, Stack, Queue)정리 새 맥북 환경 설정 조금씩 마저 더하기 DailyScrum Upload 오늘 한 일은 무엇인가 Selenium NBA Page Crawling 해서, DataFrame으로 정리 (17:47 - 18:55) Stack 정리 Github 설정 내일 할 일은 무엇인가 확률 분포 공부 Queue 정리 무엇을 느꼈는가 맥북을 바꾸면서, 환경 설정하는데 너무 많은 시간이 든다. 틈틈히 하고 있는데도 아직 이전 맥북의 환경에서 100% 똑같은 환경을 만들지 못하고 있다. 장비를 바꿀 때, 혹은 다른 작업 환경에서 연속성을 이어 나가기 위해 나에게 맞는 환경설정도 정리할 필요성을 느낀다. 오늘 공부한 PseudoLinkedList, Stack, Queue 의 활용성에 대해 깊이 고민 해 볼 수 있어서 뿌듯하다. 특히나 PseudoLinkedList의 경우 내가 사용하지 않았던 자료구조나 class를 목적에 맞게 customize 할 수 있는 측면에서 활용성이 높다고 생각된다. 다른 사람이 작성한 class 를 잘 읽어보고, 그 활용도를 높이기 위해 overriding 하는 방법을 틈틈히 챙겨 봐야겠다.","link":"/2018/10/22/181022-TodayWhatILearned/"},{"title":"181023 TodayWhatILearned","text":"181023 TWIL오늘 할 일은 무엇인가 Queue 정리 하기 확률 분포 공부 &lt;스터디&gt;멀티 스레딩 개념을 포함한 네이버 크롤링 코드짜기 오늘 한 일은 무엇인가 &lt;스터디&gt; 멀티 스레딩에 관해 이야기, 공부해볼 정보들 공유, 어떻게 코드를 구성할 것인지 의견을 나누었다. Queue 개념 정리 및 코드 구현 연습 베르누이 분포, 이항분포 정리 및 Searching 블로그 수정 내일 할 일은 무엇인가 오후 4시 멀티 스레드 관련한 내용으로 온라인 스터디 모임 예정 멀티스레딩 개념을 활용한 네이버 크롤링 코드 짜기 각종 확률 분포 및 검정 부분까지 복습 무엇을 느꼈는가 오늘 공부한 testing 시작부분부터는 그간 배웠던 것을 한꺼번에 적용한다. 배웠던 여러가지 분포를 활용하여,가설을 세우고 이 가설이 선택한 분포의 관점에서 봤을 떄, 가설을 선택할지 기각할지에 관한 내용은 점차data를 통해 prediction 을 해가는 과정에 있는 듯한 느낌을 받았다. 아직 검정 과정과 앞으로 공부하게 될 여러가지 분석 모델을 이해하기에 앞서배운 확률분포 부분의 내용이부족한듯하여, 계속 꾸준히 앞부분을 공부해야될 것 같다.","link":"/2018/10/23/181023-TodayWhatILearned/"},{"title":"181024 TodayWhatILearned","text":"181024 TWIL오늘 할 일은 무엇인가 멀티스레딩으로 네이버 크롤링 코드 작성 오후 4시 스터디 확률 분포 공부 오늘 한 일은 무엇인가 멀티스레딩 개념을 활용한 크롤링 코드 작성 오후 4시 스터디 확률분포 공부 내일 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Pandas 정리해보기 무엇을 느꼈는가 Documentation 과 각종 개념 자료들을 혼자 보고 공부하면서, 파이썬이라는 언어가 단순히 책 한권 끝냈다고 해서 끝나는 언어가 아님을 깨달았다. 기본적인 문법을 금방 익숙해져서, ‘역시 쉬운 언어인가’라고 생각했다가 오늘 다양한 자료를 찾아보고 읽어보면서 언어 하나만해도 아직 공부할게 무궁무진 하다는 것을 깨달았다.","link":"/2018/10/24/181024-TodayWhatILearned/"},{"title":"181026_TodayWhatILearned","text":"181026 TWIL오늘 한 일은 무엇인가 Xpath, Scrapy 를 활용한 Crawling 공부 DataScience 와 DataEngineering 의 차이점 찾아보기 내일 할 일은 무엇인가 확률분포 공부 Scrapy 활용해보기 무엇을 느꼈는가 DataScience 와 DataEngineering 의 공통점과 차이점에 대해 알아보았는데, 아직은 현업에서 각분야가 하고 있는 일이 어떻게 다른지 확실한 감이 오지 않는 것 같다. 내가 명확히 재밌어하는 것은 아직까지는어떤 디테일한 분야가 아니라, 수학적인 것, 프로그래밍 적인 것들을 배운 것을 적용해보고 응용하는 것에흥미를 느끼는 것 같은데 이보다 더 앞서 생각해보고 결정하려고 하니 감이 잘 오지 않는 것 같다. 지금 현재로서는, 다양한 기계학습 알고리즘과 머신러닝 등을 이용하여 prediction 등을 통해 새로운 Insight를 얻어내는 것에 관심이 있는 것 같다. 이 분야를 공부하고 배워 나가면서 엔지니어링 분야를 필요에 의해 점차공부해 나가는 방향으로 삼고 싶다.","link":"/2018/10/26/181026-TodayWhatILearned/"},{"title":"181019 TodayWhatILearned","text":"181019오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다. 181019오늘 한 일 Web Crawling 공부 : Selenium을 활용한 크롤링 Pandas 활용하여 여러가지 data import 하고, 정렬 바꾸는 연습 Terminal 환경설정 zsh, oh-my-zsh 설치했다. git을 적극적으로 활용할 계획이므로, git사용에 유리한 zsh 설정을 마쳤다. 팀 프로젝트에 활용할 데이터 셋 탐색 github 블로그 Deploy 에 성공했다. Daily 스크럼을 작성하는 것이 하루의 계획과 정리를 하는데 유용하기에, 꾸준히 작성하면서, github 블로그에도 올려볼 계획이다. 내일 할 일 Pandas 내용 정리, 반복 숙달 Naver Article Crawling, NBA Data Crawling 확률과 통계 정리 (연속확률분포 부분) 뭘 느꼈는가 점차 학습하는 내용과 알아야 될 내용이 많아지면서, 내가 배운 것들, 알고 있는 것들. 정확히는 어떤 것에 관해 존재는 알고 있으나 내 머릿속에서 당장 꺼내서 쓰기에는 어려운 것들이 많아지고 있다. 또한, 내 머릿속에서 꺼내기 쓰기 힘들어 구글을 통해 찾고자하면 내가 기억했던, 알고 있는 정보들과 조금은 내용이 다르고, 이것을 Searching 하는데 쓰는 시간이 아깝다는 생각이 들었다. 앞으로는 매일 학습하는 내용을 바탕으로, 직접 찾기 쉬운 형태로 정리할 필요성을 느꼈다. 개인. WIKI화.","link":"/2018/10/19/181019_TodayWhatILearned/"},{"title":"181025_TodayWhatILearned","text":"181025 TWIL오늘 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Selenium, Scrapy, Pillow 활용 공부 오늘 한 일은 무엇인가 Selenium, Pillow 활용 공부 내일 할 일은 무엇인가 확률분포 공부 무엇을 느꼈는가 동영상과 이미지를 처리하는 방법에 대해 공부하면서, Selenium 을 활용해 많은 활용도를 느꼈다.주말을 이용해 동영상이나 이미지들을 Crawling 해서 class 형태로 만드는 방법을 시도해봐야겠다.","link":"/2018/10/25/181025-TodayWhatILearned/"},{"title":"181028-TodayWhatILearned","text":"181028 TWIL 오늘 한 일은 무엇인가 검정과 추정 공부 Blog 테마 수정 내일 할 일은 무엇인가 ‘추정’에서 수식 부분 다시 보기 (Study) 데이터 구조 강의 듣기 무엇을 느꼈는가 MaximumLikelihood 를 실제로 손으로 써가며 풀어보는 과정에서, 지금까지 배웠던 수학적 테크닉들이 모두 쓰이는 것을 보고 뿌듯하면서도 재미있었다. 뿌듯한 이유는 아마도 한줄 한줄 써가는 내용이 여태 공부한 수학 개념들로 이루어진것 때문일 것이다. 그 중에서도 다음 식을 전개하는 과정에서 눈에 잘 들어오지 않아 전개하지 못할 때도 있었다. 이것은 아마 앞부분 개념이 그 순간에 적용이 되지 않기 때문이라고 생각된다. 행렬의 내용중 몇가지 특성들과 라그랑주 멀티플라이어에 대한 수식을 틈이 생길떄 챙겨서 봐야겠다. 오늘은 수학을 공부하느라 프로그래밍은 하지 못했다. 중간에 졸린 걸 해소해보고자 블로그 테마 색깔 수정과 그 수정을 위해 블로그의 코드 구조를 본게 전부 였다. 30분 푹빠져서 하다가 주객전도 되지않으려 다음으로 미뤘다. 위 주제들에 관해 알게된 것도 많은 하루였고, 뿌듯한 하루였다.","link":"/2018/10/28/181028-TodayWhatILearned/"},{"title":"181027-TodayWhatILearned","text":"181027 TWIL오늘 한 일은 무엇인가 확률 수학 공부 자기전 (스터디)데이터구조 부분 1강 듣기내일 할 일은 무엇인가 확률분포를 다시 복습하면서 검정, 추정에서 이어지는 부분 꼼꼼히 공부 데이터 구조 강의 수강 계획 세우기 무엇을 느꼈는가 검정방법론에 대해 좀더 꼼꼼히 보았다. 수학적인 수식들은 수업시간에 다 이해가 되는 편이지만 이 수식들을말로써 표현하고, 글로 풀어쓰는 순간 머리가 빠릿빠릿 안돌아가는 느낌이어서, 한단계한단계 논리적으로 따져가며공부하니 이제야 좀 편해진것 같다. 가설 검정 같은 때에도, 말로 풀어쓰는 것 보다 간단하게 수식으로 표현하고,각각의 p-value 를 확인한뒤 원래 작성했던 H_0, H_a 에 대해 생각해보면 쉽게 되었으나 이것을 말로 표현하고글로 구성하려고 하니 간단한것도 복잡하게 생각했던 것 같다. 결국 내가 알게 된것을 상대방과 논의하고앞으로 만나게 될 클라이언트들을 대상으로 설명해야하는 것이 모두 이런 부분에서 시작되는 것임을 느꼈기에,내 생각과 가정 -&gt; 수식으로 표현 -&gt; 다시 말 혹은 글로 표현 하는 것을 습관처럼 해야겠다.","link":"/2018/10/27/181027-TodayWhatILearned/"},{"title":"181029-TodayWhatILearned","text":"181029 TWIL 오늘 한 일은 무엇인가 자료구조 Binary Tree, Stack 으로 Queue구현, Stack 응용 추정 부분 복습 스터디 나갈 방향 이야기 A star algorithm search 내일 할 일은 무엇인가 ‘추정’에서 수식 부분 다시 보기 무엇을 느꼈는가 스터디 첫 모임을 가졌다. 프로그래밍에 관해 알고있는 것들을 활용하고 응용하는 쪽으로 어떻게 하면 될지 많은 의견을 나누었다. 상상의 나래를 펼치니, 타오르는 열정과 함께 만들어보고 싶은 것은 많았으나…. 아직 아는게 많지 않기에.. 다시 현실에 눈을 돌렸다. Maze 문제에서 다른 알고리즘을 통해 구현해보려고 의견이 모아졌다. 직접 찾아서 적용해보는 첫 algorithm 이기에, 직접 찾아가면서 공부하고, 이것을 적용하는 경험을 통해 또 다른 배울 것이 있을 것 같아 매우 기대된다.","link":"/2018/10/29/181029-TodayWhatILearned/"},{"title":"181030-TodayWhatILearned","text":"181030 TWIL 오늘 한 일은 무엇인가 검정, 추정 부분 수식 꼼꼼히 다시 보기 A star(A*) Algorithm 개념 읽기 내일 할 일은 무엇인가 선형회귀분석 정리 (패키지 별로 특징, 메서드 파라미터 위주) 데이터 전처리 부분 공부 Test data 처리는 어떻게 하는 건지 공부해보기 무엇을 느꼈는가 추정부분 수식을 공부하면서 eigenvalue 부분이 기억이 잘 안나던 것을 시간이 될 때 자료들을 챙겨봐야겠다. 오늘 공부한 선형 회귀분석을 배우니 조금이나마 프로젝트를 어떻게 진행해야되는 건지 감이 잡힌 것 같다. 미약한시작을 하기위해 오늘 공부한 개념들을 사용해서 여러가지 돌려보고 데이터를 파악해볼 수 있을 것 같다.하지만, 우리가 가지고 있는 데이터셋이 바로 적용 할 수 없는 현실적인 데이터이기 때문에, 여러가지 전처리작업이 필요할 것 같다. 오늘 배운 것을 적용해보기 위해 내일은 데이터 전처리를 공부해보고, 또한 test data의생성 역시 공부해보아야할 부분인 것 같다.","link":"/2018/10/30/181030-TodayWhatILearned/"},{"title":"181031-TodayWhatILearned","text":"181031 TWIL 오늘 한 일은 무엇인가 선형회귀분석 공부 행렬의 미분 공부 (PROJECTmini)A star Algorithm 개념 정리 (PROJECT)프로젝트 진행 data 탐구 데이터 전처리 공부 내일 할 일은 무엇인가 Project 모임 데이터 전처리, 데이터들의 의미 파악, 예측할 delay 부분 정의하기 무엇을 느꼈는가 오늘은 아침부터 하루종일 컴퓨터 앞에 앉아 있어서, 눈과 목이 너무 아프다. 그래도 새로운 알고리즘 내용에관한 이해와, 앞으로 진행할 메인 프로젝트의 data들의 의미를 파악해서 뿌듯한 하루였다. Data의 의미를파악했으나 이것을 처리하기 위해서 numpy 와 pandas 의 documentation 을 읽으면서 데이터들을 다뤘다.라이브러리와 패키지들의 메소드와 클래스들을 자유자재로 다루기 위해선, 계속 사용해보면서 익혀야 함을 느꼈다.메소드들을 알면 복잡하게 코드를 안짜도 이미 내장되어있는 메소드로 손쉽게 처리할 수 있기 때문이다. data를 혼자 곰곰히 보다, 시간에 관한 column 의 의미를 파악했으나 이것을 손쉽게 합쳐서 데이터들을재정렬하는데 오늘 실패했다. 의미는 파악했으니, 내일 좀더 시도해보면 시간에 관한 data 를 정리할 수 있지않을까 한다.","link":"/2018/10/31/181031-TodayWhatILearned/"},{"title":"181101-TodayWhatILearned","text":"181101 TWIL 오늘 한 일은 무엇인가 Project 모임 Data 일부 전처리 Crawling 공부 내일 할 일은 무엇인가 Database - MySQL 공부 Pandas 라이브러리 살펴보기 miniProject Crawling 주제 선정, 코드짜기 무엇을 느꼈는가 프로젝트 모임에서 우리가 예측할 Data (Departure Delay)를 정의하고, 나는 오늘 시각 data 의 전처리를 하였다. Data 들이 의미없는 값들을 가지고 있을 때 어떻게 처리해야 할지 고민하다가 확실한 답은 못얻은 채, 우선 시각 데이터의 formatting 만 바꾸었다. data들을 어떻게 채워넣어주어야 할지는 조금더 공부해 보아야 할 것 같다. 첫 프로젝트 모임의 느낌이 매우 좋았다. 모임을 하기 전까지는 내가 했던 것들이 맞는가 하는 의구심과 project를 하기에는 아직 부족한 지식과 실력이라는 걱정이 앞섰다. 오늘 모임을 하면서 각자 살펴보았던 data 의 특징들과 앞으로 어떻게 data 를 다듬을 것인지 얘기하면서, 더 좋은 방향과 몰랐던 것들, 알았던 것들을 서로 나누면서 발전적인 대화가 되었다는게 매우 뿌듯하다. 모임에서 받은 좋은 느낌을 이어서, 오늘 내가 맡기로한 부분을 해결하기 위해 앉았고, 또 나름 해결한 부분이 있는 것 같아 작은 성취감을 맛보았다. 협업을 하기 위해 git에 관해 좀더 공부하고 나눌 필요성이 느껴졌다. 특히나 code conflict 가 실제로 발생하기 전에 어떻게 다루어야 하는 것인지 좀더 깊게 공부할 필요성을 느꼈다.","link":"/2018/11/01/181101-TodayWhatILearned/"},{"title":"181102-TodayWhatILearned","text":"181102 TWIL 오늘 한 일은 무엇인가 Database - MySQL 공부 A * 알고리즘 스터디 NaN 값 처리에관한 자료 서칭 내일 할 일은 무엇인가 Project 모임 Pandas 라이브러리 정리 miniProject Crawling 코드짜기 A * 알고리즘 짜기 무엇을 느꼈는가 오늘 스터디에서 A* 알고리즘를 주제로 얘기를 좀더 나누었다. 각자 공부해오신 내용을 바탕으로 알고리즘의흐름이 어떻게 되어가는가 좀더 구체적으로 생각해보고자 했다. 좀더 해결법에 가까워진 느낌을 받았으나,이제는 좀더 구체적으로 구현해보면서 다가오는 문제들을 해결해보고자 했다. 모든걸 완벽하게 이해하고 실현하는것만이 답은 아니기 때문이라고 생각한다. 주말동안은 프로젝트, 미니프로젝트, 알고리즘 적용 코딩, 수학 공부, DB공부.. 산더미지만 하나씩 그어나가야겠다.","link":"/2018/11/02/181102-TodayWhatILearned/"},{"title":"181103-TodayWhatILearned","text":"181103 TWIL 오늘 한 일은 무엇인가 Crawling miniProject 코드 작성 Pandas 라이브러리 정리 Project 주말동안 할일 정하기 내일 할 일은 무엇인가 Project Data 회기 돌려보기 회기 부분 공부 크롤링 프로젝트 개선 사항 고민 (시간이 되면) A* 알고리즘 코드 짜보기 무엇을 느꼈는가 whoscored 사이트의 Player 정보를 크롤링 하는 miniProject 의 코드를 작성하였다. Selenium을통해서 크롤링하는데, headless 적용을 하면 에러가 나는 부분을 어떻게 처리해야할지 고민해보아야 한다.Chrome webdriver 를 열어놓는 환경과 headless 환경의 차이가 있는 것 같은데, 이 부분은 document를 살펴보아야 할 것같다. window창을 열어놓는것 과 그렇지 않은 것의 차이점이 있는지 확인해야한다. 함수로 짠 코드를 class 화 시킬 때는 다루는 범위가 커져야 한다는 강박관념이 있다. class 화 시켰을때의 편의성 부분을 고민하면서 위 생각으로 흐름이 이어지는 것 같은데, 그렇지 않기 위해 class의 장점을좀더 체감해볼 필요가 있다. Linear Regression 강의를 들으면서, LineByLine 수식을 이해하는데는 문제가 없으나 이야기의 큰 그림을 놓치는 경향이 있는 것 같다. 내일은 이 부분을 중점적으로 공부해보고, 메인 프로젝트에 적용해보는 것 까지해봐야겠다.","link":"/2018/11/03/181103-TodayWhatILearned/"},{"title":"181104-TodayWhatILearned","text":"181104 TWIL 오늘 한 일은 무엇인가 회기 부분 공부 Project Data 회기 돌려보기 내일 할 일은 무엇인가 Project 모임 회기 부분 정리 크롤링 프로젝트 개선 사항 고민 (시간이 되면) A* 알고리즘 코드 짜보기 무엇을 느꼈는가 다리를 다치고 통증으로 인해 집중해서 코드를 짜기 힘들었다. 수학 이론을 공부하면서 내일 있을 프로젝트모임을 준비했고, 팀원들께서 만든 코드를 정리하며 데이터를 다시 추출하였다. 이 데이터들을 통해 회기의 몇몇가지를 돌려보았다. 이것을 토대로 내일 팀원들과 함께 이야기를 나눠보며 좀더 어떻게 할 수 있을지 고민해봐야겠다.","link":"/2018/11/04/181104-TodayWhatILearned/"},{"title":"181105-TodayWhatILearned","text":"181105 TWIL 프로젝트 STEP 전처리 Nan 값 처리 해결 Partial Regression Plot 그려보기 오늘 한 일은 무엇인가 프로젝트 모임 Crawling miniProject 와 mainProject 진행 내일 할 일은 무엇인가 프로젝트, 오늘까지 진행사항 코드 정리 수학 회기 전체 정리 프로젝트 생각하기 무엇을 느꼈는가 오늘은 프로젝트 모임을 통해서, 많은 것을 얻었다. OLS function 을 실제로 돌림에 있어서 많은 제약조건이필요함을 알게되었고, 그만큼 데이터전처리가 매우 중요하다는 것을 알게 되었다. 다양한 feature 들마다 다양한 방법으로 전처리를 해주어야 한다. 처음엔 NaN값의 처리 방식에 대해 고민했으나,이제는 각 feature 의 특성들마다 전처리 해주는 방식이 달라져야 하고, 또 이번 고비가 넘어가게 되면좋은 prediction 결과를 얻기 위해 다양한 feature 의 조합이 필요함이 피부에 와닿았다. 만족스런 결과물을 얻기 위해선, 아는게 많은 것 보다, 그 결과물을 만들고자 하는 구성원이 중요함을 느꼈다.알고있는 지식은 해결해야할 문제보다 항상 작기 마련이다. 또한 알고있는 지식이 완벽한지는 계속 스스로 의문을던지며 업데이트 해야만한다. 하지만 이보다 더 중요한 것은 문제에 부딪힐 때마다 의욕적이고, 해결해보고자 하는 팀원들덕분에 오늘의 보람과 뿌듯함을 얻을 수 있었던 것 같다.","link":"/2018/11/05/181105-TodayWhatILearned/"},{"title":"181107-TodayWhatILearned","text":"181107 TWIL 오늘 한 일은 무엇인가 Crawling Project 정리 및 제출 내일 할 일은 무엇인가 Prediction Project","link":"/2018/11/07/181107-TodayWhatILearned/"},{"title":"181108-TodayWhatILearned","text":"181108 TWIL 오늘 한 일은 무엇인가 Project Data EDA, OLS 돌려보기 내일 할 일은 무엇인가 Project Data EDA, OLS 돌려보기 무엇을 느꼈는가 프로젝트 과정에서 Performance 가 안나오는 이유에 대해 알게된 계기였다. EDA 는 계속 하더라도 부족함이 많은것이고,데이터를 처리할 때 line by line 근거가 있어야 한다고 느꼈다. 처음부터 전 과정을 진행하는데 있어 매우 시행착오가 많았고, 또 앞으로도 많을 것이지만 계속 반복해서 시행해보는 것이 중요할 것 같다.","link":"/2018/11/08/181108-TodayWhatILearned/"},{"title":"181109-TodayWhatILearned","text":"181109 TWIL 오늘 한 일은 무엇인가 Crawling miniproject 발표 Prediction main project data 탐색 내일 할 일은 무엇인가 Project 모임 무엇을 느꼈는가 Crawling miniproject 의 troubleshooting 시간이 있었다. 발표를 끝내고 나서는 하나를 마쳤다는 시원한 감이 있었지만, comment 들을 듣고 많이 부족함을 느낀 시간이었다. 코드를 작성할 때, 공통된 요소의 추상화가 선택적이 아니라 필수적임을 알게되었다. 간단한 것이라도 반복적으로 작성하는 것은 python convention 에도, 또 프로그래밍의 근본적인 취지에도 어긋나는 것이다. 처음부터 일반화된 포맷으로 작성하기엔 아직 실력이 많이 부족하다. 간단한 구현에서 module 화까지 내가 짜는 대부분의 코드를 연습하다 보면 늘겠지… 메인 프로젝트의 data 간의 연관성이 잘 보이지 않는다. 지난 시간에 알게된 하루 주기로의 delay가 반복됨을 알고 있었으나, datetime 형식으로, 혹은 int나 float 형식으로 ols 에 집어 넣으면 주기성을 잡아내지 못하는 것 같다. 시간 단위로 X feature 로 들어가면 좋을 것 같은데, ols formula 에 시간을 집어넣으면 계속 category 화 된다. 이렇기 때문에 그 해당 정확한 시간이 있지 않으면 coefficient 가 먹지 않지…. 시간을 표현하기 위해 60진법도 찾아보고, 1분 단위로 int 숫자에 mapping 할까도 생각해보았으나, 결국 mapping 해서 ols 에 돌리면 2400 이후 값은 없는데도 x 축에 들어가게 된다….","link":"/2018/11/09/181109-TodayWhatILearned/"},{"title":"181110-TodayWhatILearned","text":"181110 TWIL 오늘 한 일은 무엇인가 Prediction main project data 탐색 내일 할 일은 무엇인가 Project 모임 무엇을 느꼈는가 어제 고민했던, 시간을 사용하여 Regression 을 돌리는데는 성공하였다. 단위를 바꿔주는 방법으로 60진법도 생각하고, radian 으로 바꾸는 방법도 생각해보다가 epoch 방법으로 기준시점에서 지난 초 수 로 scale 과 unit 을 바꿔준디 돌렸더니, 돌아는간다… 문제는 전혀 예측했던 모형이 아니었다. scatter plot 의 외형만 보고 판단했다는 것을 마지막에 깨달은 것 같다. 모델을 하기위해 묶어보고 새로운 feature 라고 생각되는 것도 빼보지만 아직 경향성이나 공통적인 특정 같은 것은 보이지 않는다.","link":"/2018/11/10/181110-TodayWhatILearned/"},{"title":"181111-TodayWhatILearned","text":"181111 TWIL 오늘 한 일은 무엇인가 Project 모임 내일 할 일은 무엇인가 Project_Data 탐색 시계열 분석 공부 무엇을 느꼈는가 어제보다는 약간의 진보가 있었지만, 이에 대한 이유는 명확히 몰라 사실 분석이라기보다 얻어걸린 기분이든다. EDA 를 통해 작성한 모델링에서의 식은 아직까지 완성된 느낌을 받지 못해 답답하고, 방향성을 잘못 접근하고 있는 것인가 하는 느낌을 받기도 했지만, 마지막에 조금 기분이 나아졌다. 내일 오전에는 프로젝트에 밀렸던 시계열 데이터 분석에 관한 기초적인 공부를 다시 하고, 오후에는 프로젝트 데이터를 좀더 살펴보아야겠다.","link":"/2018/11/11/181111-TodayWhatILearned/"},{"title":"181112-TodayWhatILearned","text":"181112 TWIL 오늘 한 일은 무엇인가 프로젝트 발표 및 Feedback 내일 할 일은 무엇인가 프로젝트 모임 &amp; 시계열 공부 무엇을 느꼈는가 메인프로젝트를 다루는 기간동안의 중간결과를 발표하는 시간을 가졌다. 전달력이 충분하지 못한 느낌을 많이 받았다. 내 머릿속에 있는 것을 다른 사람에게 전달하는 능력 역시 많이 필요함을 깨달은 시간이었다. 프로젝트 내용에 있어서는, 아직 부족한것이 많다. 퍼포먼스도 눈에 띄게 나오는 것이 없다. 원인은 아마도 눈에 띄는 데이터들의 경향성을 보지 못한 탓이 아닐까 싶다. 가지고 있는 train 데이터 전체를 두고는 뚜렷한 경향성을 보지 못하고 있다. 여러가지 제한 조건을 두면서 쪼개서 봐야하는 작업이 더 필요한 것 같다.","link":"/2018/11/12/181112-TodayWhatILearned/"},{"title":"181114-TodayWhatILearned","text":"181114 TWIL 오늘 한 일은 무엇인가 flight prediction 내일 할 일은 무엇인가 프로젝트 모임 무엇을 느꼈는가 계속 비행기 delay시간을 예측하는 프로젝트를 진행하고 있다. 여러가지 feature 들을 다루면서 arrival delay 를 예측하는 것을 목표로 삼고 있는데, 가지고 있는 데이터에서 공통적인 특징을 발견할 수가 없다. 외부 데이터를 사용해야할지, 갸지고 있는 데이터 셋에서 새로운 컬럼을 만들어야 할지 모르겠다. 많은 컬럼을 만들고 지우면서, 단순히 아이디어, 혹은 이러지 않을까라는 추측으로 컬럼을 만드는 것이 방법론 적으로 잘못된 것인가하는 의문이 들기도 한다. correlation 이 높은 것을 가지고 OLS 를 돌리는 것 외에 새로운 컬럼을 가지고 만드는 것. 어떻게 다루어야 할지 고민이다. 이제 due date 가 얼마 남지 않은 만큼 최대한 남은 기간 열심히 돌려보고 만들어보면서 계속 trial and error 로 찾아보아야 겠다.","link":"/2018/11/14/181114-TodayWhatILearned/"},{"title":"181119-TodayWhatILearned","text":"181119 TWIL 오늘 한 일은 무엇인가 시계열 공부 MA, AR, ARMA 공부 및 수식 꼼꼼히 전개 스터디 모임 : 프로그래밍 구현계획짜기 내일 할 일은 무엇인가 ARIMA, SARIMA 수식 정리 DataStructure 코드 정리 A* 알고리즘 공부 회기분석 리뷰 무엇을 느꼈는가 할게 산더미인듯한 느낌이지만, 하나씩 그어나가다보면 되겠지라는 마음으로…체력적으로 힘듬이 느껴진다..","link":"/2018/11/19/181119-TodayWhatILearned/"},{"title":"181120-TodayWhatILearned","text":"181120 TWIL 오늘 한 일은 무엇인가 시계열 공부(SARIMA) BST 공부 내일 할 일은 무엇인가 A* 알고리즘 공부 회기분석 리뷰 무엇을 느꼈는가 오늘 시계열을 강의가 마무리 되면서, 가장 중요한 교훈은 ‘세상의 모든 시계열 데이터를 현재 배운 모델로구현할 수 없다’인 것 같다. 회기 분석보다 훨씬더 어려웠던, 수식 전개들이 많았는데 모델링 할 수 있는 실제데이터가 많이 없다는 것이 안타까웠다. 그럼에도 불구하고 이전 프로젝트에서 다루었던 데이터를 ARIMA 등을 적용할 수 있을지 다시한번 프로젝트파일을 열어봐야겠다. 내일은 Projectmini 에서 하기로한 A* 알고리즘을 구현하는 것을 고민해 봐야한다. 프로젝트 기간동안미뤄두었던 것을 하는 것이라, 의지는 충만하지만, 이전에 막혔던 부분에서 다시 어디서부터 봐야할지 복기 하는시간이 오래 걸릴 것 같다.","link":"/2018/11/20/181120-TodayWhatILearned/"},{"title":"181121-TodayWhatILearned","text":"181121 TWIL 오늘 한 일은 무엇인가 A* Alogorithm : Pseudo Code 작성 ~ PACF 공부 내일 할 일은 무엇인가 시계열 모형의 추정 부분 나머지 공부 NLTK, KoNLPY 메소드 정리 무엇을 느꼈는가 A* 알고리즘에 관련된 많은 자료들을 읽어보고, 정리하면서 어떻게 구체적으로 이 알고리즘이 흘러가는지파악했다. 기초적인 알고리즘들은 체계적으로 정리되어있는 자료들이 많지만, 이렇게 응용이 된 알고리즘들은업계와 학계에서 사용하면서 점차 개발됨에 따라 다양하고, 어느 방면에 효율성을 맞추느냐에 따라서 정말 다양하게파생된 것들이 많다. 그 중에서도 가장 간단하고 기본적인 원리가 설명되어있는 documentation 을 찾고,이해하려고 하다보니 시간이 많이 걸린 것 같다. 이렇게 기본적인 이해를하고, 다양한 응용된 문서를 보니, 그 각각이 추구하고 있는 방향도 읽혀지는 것 같다.내일부터는 이렇게 파악된 것을 바탕으로 본격적인 코드 구현을 해보아야할 것 같다. 오늘 잠깐 생각한 바로는,우리가 배웠던 자료구조를 사용하는 것이 효율적인지, 파이썬에 내장되어있는 자료구조를 사용 할 것인지 간에효율을 결정하는데, 어떤 factor 를 기준으로 정해야 할지 모르겠다. 스터디 때 이 부분도 물어보고, 이야기를나눠보면 좋을 것 같다. AR, MA 가 p, q 차로 되었을 때, k에 따른 PACF 함수의 모양을 증명해 보면서, AR, MA 의 ACF,COV general form 을 계속 찾아보면서 증명하게 되었다. 사실은 이 부분까지 다시한번더 증명하고 이를참조하는 방향으로 증명을 써 내려가야하는 것이 맞지만, 이전에 했다는 핑계로 참조했는데, 찜찜하다.증명은 어렵지 않았으나, 외워야 한다는 것이 좀 압박이다.","link":"/2018/11/21/181121-TodayWhatILearned/"},{"title":"181123-TodayWhatILearned","text":"181123 TWIL Today’s To-Do-List (스터디) 스터디 원들과 A* Alogorithm 개념 정립 (스터디) 스터디 원들과 함께할 깃 레포 설정, Code Convention 정하기 (개인공부) AWS EC2, crontab, shell-script 공부 (개인공부) 기본 통계량 복습 (개인공부) KoNLPY, NLTK 훑어보기 오늘 한 일은 무엇인가 (스터디) A* Alogorithm 공부 (스터디) A* 구현, Class 단위로 윤곽 잡기 (개인공부) 확률분포, 기본 통계량 꼼꼼히 보기 (개인공부) shell-script 내일 할 일은 무엇인가 확률분포, 기본 통계량 스터디 main프로젝트 Review (Procedure 정리, 시계열 적용 고민) AWS 복습 (스터디) git repo 설정, A* Algorithm 코드 작성 무엇을 느꼈는가 열심히 개념을 정립하는 것도 중요하지만, 계속 해서 정리하고 스스로에게 Feedback 을 주는 것이 중요하다. 어떤 Procedure 중에 있는 것인가를 아는 것이 가장 중요하다. 앞으로의 적용을 항상 고민해야한다. 회기분석 프로젝트를 마친 것을 다시한번 복기할 필요가 있기에, 이번 주말을 통해 개인적으로 복기를 해보려고 한다. 이에 앞서 이번주부터 시작한 기본 통계량 부터의 복습을 같이 하면서 프로젝트 과정중 잊었던 개념들을 적용하는 연습을 주말동안 다시한번 차근차근 해보아야 한다.","link":"/2018/11/23/181123-TodayWhatILearned/"},{"title":"181125-TodayWhatILearned","text":"181125 TWIL 오늘 한 일은 무엇인가 (스터디) A* Alogorithm git 정리 (프로젝트) 프로젝트 리뷰 (프로젝트) 깃허브 다루기 (개인공부) 판다스 공부 (개인공부) 자연어 처리 메서드 정리 내일 할 일은 무엇인가 (개인공부) 자연어 처리 메서드 마저 정리 (개인공부) AWS 정리 (개인공부) 연속확률분포 정리(수식 꼼꼼하게) 무엇을 느꼈는가 지난주 프로젝트 리뷰 시간에 박사님께서 주신 Comment를 바탕으로 지난 프로젝트 했던 것을 복기하는 시간을 가졌다. 그 중 오늘 집중해서 탐구 했던 것은, Partial Regression 에서 경향이 보였을 때, 이를 구체적으로 어떻게 다루는지 연구하였다. 우리가 Partial Regression plot 을 보고 두가지 경향이 있을때, 한 경향에 대해 뽑아 냈고, 이 뽑아낸 경향을 바탕으로 다시한번 본 Feature 들이 어떤 특성을 가지고 있는지 알 수 있었다. 여기서 필요한 것은 Partial Regression 을 해석 할 수 있는 개념과 이를 바탕으로 다시한번 DataFrame 을 정렬하고, 여기서 Data 들의 특성을 뽑아 낼수 있는 판단력이 필요했다. 당연히 이를 구현하기 위해 Pandas를 다루는데 능숙함 역시 필요했다. 커멘트 받은 것을 이렇게 다시 구현했다는 것에 큰 보람을 느끼며, 프로젝트 진행 당시 좀더 꼼꼼하게 이런것을 생각 할 수 있었으면 좋았을 것이라는 아쉬움도 함꼐 느낀다. 퍼포먼스에 치중하는 것이 아니라 데이터 자체에 어떤 특성이 있는지 파악하려고 노력하는것이 무엇보다도 중요함을 느낀다. 판단을 위해서는 Data 를 보기 편하게 정렬하고, 시각화 하는 것이 매우 중요하다. 앞으로 이를 위해 알고 있는, 알아야되는, 한번 봤는데 잘 기억 안나는 것이 어쩌면 다행이기에, method 들을 간단하게 나마 정리하려고 한다. 개인적인 기록으로 git, github 를 사용하는 것은 어렵지 않았다. 하지만 팀 협업을 하려고보니, git 을 다루는게 아직 능숙지 않았다는 것을 깨달았다. 오늘은 git과 github 에 대해 가볍게 공부했고, 이를 바탕으로 어떤식으로 협업해야하는 것인지 간단하게 나마 알게 된것 같다.","link":"/2018/11/25/181125-TodayWhatILearned/"},{"title":"181126-TodayWhatILearned","text":"181126 TWIL To-do-list : 오늘 할일 이미지 데이터 처리, 사운드 데이터 처리 공부 Shell script 정리 Pandas - 50page 수학 8장-2절 오늘 한 일은 무엇인가 Shell script 공부 내일 할 일은 무엇인가 알고리즘 복습 Shell scipt cheatsheet 작성 Pandas - 100 page 수학 9장-2절 --- ## 무엇을 느꼈는가 - RedBlack tree 를 배우면서, 왜 color violation 을 해결하고, 색을 기준으로 정렬했을 때, O(logN) 으로 장점이 되는지에 관해 궁금하다. 자세한 수학적 증명보다, color 와 알고리즘의 효율이 어디서 연결 되는지 궁금하다. 이런 궁금증들은 이렇게 메모 해두었다가, 여유가 생길 때 꼭 찾아 보아야 겠다. - shell script 기본 문법을 공부하면서, Python 이 얼마나 직관적인 언어인지 느꼈다. 직접 해보면서, while, if 조건문 등을 쓸때, 띄어쓰기에 예민했고, DO ~ DONE 의 couple 이 중요했다. 또한, if , case 가 끝날 때는, 직관대로 done 이 아니라 fi, esac 등으로 닫아 줘야 하는 것이 위트 있지만서도, 쉽게 기억될 것은 아님을 알게 되었다. DO 도 OD 였다는데, 지금 OD 는 안되네.. DO 는 DONE 으로 쓰자 !!!!","link":"/2018/11/26/181126-TodayWhatILearned/"},{"title":"181128-TodayWhatILearned","text":"181128 TWIL To-do-list : 오늘 할일 ~ Classification 성능평가 수학 9장 2절 Pandas ~ 100pages 오늘 한 일은 무엇인가 ~ Classification 성능평가 수학 9장 2절 내일 할 일은 무엇인가 Pandas 100page 무엇을 느꼈는가 프로젝트를 한번 해서 그런지, 지금 공부하는 내용도 앞으로 프로젝트에서 어떻게 활용해야할지 염두? 걱정?하며 공부를 하게 되는 것 같다. 메소드를 정리할 필요가 있고, 배우는 각 개념들이 어떤 판단을 바탕으로 사용해야하는지 생각하며 공부하게 되는 것 같다. 시간이 부족하여, 메소드들을 좀더 디테일하게 보지 못하는 아쉬움이 있지만, 이런 것들을 주말 등을 통하여보충할 필요가 있다.","link":"/2018/11/28/181128-TodayWhatILearned/"},{"title":"181129-TodayWhatILearned","text":"181129 TWIL To-do-list : 오늘 할일 Pandas ~ 150page wiki 구성 방안 REST API, NGINX, AWS 공부 오늘 한 일은 무엇인가 Pandas 80page wiki 구성 REST API, NGINX, AWS 공부 내일 할 일은 무엇인가 Pandas 150page 마무리 복습 --- ## 무엇을 느꼈는가 - 지금껏 구성해온 개인 wiki 혹은 cheatsheet등을 로컬 서버등을 이용해 wiki page 를 이용하고 싶었다. 가장 많이 사용한다는 gollum api 를 설치해보았지만, 생각보다 편하지 않았다. gitlab이나 github 등은 온라인에서 작성이 가능하지만, 로컬에서 그 해당 파일을 갖기가 불편했다. 또 검색기능도 중요했다. - 결국, 지금 작성하고 있는 git blog 에 wiki tab 관리로 하기로 했다. - 돌고돌아 gitblog 의 tab 을 활용해서 wiki 하기로 했다. 앞으로 좀더 깔끔하게 정리 파일을 관리하고, - 직접 참고할 자료이니, 무엇보다도 내가 보기 편해야 할 것이다.","link":"/2018/11/29/181129-TodayWhatILearned-1/"},{"title":"181204-TodayWhatILearned","text":"181204 TWIL 오늘 한 일은 무엇인가 Heap 구조 공부, 코드 짜보기 Logistic Regression method 정리 내일 할 일은 무엇인가 QDA, LDA, 나이브 베이지안 모델 복습, 정리 특히, 수식들 정리 + Method보다는 predict 결과가 나오는 데까지 확률계산, 흐름 중점적으로 공부 (Study) A* 알고리즘 구현 Pandas 3장 무엇을 느꼈는가 Heap 구조를 공부하면서, class 단위로 짜져 있는 코드들을 혼자서 구현하려면 어디서부터 계획을 세워야되는가에 대해 고민했다. 또한, 어떤 이슈가 있을 때, 어떤 알고리즘과 자료구조를 선택해야하는지에 대한 판단근거 역시 앞으로 정리 해야할 필요성을 느낀다. 우선 간단하게 알고 있는 데이터 구조 전체에 대해 서로 비교하여 장, 단점 정도 빠른시일 내에 정리해야겠다.","link":"/2018/12/04/181204-TodayWhatILearned/"},{"title":"181203-TodayWhatILearned","text":"181203 TWIL To-do-list : 오늘 할일 Pandas Cookbook 2장 복습, 3장 ROC 커브, Logistic Regression 시간 날 때, 새로운 데이터셋 EDA 오늘 한 일은 무엇인가 ROC, AUC, Logistic Regression 정리 Flask_app : classification model application 코드 해석 Pandas 2장 복습 내일 할 일은 무엇인가 Heap, Graph 부분 복습 Pandas 3장, 4장 새로운 데이터셋 EDA, Regression 함수 적용 --- ## 무엇을 느꼈는가 - Pandas를 조금씩 정리하고, 새로운 것들을 알게 되면서 이전에는 몰랐던 편리한 기능들이 많이 있음을 알게 되었다. 손에 익히게 되면서 아주 조금씩 판다스에대해서 편해지기 시작하였다. Pandas 를 심도 있고 공부해보고자 했던 처음 이유인 '데이터셋을 보고 먼저 손이 나가도록 하자'는 목표에는 아직도 멀었지만, 하루하루 조금씩 노력해온 1주 반동안, 이전보다 조금더 편해졌다는 것이 느껴진다. - 이렇게 체감적으로, 한파트 한파트씩 아주 더디지만 조금씩 편해지고, 손에 익는 다는 것을 느끼면서, 점점더 재미있어 질것이라 믿는다. ---","link":"/2018/12/03/181203-TodayWhatILearned/"},{"title":"181127-TodayWhatILearned","text":"181127 TWIL To-do-list : 오늘 할일 알고리즘 복습 Shell script cheatsheet 작성 Pandas - 100page 수학 9장-2절 오늘 한 일은 무엇인가 알고리즘 복습 Shell script cheatsheet 작성 내일 할 일은 무엇인가 Pandas - 100page 수학 9장-2절 Classification Intro, Scikit-Learn 의 전처리 부분 code 정리 무엇을 느꼈는가 알고리즘 복습이 계획했던 것 보다 훨씬더 오래 걸려서, 기존에 계획했던 Pandas 와 수학은 보지 못했다. 오늘 공부한 분류도 다시 공부해야하는 만큼, 내일 공부할 양은 매우 많을 것 같다. 정리하는 내용들을 내가 보기 편하고, 쉽게 사용 할 수 있도록 하는 방법을 많이 고민해보아야겠다. 파일들이 누적되고 쌓이면서, 이제는 효율적으로 검색하거나 찾아서 볼 수 있는 것이 매우 중요한 것 같다. 어떻게 정리하거나 검색할 수 있는 Tool이 있을지 틈나는대로 생각해봐야겠다.","link":"/2018/11/27/181127-TodayWhatILearned/"},{"title":"181205-TodayWhatILearned","text":"181205 TWIL To-do-list : 오늘 할일 QDA, LDA, 나이브 베이지안 모델 복습, 정리 수식, Predict 과정 중심적으로 공부 (Study) A* 알고리즘 구현 Pandas 3장 오늘 한 일은 무엇인가 (Study) A* 알고리즘 손코딩 QDA, LDA, 나이브 베이지안 모델 복습, 정리 내일 할 일은 무엇인가 (Study) A* 알고리즘 구현 Pandas 3장 --- ## 무엇을 느꼈는가 - 스터디에서 다같이 알고리즘을 파악하고, 이를 바탕으로 손코딩을 진행했다. Class 단위로 짜는 거였고, 우리끼리 자료들을 바탕으로 알고리즘을 공부하고, 이를 구현하는 것까지 개인적으로 매우 Challenging 했다. 오늘 손코딩을 마무리 하면서, 머릿속에 있는 알고리즘을 어떻게 풀어내는지, 설계부터 각각의 함수까지 계획을 세우는데 매우 큰 보람과 배움이 있었다. - 내일부터는 스터디원들과 함께 오늘 작성한 것을 바탕으로 실제 코딩을 들어갈 계획이다. 아직 미숙한 점이 많기에, 손코딩을 진행한 것들에 대해 Debugging 이 많이 필요하겠지만, 너무 재미있는 시간이었다. - 오후 남은 시간에는 QDA 부터 나이브 베이지안 모델까지(gaussain, bernoulli, multinomial) 수식과 그 메소드들을 하나하나 뜯어보고 분석하는 시간을 가졌다. 단순히 패키지의 메소드를 돌리면 되겠지가 아니라, 메소드를 돌리기전에 간단한 data에 대해 결과를 얻기까지 직접 손으로 풀어보고 계산하여, 메소드를 돌렸을 때 결과와 비슷한지 확인 하는 시간을 통해, 각 분포가 모델링 되는데까지 과정을 이해하는데 도움이 많이 되었다. - 또한 자료에 나오는 다양한 시각화 메소드, 시각화 하는데 필요한 domain 설정, Numpy와 Seaborn 의 메소드들을 파악해보면서, 시각자료의 의도를 생각해보기도 하였다. - 많지 않은 범위를 이렇게 공부하다보니, 꽤많은 시간이 필요하여 오늘의 다른 목표들을 채우지는 못했지만, 스터디와 개인 공부 두가지를 통해, 하루를 매우 생산적으로 보낸 것 같아 뿌듯하다.","link":"/2018/12/05/181205-TodayWhatILearned/"},{"title":"181208-TodayWhatILearned","text":"181208 TWIL 오늘 한 일은 무엇인가 Decision Tree 연습문제 18.4.2 풀기 Imporve Flights_Delay Regression Project Insurance Cost dataset EDA MySQL Quiz 마무리 내일 할 일은 무엇인가 Classification 복습 MySQL 복습 MySQL Query Code Imporvement Insurance Data EDA Pandas 정리 Regression 수학 12-5장 복습 --- ## 무엇을 느꼈는가 - Query 문을 작성하면서, 다양한 방법으로 같은 결과를 얻을 수 있음을 알게 되었다. 같은 결과를 뽑아내는 Query 문의 비교에서, 오늘은 간결하고, 깔끔한 코드를 작성하려고 노력을 하였다. 하지만, 결국 가장 중요한 것은 Query 문이 얼마나 빠르게 동작 하느냐의 문제인 것 같다. 내일은 간결하면서도, 빠르게 동작하는 코드를 작성하는데 좀더 고민해봐야겠다. - 지난 Flights_delay regression project 를 개선하는데 있어, 코드적으로, 기술적으로 부분회기 plot 를 바탕으로 다시 모델링을 하는 방법에 대해 알게 되었다. 여기까지 알게 된 것을 바탕으로, 새로운 Medical cost(Insurance) data 를 regression 으로 모델링 해보려고 한다. 오늘 간단한 EDA 를 진행하였으나, 조금더 데이터를 자세하게 보는 방법론에 대해 공부를 하면서 진행해 보려고 한다. ---","link":"/2018/12/08/181208-TodayWhatILearned/"},{"title":"181207_TodayWhatILearned","text":"181207 TWIL 오늘 한 일은 무엇인가 Pandas 3장, 4장(일부) (Project) 데이터셋 정하기 내일 할 일은 무엇인가 MySQL 복습, QUIZ 풀기 새로운 데이터셋 EDA, Regression Pandas 4장, 7장 지난 프로젝트(Flight_Delay Regression) Update 알고리즘 추가과제 하나씩 풀기 --- ## 무엇을 느꼈는가 - 와.. 오늘은 그간의 파이썬과 알고리즘, 데이터 구조의 공부한 것을 평가받는 첫 시간이었다. 시간내에 주어진 문제를 효율적으로 푸는 것이 생각보다 어려웠다. pandas의 method 나 numpy method 를 자주 사용하면서, 기본적인 내장 method 는 오히려 더 어색했다. 함수를 짜서, 테스트 케이스를 통과하다가 중간에 발생한 에러는 debugging을 하지 못했다. 또한, 코드 역시 비효율적이고 못생겼다. pyint 의 PEP 8 점수 역시 매우 낮았다. - 다시한번 많이 부족함을 느꼈고, 더 공부해야되고 알아야 하는 것이 한참이나 많다. - 쌓이고 쌓이는 ToDoList 에서 우선순위를 매번 잘 매기고, 너무 한 issue 에만 묻혀있지 말아야하며, 동시에 다양한 주제를 공부해야하므로 정리를 잘해야하고, 그 정리를 다음번에 참조할 수 있게 잘 기록해야하며, scheduling 을 효율적으로 해야하고, 무엇보다 그 순간에 매우 집중해야한다. ---","link":"/2018/12/07/181207-TodayWhatILearned/"},{"title":"181209-TodayWhatILearned","text":"181209 TWIL 오늘 한 일은 무엇인가 Classification 복습 (Entropy, DecisionTree) MySQL Query Code Imporvement, CodeReview Insurance Data regression Algorithm 추가 과제 (2진수, 8진수, 16진수) 내일 할 일은 무엇인가 MySQl, NOSQL 복습, 정리 Insurance Regression 과정 정리, 알게된 것들 정리 Pandas 4장, 7장, 9장 정리 Algorithm 추가 과제 마무리 수학 12-5장까지 복습 --- ## 무엇을 느꼈는가 - Insuracne Data regression 을 진행하면서, 지난 프로젝트에서는 다가가지 못한 Step 들을 수행하였다. 잔차의 정규성을 검토하면서, 이를 발전 시킬 수 있는 방법에 대해 코드를 작성하였고, EDA 과정을 꼼꼼히 한 덕분인지, regression formula 를 돌리는데 있어 각 스텝마다, 작은 근거들이 생겨났다. - 아직은 수업 자료에서 모든 내용을 담을 만큼 Performance 와 스텝간의 근거들이 명확하지 않기 때문에 공부할 것이 한참남았지만, 오늘 새로운 데이터셋을 좀더 꼼꼼히 EDA 를 하면서, 새롭게 알게되고, 적용할 수 있는 것들이 생겨 뿌듯했다. ---","link":"/2018/12/09/181209-TodayWhatILearned/"},{"title":"181211-TodayWhatILearned","text":"181210 TWIL To-Do-list Insurance EDA 정리 Pandas 12-5장까지 수학복습 MST 복습오늘 한 일은 무엇인가 Insurance EDA 정리 (STUDY) A* 알고리즘 시각화 선형회귀 개념 복습 내일 할 일은 무엇인가 데이터톤 무엇을 느꼈는가 파이썬으로 작성한 A*알고리즘의 시각화코드를 작성하였다. 제일 빠른 길을 찾아 주었으나, 이것을 시각화하는것이 생각보다 어렵다. 그리고 좀더 동적으로 시각화를 해주고 싶은데, 좀더 삽질을 많이해봐야겠다.결과데이터를 보여주면 되겠지 했지만, 답을 얻는 과정을 세세하게 시각화하여 보여주는 것 역시 어려운 문제였다.비단 알고리즘을 보여주는 것만이 아니라, 지금 공부하는 모든 것이 아마 그럴 것이다. A* 알고리즘을 적용해서 주어진 미로의 최적의 길을 찾는 것은 미로의 크기가 커질수록 검증이 어렵다. 우리가작성한 알고리즘으로 풀어준 path가 진짜 제일 빠른 길인지 확인하는 방법이 무엇인가 하는 생각이 든다. 크기가작은 미로에서 우리가 한길 한길 찾아가는 정답과 맞아서, 알고리즘이 제대로 작동하고 있다고 생각했다. 하지만,점차 크기가 커지면서 정답이 맞는가 확인하는 것은 어려웠다. 작은단위에서 맞는 것이라고 해서, 큰 단위에서 내놓은 답이 과연 정답일 것인지는 어떻게 검증해야하는가. 우리가 소단위에서 다 맞춘 알고리즘이라고 해서 전부 믿어야 하는 것인가라는 생각이든다. 내일은 데이터톤이다. 여태까지 공부한 것을 적용해보고, 실제로 제한시간내에 데이터를 분석해야한다. 아직도 많이 모르고,아는 것도 확신하기 어려운데, 잘 할 수 있을지 걱정된다.","link":"/2018/12/11/181211-TodayWhatILearned/"},{"title":"181216-TodayWhatILearned","text":"181216 TWIL 오늘 한 일은 무엇인가 DataThon 발표 Perceptron 공부 (Project) Quara Dataset EDA question_text 에서 vectorize 하기전에 특징값들을 뽑아내기 나이브 베이지안 돌려보기 (Study) 스터디때 나눌 WebApplication 의 구조, MVC model 나누기 내일 할 일은 무엇인가 (Study) 스터디원 블로그 개설, Flask에 비유한 WebApplication, MVC model 공부하기 (Stydy) PROJECTmini WebApplication 계획 세우기 (Project) EDA 짬짬히 계속하기 SVM 공부 무엇을 느꼈는가 Datathon에서 분석했던 내용을 발표하는 시간을 가졌다. 발표를 하면서 부족하다고 생각했던 점과 comment 를잊기 전에 정리해본다. 후기 및 생각과 느낌 프레젠테이션 능력이 부족하다. 긴장, 생각의 흐름을 말로 표현하는 것이 부족했다. 나름대로 이야기 할 것을 리스트업해갔지만, 잘 눈에 들어오지 않았다. 스크립트를 다 작성해가는 것이 좋은 것일까? 프레젠테이션 혹은 데이터를 모르는 사람도 읽을 수 있는 마크다운 정리가 부족했다. 데이터톤 당시 시간에 쫓기는 것도 있었고, 데이터를 분석하고 코드를 작성하면서 나중에 하면 되겠지 라고 생각했다. 결과는 제대로 마무리와 정리를 하지 못한 채로 제출했고, 이는 발표할 때 쓰는 자료로서는 0점에 가까웠다. 프로젝트나, 코드를 작성할 때 comment 를 좀더 세세하게 작성하도록 노력해야겠다. 지적해주신 comment Regression 에서 intercept 의 의미 실제로 모델링한 결과를 현실 데이터에서 사용하기 위해서는 intercept 를 꼭 추가해야한다고 말씀해주셨다. comment 를 듣자 마자, 조금 찾아봤을 때, intercept 가 error 의 mean 값을 잡아준다고 한다. 이 부분은 좀더 보충이 필요하다. 더미변수를 사용하지만, Intercept 의 효과에 대해서.. 단지 해석의 의미로만 상수항을 생각하였는데, 좀더 본질적인 이유가 있는 것 같다. 꼭 보충할 것! 데이터를 분석하는 과정에서 insight를 얻었을 때, 이를 꼭 알기 쉽게 기록하라. 개인적으로 느꼈던 후기와 생각에서와 비슷한 취지의 말씀이었다. 자신과 다른사람이 알 수 있게 insight 를 꼭 기록하라고 말씀하셨다. R square 를 기준으로 분석을 진행할 때는 조심하여야 한다.","link":"/2018/12/16/181216-TodayWhatILearned/"},{"title":"181215-TodayWhatILearned","text":"181210 TWIL To-Do-list DataThon 발표 준비 Pandas 4장 공부 알고리즘 문제풀기 (Study) 프로젝트 미니(웹어플리케이션) 준비 (Project) Classification Project Data EDA 오늘 한 일은 무엇인가 Datathon 발표준비 알고리즘 문제 풀기 (Study) 프로젝트 미니(웹어플리케이션) 준비 내일 할 일은 무엇인가 Pandas 4장 공부 Support Vector Machine 공부 알고리즘 문제풀기 NoSQL, MySQL syntax 정리 무엇을 느꼈는가 데이터톤 발표를 준비하면서, 제출했던 코드와 과정을 다시 살펴보니 Markdown 이나주석이 부족함을 느꼈다. 다시 볼 때 좀더 편할 수 있도록, 코드와 과정을 다시 이어 나가는데 시간을 덜 소비하도록 나름 신경써서 작성하며 진행했는데, 다시 보려고 하니 머릿속에 있었던 것들이 다 작성되어 있지 않았다. 지금은 데이터톤에서 얼마 지나지 않았기 때문에, 기억에 남는 것이겠다. 하지만 추후에 다시 볼때는 기억이 나지 않아, 내가 작성한 코드와 문서임에도 불구하고 그 맥락을 이해하기 위해 처음부터 읽어 볼 것이다. 앞으로는 좀더 주석과 마크다운 문서에 신경을 많이 써야겠다. 짤막하게라도, 데이터 분석과정 중에 들었던 생각들을 작성해 놓아야, 그 시간이 지난뒤에 Develop 을 하던, 복기를 하던 계속 생각의 흐름을 이어 나갈 수 있을 것이다. 일일코딩, DailyCommit 등에 관한 글을 읽었다. 개인의 다짐과도 비슷하고, 개인 프로젝트로 개발자들이 많이 하는 것 같다. 글을 읽은 직후에는 나도 하고 싶다는 생각을 했지만, 과연 할 수 있을 것인가 하며 반문을 하였다. 다짐의 문제라고 하기엔, 너무 정신 없는 나날을 보내고 있기에.. 도전할 것인지 하루만 더 고민해봐야겠다.","link":"/2018/12/15/181215-TodayWhatILearned/"},{"title":"181220-TodayWhatILearned","text":"181220 TWIL 오늘 한 일은 무엇인가 (Project) Classification Project 모임 딥러닝 엔지니어 현업자 특강 Celery 복습 간단한 알고리즘 문제 풀기 Linear Algebra(Gilbert) 1강 내일 할 일은 무엇인가 Linear Algebra(Gilbert) 2강 (Project) Classification Project Classification 개념 다시 보기 무엇을 느꼈는가 즐기자","link":"/2018/12/20/181220-TodayWhatILearned/"},{"title":"181225-TodayWhatILearned","text":"181225 TWIL 오늘 한 일은 무엇인가 (Project) Text Preprocessing 내일 할 일은 무엇인가 (Project) Project 모임 Linear Algebra 강의 2강, 3강 듣기 무엇을 느꼈는가 모든 전처리가 그렇겠지만, 텍스트 데이터의 전처리는 유독 할게 많다. 실제 사람이 사용하는 언어 데이터이다 보니,예외사항들이 많고 모델 성능에 이 전처리들이 큰 영향을 미친다고 하기에, 열심히 전처리를 하고 있다. 오늘은 embedding 데이터를 활용해 줄임말들 (I’d, We’re 등) 늘려주는 작업을(I would, We are 등) 해줬다.기존에 짰던 영어이냐 아니냐를 분류하려고 만든 알고리즘의 성능이 위 작업을 통해 좀 더 좋아질 것이라 예상된다. 또한오늘 한 작업이 main modeling 을 하기 위해 진행할 Tokenizing 에도 좋은 영향을 줄 것이다. Stopwords 들을빼거나 더할 때도, What’s 보다는 What is 로 늘려주었을 때, 훨씬더 세밀해 질 것이다. 내일은 spelling 체크, 띄어쓰기 체크해서 올바르게 고쳐주는 작업을 해야한다. 그리고, Baseline 모델을잡기 위해 본격적인 modeling 에 들어가야한다.","link":"/2018/12/26/181225-TodayWhatILearned/"},{"title":"REBOOT","text":"** REBOOT ** Text Classification Project 를 한다는 핑계로 그간 TodayWhatILearned의 작성을 하지 못했다.프로젝트를 하는 동안은 매일 어떤 것을 공부할 계획이고, 어떤 공부를 했는지 남길 만한 내용이 없었던 것도 사실이다.프로젝트 동안 미뤄뒀던 공부들, 보고싶었던 주제들을 이제 다시 새로운 마음가짐을 가지고 시작할 것이다.새해가 밝은 만큼 블로그를 만들기 시작하면서 다짐했던 초심을 상기하자. To-Do-List Graph모형, 네트워크 추론 공부 (수식) - 새로운 패키지, 코드 정리하면서 공부 LinearAlgebra 1강, 2강 다시 시작 오늘 한 일은 무엇인가 Graph모형 공부 LinearAlgebra 1강, 2강 내일 할 일은 무엇인가 네트워크 추론 공부(수식 위주로 공부)","link":"/2019/01/08/190108-TodayWhatILearned/"},{"title":"190109-TodayWhatILearned","text":"190109 TWIL 오늘 한 일은 무엇인가 BLOG RENEWAL 내일 할 일은 무엇인가 Graph모형 공부 LinearAlgebra 1강, 2강 무엇을 느꼈는가 새해를 맞아 블로그를 새 테마로 바꾸었다. 기존에 hueman theme 에 익숙해져 있어서, 새로운 테마의 기능을수정하고, 전처럼 편해지려면 또 적응의 시간이 필요할 것 같다. 블로그의 테마는 작년부터 글의 양이 늘어나면 늘어날수록그 욕구가 더 심해 졌다. 특정 카테고리에서 글이 누적되가면서, 어떤 글들이 담겨있는지 제목을 통해 직관적으로보고싶었다. hueman 은 글마다 썸네일들이 있고, 글의 순서가 조금 불편하게 배치되어 있다. 시리즈성 글들을 올린다거나,주제가 1, 2, 3 등으로 나뉘는 글들이 있을 때, 글 제목으로 연속성을 보기가 힘들었다. 위의 이유로 선택한 이번 테마는 내가 중점적으로 생각한 부분을 조금이나마 개선할 수 있는 것 같다. 틈틈히새로운 테마의 세팅도 마쳐야겠다.","link":"/2019/01/09/190109-TodayWhatILearned/"},{"title":"Basic Classification with Pytorch","text":"Basic Classification with Pytorch 이번 post 는 pytorch 를 활용해 기초적인 분류 모델링을 해보면서, pytorch에 익숙함을 높이는 것이 목적입니다. 123456789101112import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport numpy as npimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings(\"ignore\")%matplotlib inline%config InlineBackend.figure_format = 'retina' 1. Binary Classification Modeling Sigmoid Loss : Binary Cross Entropy 1.1 Generate Data123456789101112# plotting functiondef plot_scatter(W_, xy, labels): for k, color in [(0, 'b'), (1, 'r')]: idx = labels.flatten() == k plt.scatter(xy[idx, 0], xy[idx, 1], c=color) x1 = np.linspace(-0.1, 1.1) x2 = -W_[1] / W_[2] * x1 - W_[0] / W_[2] plt.plot(x1, x2, '--k') plt.grid() plt.show() 1234567# Generate dataW = np.array([-4./5, 3./4., 1.0])xy = np.random.rand(30, 2)labels = np.zeros(len(xy))labels[W[0] + W[1] * xy[:, 0] + W[2] * xy[:, 1] &gt; 0] = 1 1plot_scatter(W, xy, labels) 1.2 Train data Generate 한 data 로 부터, x 축 값, y 축 값, augmented term 으로 3가지 column 을 만들어 train data 로 만들어 줍니다. 또한 대응 되는 label 도 model 에 적합한 모양으로 바꾸어 줍니다. 1234x_train = torch.FloatTensor([[1.0, xval, yval] for xval, yval in xy])y_train = torch.FloatTensor(labels).view(-1, 1)print(x_train[:5])print(y_train[:5]) tensor([[1.0000, 0.0192, 0.6049], [1.0000, 0.0485, 0.2529], [1.0000, 0.2412, 0.9115], [1.0000, 0.9764, 0.1665], [1.0000, 0.9021, 0.5825]]) tensor([[0.], [0.], [1.], [1.], [1.]])1.3 Modeling Linear Model 형태와 Sigmoid 함수, Loss function 은 cross entropy 를 활용해 모델링을 합니다. 여기선, 내장되어있는 함수들을 되도록 사용하지 않고, Low level 로 코드를 작성해 보겠습니다. 12345678910111213141516171819202122# Low level modelingparameter_W = torch.FloatTensor([[-0.5, 0.7, 1.8]]).view(-1, 1)parameter_W.requires_grad_(True)optimizer = optim.SGD([parameter_W], lr=0.01)epochs = 10000for epoch in range(1, epochs + 1): # Prediction y_hat = F.sigmoid(torch.matmul(x_train, parameter_W)) # Loss function loss = (-y_train * torch.log(y_hat) - (1 - y_train) * torch.log((1 - y_hat))).sum().mean() # Backprop &amp; update optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch {} -- loss {}\".format(epoch, loss.data)) epoch 1000 -- loss 6.368619441986084 epoch 2000 -- loss 4.5249152183532715 epoch 3000 -- loss 3.654862403869629 epoch 4000 -- loss 3.122910261154175 epoch 5000 -- loss 2.7545464038848877 epoch 6000 -- loss 2.4800000190734863 epoch 7000 -- loss 2.2651939392089844 epoch 8000 -- loss 2.091233253479004 epoch 9000 -- loss 1.9466761350631714 epoch 10000 -- loss 1.82413184642791751parameter_W.data.numpy() array([[-16.748823], [ 16.618748], [ 17.622692]], dtype=float32) 아래 그림을 보면, Train이 잘 된 것을 알 수 있습니다. 1plot_scatter(parameter_W.data.numpy(), xy, labels) 2. Multiclass Classification2.1 Generate Data 이번에는 3개의 label 을 가지고 있는 classification 을 Modeling 해 보겠습니다. 또한, High Level 로 pytorch 의 추상 클래스를 이용해 모델링 해보겠습니다. 123456789101112def plot_scatter(W1, W2, xy, labels): for k, color in [(0, 'b'), (1, 'r'), (2, 'y')]: idx = labels.flatten() == k plt.scatter(xy[idx, 0], xy[idx, 1], c=color) x1 = np.linspace(-0.6, 1.6) x2 = -W1[1] / W1[2] * x1 - W1[0] / W1[2] x3 = -W2[1] / W2[2] * x1 - W2[0] / W2[2] plt.plot(x1, x2, '--k') plt.plot(x1, x3, '--k') plt.show() 123456789# Generate dataW1 = np.array([-1, 3./4., 1.0])W2 = np.array([-1./5, 3./4., 1.0])xy = 2 * np.random.rand(100, 2) - 0.5labels = np.zeros(len(xy))labels[(W1[0] + W1[1] * xy[:, 0] + W1[2] * xy[:, 1] &gt; 0)] = 1labels[(W2[0] + W2[1] * xy[:, 0] + W2[2] * xy[:, 1] &lt; 0)] = 2 1plot_scatter(W1, W2, xy, labels) 2.2 Train data1234x_train = torch.FloatTensor([[1.0, xval, yval] for xval, yval in xy])y_train = torch.LongTensor(labels)print(x_train[-5:])print(y_train[-5:]) tensor([[ 1.0000, 0.9641, 1.3851], [ 1.0000, -0.4445, 1.0595], [ 1.0000, 1.0854, -0.1216], [ 1.0000, 0.8707, 0.1640], [ 1.0000, 0.7043, 1.3483]]) tensor([1, 0, 0, 0, 1])2.3 Modeling1234567class MultiModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3, 3) def forward(self, x): return self.linear(x) 1234567891011121314151617model = MultiModel()optimizer = optim.SGD(model.parameters(), lr=0.01)epochs = 10000for epoch in range(1, epochs + 1): y_hat = model(x_train) loss = F.cross_entropy(y_hat, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch {} -- loss {}\".format(epoch, loss.data)) epoch 1000 -- loss 0.6635385155677795 epoch 2000 -- loss 0.5513403415679932 epoch 3000 -- loss 0.4890784025192261 epoch 4000 -- loss 0.44675758481025696 epoch 5000 -- loss 0.4151267111301422 epoch 6000 -- loss 0.39017024636268616 epoch 7000 -- loss 0.369760125875473 epoch 8000 -- loss 0.35262930393218994 epoch 9000 -- loss 0.3379631042480469 epoch 10000 -- loss 0.32520908117294312.4 Accuracy 계산 Accuracy 가 96 % 로 비교적 잘 분류 된 것을 확인 할 수 있습니다. 12accuracy = (torch.ByteTensor(model(x_train).max(dim=1)[1] == y_train)).sum().item() / len(y_train)print(\"Accuracy: {}\".format(accuracy)) Accuracy: 0.96","link":"/2019/05/06/Basic-Classification-with-Pytorch/"},{"title":"[CS224n]Lecture01-WordVecs","text":"[CS224n] Lecture 1: Introduction and Word VectorsStandford University 의 CS224n 강의를 듣고 정리하는 글입니다. 1. Human Language and Word MeaningWord meaning 의 뜻 : symbol → idea or thing : Denotational Semantics 우리가 ‘의자’라는 단어를 예로 들자면, 말하는 사람과 듣는 사람 모두 의자와 관련된 특정한 이미지와 아이디어 등을 생각해 볼 수 있다. 이렇게 단어에 대해 관념적인 것을 word meaning 이라고 할 수 있다. 그렇다면 컴퓨터에서 사용할 수 있는 word meaning 은 어떤 것이 있을까? In Computer Word Representations WordNet: synonym 과 hypernyms 의 집합으로 표현 (e.g: nltk wordnet) 아주 귀한 dataset 이지만, 단점: Nuance (뉘앙스)를 담아내지 못함, 새로운 단어에 대해 사람이 직접 가공하여 추가해주어야 한다. 단어의 동의어에 대해 계산할 수 없음 Discrete Symbols: One-hot-vectors 단점 : 벡터의 차원이 우리가 가지고 있는 단어의 갯수만큼 커지게 된다. Corpus 의 구성 단어가 커질 수록 계산 해야하는 벡터 차원이 매우 커진다. 이는 컴퓨팅 자원의 부족으로 인한 현실적 구현의 어려움을 야기시킨다. One-hot-Vector 는 Vector Space 에서 모두 서로 orthogonal (직교)한다. 즉, similarity 가 모두 0으로 단어간의 관계를 알기 힘들다. By CONTEXT: Word Vecttors “You shall know a word by the company it keeps” (J.R.Firth) 어떤 특정 단어 w 가 나온다는 것은, 그 주변 단어들(context)이 있기 때문이다. Word Vectors == Word Embeddings == Word Representations == Distributed Representation 2. Word2vec IntroductionWord2vec(Mikolov et al. 2013) 은 Word Vectors 를 만들기 위한 알고리즘 중 가장 기초 뼈대를 이루는 알고리즘이다. 가지고 있는 텍스트 데이터를 구성하는 큰 CORPUS Corpus 의 모든 단어는 RandomVector 로 시작한다. 각 position t 에 대해, 중심단어 c(center) 와 주변단어 o(outside) 를 생각할 수 있다. 중심단어 c 가 주어질 때, 주변단어 o 의 확률을 구하기 위해(skip-gram)(반대로도 가능(cbow): 주변단어가 주어질 때, 중심단어의 확률을 구하는 방법), 중심단어 c 와 주변단어 o, word 벡터들에 대해 similarity를 구한다. 위의 확률을 최대화 하기 위해 word vector 를 updating 한다. 3. Word2vec objective function gradients문장의 특정 위치 t 에 대해 (t = 1, …, T), 중심단어w_j가 주어졌을때, 주변단어를 한정하는 window size m 에 대해 주변단어들을 예측하는 Likelihood 는 다음과 같습니다. likelihood 식을 해석해보면, 중심단어 w_t 에 대해 중심단어를 중심으로 window 사이즈 2m 개의 단어들의 확률을 모두 곱하고, T 개의 단어 갯수에 대해 또 다시 모두 곱합니다. $$Likelihood\\quad L(\\theta)= \\prod_{t=1}^{T}\\prod_{-m \\leq j \\leq m}P(w_{t+j}|w_{t};\\theta)$$ 목적함수는 likelihood 를 바탕으로 minimize와 average, 계산 편의를 위해 log 를 씌웠다는 것외에 likelihood 와 동일하다. $$objective function \\quad J(\\theta) = - \\frac{1}{T}logL(\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-m \\leq j \\leq m, j\\neq0}logP(w_{t+j} | w_{t};\\theta)$$ 단어가 등장할 확률을 구하는 방법은 Softmax Function 을 활용합니다. Objective Function 의 derivativeObjective Function 과 단어 확률 (Softmax) 의 미분을 구하는 과정이다.","link":"/2019/07/06/CS224n-Lecture01-Summary/"},{"title":"[CS224n]Lecture04-BackPropagation","text":"Lecture 04: Backpropagation and computation graphsStandford University 의 CS224n 강의를 듣고 정리하는 글입니다. 기본적인 computation graph 와 backpropagation 에 관한 내용은 스킵하도록 하겠습니다. 새롭고, 핵심적인 내용만 간추린 내용입니다. 1. Word Vector를 Retraining?Quetion: Retraining 에 대한 판단의 시작 예: TV, telly, television 이 pre-trained word vector 에서는 비슷한 공간에 분포되어있다고 가정할 때, training data 에는 TV 와 telly 단어만 존재하고, test data 에 television이 존재 할 때, word vector 는 어떻게 될 것인가? Answer: Training data 에 있는 TV 와 telly 에 해당하는 word vector 는 back prop을 진행하면서, 미세하게 업데이트 되며, 같은 방향으로 이동하게 된다. 반면, television에 해당하는 word vector는 weight parameter 업데이트가 일어나지 않으므로, 처음에는 비슷한 공간에 분포 했으나, TV 와 telly 와 멀어지게 된다. 1-1. Word Vector의 Retraining 여부 Word Vector가 학습 할 때는 매우 큰 데이터셋을 가지고 학습하게 된다. 따라서 매우 다양한 단어들이 Corpus 로 존재한다. Fine tuning?: 우리가 가지고 있는 training dataset 이 매우 작다면, 위에서 든 예에서 직감할 수 있듯이, pre trained vector 를 fine tuning 하게 되면, training set 에 fitting 되는 효과가 있고, 우리가 의도치 않는 weight 의 업데이트가 되거나 혹은 되지 않을 수 있다. 2. 효율적인 gradient 계산 사실 당연하게 여김에도, 우리가 손으로 계산하는 (upstream network * local gradient) 과정을 아래 식에서도 확인 할 수 있듯이, ds/dh, dh/dz term 은 중복되는 과정이다. $$\\frac{ds}{dW}=\\frac{ds}{dh}\\frac{dh}{dz}\\frac{dz}{dW}$$ $$\\frac{ds}{db}=\\frac{ds}{dh}\\frac{dh}{dz}\\frac{dz}{db}$$ 따라서 효율적인 computation을 구현하기 위해서, back propagation에서 upstream network 를 저장하고, 각 parameter 에 대한 local gradient를 구해, 동시에 곱해주어 back prop 을 진행 할 수 있다. 3. Regularization우리가 parameter 가 많아지면 많아질 수록, training error 와 test error 는 낮아지기 마련이다. 하지만, 어느 수준을 넘어가게 되면, training set 에 대해서는 매우 정확해지는 반면, test data(validate data)에 대해서는 generalization에서 실패한 그래프나 수치들을 확인할 수 있다. 따라서, 우리는 반드시 우리가 최적화 하려고 하는 Loss 에 대해 Regularization 을 해주어야만 한다. 다음은 L2 Regularization term 이 추가된 loss function 이다. 지겹도록 바왔음에도, 꼭 식을 보면 해석해야겠기에, Model parameter (weight) theta 가 제곱term 으로 너무 커지는 것을 방지하기 위해 lambda 에 비례하여 penalty term 을 추가한다. 4. Vectorization우리가 forward/backward propagation 을 진행하면서, 각 data의 계산을 looping 하여 계산한다면, 매우 비효율적인 계산 방식이 된다. 우리는 위대한 Vecor/Matrix Multiplication 방법으로, 즉, 모든 data 와 weight을 행렬로 만들어 forward/backward 계산을 진행해야한다. 또한 이렇게 진행했을 때, 가속화 도구인 GPU 활용의 이점을 사용할 수 있다.","link":"/2019/08/09/CS224n-Lecture04-Summary/"},{"title":"[CS231n]Lecture02-Image Classification Pipeline","text":"Lecture 02: Image Classification Pipeline 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Image Classification 의 기본 TASK 위 사진을 보고, → ‘CAT’ 혹은 ‘고양이’ 라고 classification 자연스럽게 따라오는 문제는 “Sementic Gap” : 우리가 준 data (pixel 값 [0, 255]) 와 Label 간의 gap 또한, 이 과정에서 극복해야 하는 Challenges Viewpoint Variation ( 같은 객체에 대해 시점이 이동해도 robust) Illumination ( 빛, 밝기, 명암 등에도 robust) Deformation ( 다양한 Position, 형태의 변형에도 robust) Occlusion ( 다른 물체나 환경에 의해 가려지는 data 에도 robust) Background Clutter ( 배경과 비슷하게 보이는 객체에도 robust) Intraclass Variation ( 한 종류의 클랫스에도 다양한 색과 모습의 객체가 있을 수 있다.) 2. 기존의 시도와 New Era Hard Coded Algorithm 과 여러 규칙 (rule-based로 해석된다) 들을 통해서 Image를 Classify 하는 노력들이 있어왔다. 이들의 문제는, (1) 위에 언급한 문제들이 Robust 하지 않다. (2) 객체가 달라지면, (고양이, 호랑이, 비행기 등) 객체마다 다 다른 규칙을 성립해줘야 한다. 즉 한마디로 요약하자면, Algorithm의 확장성이 없다. 이런 문제에 좀더 강한 방법이 지금 우리가 공부하고 있는, Data-Driven Approach Image 와 Label pair 의 dataset 을 모은다. Machine Learning 알고리즘을 이용해 classifier 를 학습시킨다. Classifier 를 new images 에 테스트에 평가한다. 3. First Classifier : Nearest Neighbor3-1. Nearest Neighbor 의 기본 알고리즘 train set 에서의 모든 data 와 label 을 기억한다. test Image 와 가장 가까운 train Image 의 label로 test image를 predict 한다. 이 때 ‘가장 가까운’ 을 계산 할 때, L1 distance 와 L2 distance 가 쓰일 수 있다. 이 외에도, 다양한 distance 지표가 쓰일 수 있다. 3-2. Nearest Neighbor Classifier 의 문제 이미지 classification 에는 잘 사용되지 않는다. train 보다 predict 하는데 훨씬 오래 걸린다. train 은 train data set 의 기억만 하면 되지만, predict 할 때는 전체 train data 에 대해 거리를 측정해야하고, sorting 해야하는 문제가 발생한다. Time Complexity - train O(1), predict O(N) (N은 train data 수) 위 그림을 보면, 연두색 공간에 노란색 class 가 포함 되어 있는 것을 볼 수 있다. 이는 generalize 면에서 부족한 모델이라고 볼 수 있다. 같은 알고리즘 이지만, 이를 해결 하는 방법은 K 개의 가까운 neighbor 로 부터 majority voting 을 받은 것으로 classify 를 하는 것이다. 3-3. K-Nearest NeightborsSingle Nearest 만 보는 것이 아니라, K 개의 가까운 point 의 투표를 통해 해당 test data 의 label 을 예측한다. 이 때, Voting 하는 방법에는 majority voting ( 다수결 ) 과 weighted voting ( 가중치를 주어 투표: distance 가 가까운 것에 가중치를 준다.) 가중치를 주는 방법에는 distance 가 커지면 곱해지는 weight 을 줄이는 방법으로 1 / (1+distance) 등을 weight 을 곱해준다. 3-4. k-Nearest Neighbor on images NEVER USED 차원의 저주 문제 knn 이 잘 동작하기 위해서는 dataset 공간을 조밀하게 커버할 만큼의 충분한 training space 가 필요하다. 하지만, data 의 차원이 늘어날 수록 그 충분한 data 의 수가 exponential 하게 늘어난다. 4. Setting HyperparametersModel 최적의 hyperparameter 를 찾기 위해서는 data set 을 구분하여, unseened data 를 사용하여 성능 검증을 하고, model selection 을 해야한다. 이는 단순이 hyperparameter 를 찾는 용도 뿐만 아니라 우리가 세운 가설을 서로 비교 할 때는 data set 을 정확히 구분하고, test set 을 통해 비교하고, 선택해야한다. 그 방법에는 train, validation, test set 으로 dataset 을 나누는 방법과 cross validation 방법이 있다. 첫 번째 방법으로는, Validation set 을 통해 hyperparameter(가설)를 검증하고 선택하여, Test set 을 사용하여 Evaluate 과 Reporting 등을 한다. 딥러닝 모델링에서는 이 방법으로 많이 사용한다. 두 번째 방법은, data set 의 크기가 크지 않을 때, Train set 안에서 folds 들을 나누어 각 fold 가 돌아가며 validation set 이 되며, 이들의 평균값으로 가설을 비교한다. 이는 딥러닝 모델에서는 적합하지 않은 형태이다. 모델 자체의 연산이 많은데다가, 같은 모델에 대해 많은 validation 이 효율적이지 않기 때문이다. 또한 data가 많지 않은 상태에서 딥러닝 모델을 선택하는 것은 옳지 않다. 5. Second Classifier : Linear ClassifierLinear Classifier 는 Neural Network 의 기본 골격이다. (1) image data 와 W (parameters or weights) 을 통해 연산을 해주고, (2) function 을 통과해 Classification 을 해준다. 특히 Linear Classifier 의 경우 아래 와 같이, (1) image data 와 W 를 dot product 를 해주고 (2) linear function f 를 통과한다. $$f(x, W) = Wx$$ 6. ReferenceLecture 2 | Image Classification Syllabus | CS 231N","link":"/2019/05/13/CS231n-Lecture02-Summary/"},{"title":"[CS231n]Lecture03-LossFunction/Optimization","text":"Lecture 03: Loss Function &amp; Optimization 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Introduction Loss Function : 우리가 가지고 있는 W matrix 가 얼마나 안좋은지 정량화(Quantify) Optimization : 위의 Loss Function 을 minimize 해서 가장 좋은 parameter (W) 를 찾는 과정 2. Loss Function주어진 data 가 다음과 같을 때, $${(x_i, y_i)}_{i=1}^{N}$$ Loss 는 “Average of over examples” 즉, $$L = \\frac{1}{N}\\sum_{i}L_i(f(x_i, W), y_i)$$ 딥러닝 알고리즘의 General Setup W 가 얼마나 좋고, 나쁜지를 정량화하는 손실함수 만들기 W 공간을 탐색하면서 이 loss를 minimize 하는 W 를 찾기 2-1. Loss Example: Multiclass SVM LossSVM Loss 는 다음과 같다. 주어진 data example (x_i, y_i) 에 대해서, score vector s 는 다음과 같다. $$s = f(x_i, W)$$ 이 때, SVM loss는 $$L_i = \\sum_{j\\neq y_i}\\begin{cases} 0 \\quad\\quad\\quad\\quad\\quad\\quad\\quad if ;s_{y_i} \\geq s_j +1 \\ s_j - s_{y_i} + 1 \\quad\\quad otherwise \\end{cases} \\ = \\sum_{j\\neq y_i}max(0, s_j-s_{y_i}+ 1)$$ x_i 의 정답이 아닌 클래스의 score (s_j) + 1 (safety margin) 과 정답 클래스 score s_yi 를 비교하여, Loss 를 계산한다. SVM Loss 의 최대, 최솟값은 ? min : 0, max : infinite W 를 작게 초기화 하면, s 가 거의 0에 가까워 진다. 이 때, SVM Loss 는 어떻게 예상되는가? 정답이 아닌 class, 즉 class - 1 개의 score 원소들을 순회하면서 모두 더할 때, score 는 0에 가깝고, 이를 average 취하면 class 갯수 - 1 만큼의 Loss 값이 나온다. 이 특징은 debugging strategy 로 사용할 수 있다. 초기 loss 가 C-1 에 가깝지 않으면 bug 가 있는 것으로 의심해볼 수 있다. 만약 include j = y_i 이면, SVM Loss 는 어떻게 되는가? Loss Funtion 이 바뀌는 것은 아니다. 단지 전체 loss의 minimum 이 1이 될 뿐이므로 해석의 관점에서 관례상 맞지 않아 정답 class 는 빼고 계산한다. 우리가 average 를 취하지 않으면? 이 역시 바뀌는 것이 없다. 전체 class 수는 정해져 있고, 이를 나누는 average 는 scaling 만 할 뿐이다. Loss 를 max(0, s_j - s_yi + 1) ^2 를 사용하면? 이는 squared hinge function 으로 때에 따라서 사용할 수 있는 loss function 이다. 다른 Loss function 이며, 이는 위의 loss 와 다르게 해석 할 수 있다. 기존의 SVM loss 는 class score 가 각각 얼마나 차이가 나는지에 대해서는 고려하지 않는 것이라고 한다면, squared 가 들어감으로써, 차이가 많이 나는 score class 에 대해서는 좀더 가중하여 고려하겠다는 의미로 해석 할 수 있다. 2-2. Regularization만약 위 Loss Function 에 대해서, L = 0 으로 만드는 W 를 찾았다고 할때, 과연 이 W 는 유일한가? 그렇지 않다. W 일 때, L=0 이라면, 2W 역시 L=0 이다. 또한 L을 0으로 만드는 다양한 W 중에서 단지 training data 에만 fit 하는 classifier 를 원하는 것이 아니라, test data에서 좋은 성능을 발휘하는 classifier 를 찾기를 원한다. 이런 Overfitting 을 막기 위해서는 모델의 W 를 다른 의미에서 조절해줄 수 있는 Regularization term 을 추가할 수 있다. 즉, Model이 training set 에 완벽하게 fit 하지 못하도록 Model 의 복잡도에 penalty 를 부여하는 것을 말한다. Regularization 의 종류들: L2 Regularization L1 Regularization Elastic net(L1 + L2) Max norm Regularization Dropout Batch normalization, stochastic depth 2-3 Loss example: Softmax Classifier (Multinomial Logistic Regression)deeplearning 에서 훨씬 더 자주 보게 되는 loss 의 종류 중 하나이다. 위에서 살펴본 SVM loss 의 단점은 그 값 자체에 어떤 의미를 부여하기는 힘들다는 점이다. 반면에, Softmax Classifier 는 그 값 자체를 확률적 해석이 가능하기 때문이다. (cf. 콜모고로프의 공리를 통해 softmax 의 layer 의 output 이 확률로 해석 될 수 있다.) Softmax Function 은 다음과 같다. $$P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}},\\quad where \\quad s = f(x_i;W)$$ 3. OpitmizationOptimization 을 한마디로 요약하자면, 우리의 loss 를 최소화 하는 W 를 찾기 가 되겠다. 그 방법에는, (바보 같은 접근인: 강의표현) Random Search Gradient 를 구하는 방법 Numerical Gradient 수치적 접근 : 이 방법은 근사치를 구하는 것이며, 매우 느린 단점이 있다. 하지만, 쉽게 작성할 수 있다는 장점이 있다. Analytic Gradient 해석적 접근 : 미분식을 구해야하는 단점이 있다. 하지만 빠르고 정확하다. 실제로는, Analytic Gradient 방법을 사용한다. 하지만 debugging 을 위해 numerical gradient 를 사용한다. 이를 gradient check이라 한다. 3-1. Gradient Descent &amp; Stochastic Gradient DescentGradient Descent 를 방법을 이용해서 optimization 을 진행할 수 있다. 하지만 데이터의 숫자와 차원이 매우 큰 경우, parameter (W) 를 update 하는데 그 연산량이 매우 큰 단점과 위험이 있다. 이를 해결하기 위해 minibatch 를 사용하여 확률적 접근을 사용한다. 4. Image Feature ExtractionCNN 등이 등장하기 전에 Image 에서 Feature 를 뽑아내는 방법에 대해 소개한다. Feature를 뽑아내는 개념으로 생각할 수 도 있지만, Feature Transform 이라는 표현을 사용한다. Color Histogram : 이미지의 color distribution 을 사용하여 해당 이미지의 feature 로 사용할 수 있다. (출처: wikipedia ) For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image’s color space, the set of all possible colors. Histogram of Oriented Gradients (HoG) : CNN 이 등장하기 전, 매우 인기있는 Image Feature 중 하나라고 알고 있다. Edge 를 검출하는 방법이다. pixel 사이에, 값의 gradient 가 가장 큰 neighbor 가 edge 일 것이다라는 개념을 사용하여 edge 를 검출한다. 사진을 8 x 8 patch 를 만들어, 각 patch 마다 9 directional oriented gradients 를 계산하여, 이를 feature 로 사용하는 방법이다. Bag of Words : NLP 에서도 자주 사용되는 개념인 BoW 에서 차용한 개념으로, 이미지 데이터들에서 일정 크기의 patch 를 모아 clustering 을 통해 visual words (codebook) 을 만든다. 그리고 feature 뽑아내고 싶은 image 를 patch 형태로 바꾸고, codebook 에서 찾아 histogram 을 만들어 이를 feature 로 사용한다. 5. ReferenceLecture 3 | Loss Functions and Optimization Syllabus | CS 231N","link":"/2019/05/16/CS231n-Lecture03-Summary/"},{"title":"[CS231n]Lecture05-CNN","text":"Lecture 05: Convolutonal Neural Networks 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Convolution Layer1-1. Fully Connected Layer 와 Convolution Layer 의 비교32 x 32 x 3 image 가 있다고 하자. network 에 주입하기 위해, 1 x 3072 로 핀 data 를 상상해 보자. Fully Connected Layer 의 경우, 아래 그림 처럼, Weight 과 dot product 가 수행되어, activation 값이 나오게 된다. 이 때, activation 의 갯수는 W 의 크기에 따른다. Fully Connected Layer 의 경우, 사진이라는 공간적 구조(Spatial Structure) 가 중요한 data 에 대해, 공간적인 정보를 다 잃어버리는 문제가 발생한다. 공간적 정보를 보존하기 위해 Convolution Layer 를 사용한다. 1-2. Convolution Layer Overview 이 때, 실제 convolution 계산은 image 의 filter 크기 만큼의 matrix 를 vectorize 한 후, filter vector 와 dot product 로 수행한다고 한다. 따라서 이 때 헷갈리지 않도록 할 것은, 한번의 Convolution 연산 결과는 하나의 scalar value 가 된다. 따라서, 하나의 filter 가 이미지 한 장을 훑어 내려간다면, 원본 이미지보다는 조금 작은 depth 가 1인 activation map 이 결과로 나온다. 따라서 activation map 의 깊이(channel 수)는, filter 의 갯수 와 동일하다. 여러개의 필터는 이미지의 각기 다른 특징을 추출하려는 의도에서 사용된다. 그 후, 이 Convolution 의 결과인 activation map 을 비선형 함수(ReLU 등)에 통과 시킨다. 여러 유명한 ConvNet 들은 이렇게, Convolution Layer 와 비선형함수를 반복적으로 나열 한 Network 라고 볼수 있다 1-3. Convolution Layer 의 결과물이렇게 여러 계층의 Convolution Layer를 쌓는 것은, 가장 아래 Layer 부터 높은 Layer 까지 단순한 feature → 복잡한 feature 를 뽑아 내는 것으로 볼 수 있다. 1-4. Convolution Layer 연산 filter 가 이미지를 훑고 지나가면서 convolution 연산을 한 후, 나온 결과는 원본 이미지보다 그 크기가 작아지게 된다. 그 정도는 filter 의 크기와 filter 가 훑고 지나가는 간격인 stride 에 따라 바뀌게 된다. $$output ;size = (N-F)/stride + 1$$ 문제점: convolution 연산의 문제는 이미지의 모서리에 있는 정보는 가운데에 있는 이미지의 정보보다 적게 추출 되는 문제가 있다.(filter 가 모서리를 넘어서는 이동 할 수 없으므로) convolution layer 를 반복적으로 지나가다 보면, map의 크기가 매우 빠르게 작아지게 된다. 이를 위해 적용하는 것이 Padding 이다. 1-5. Padding모서리에 정보를 얻기 위해 이미지이 외곽에 숫자를 채워 주는 방법. 이 때, 많이 사용하는 방법은 zero-padding. zero-padding 외에도 다양한 방법이 있다. 2. Pooling LayerParameter 의 갯수를 줄이기 위해, 우리가 ConvLayer 를 통해 뽑아낸 image 를 작게 만드는 Layer 이다. 즉, Downsampling 을 위한 것. Maxpooling 의 intuition : 앞선 layer filter 가 각 region 에서 얼마나 활성 되었는지 보는 것이다. 3. Typical Architecture[[(Conv → RELU) * N → Pool] * M → (FC → RELU) * K ] → SOFTMAX N : ~ 5 M : Large K : 0 ~ 2 ResNet, Google net 등은 이 방식을 훨씬더 뛰어넘음 4. ReferenceLecture 5 | Convolutinoal Neural Networks Syllabus | CS 231N","link":"/2019/05/20/CS231n-Lecture05-Summary/"},{"title":"[CS231n]Lecture04-Backprop/NeuralNetworks","text":"Lecture 04: Backpropagation and Neural Networks 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Backpropagation1-1. 핵심Backprop의 한줄요약: 각 parameter 에 대해 Loss Function 의 gradient 를 구하기 위해 사용하는 graphical representation of ChainRule 언뜻 보면 어려울 수 도 있지만, just ChainRule 위의 그림을 예를 들어 살펴 보면, y 에 대한 f의 gradient 는 q에 대한 f의 gradient * y 에 대한 q 의 gradient 으로 볼 수 있다. 이를 해석하자면, f 에게 미치는 y 의 영향 = f 에게 미치는 q의 영향 * q 에게 미치는 y 의 영향으로 볼 수 있다. Backpropagation 의 가장 중요한 특징!! gradient 를 구하기 위해 node 를 기준으로 앞과 뒤만 보면 된다. 또한, 위 그림을 보게 되면, Forward passing 과 마찬가지로, Backprop 시에도, 이전 노드에서 전달 되는 Gradient 를 node 에서 local gradient 와의 연산으로 다음 Node 에 gradient 를 전달해 줄 수 있다. 1-2. Back prop 시, 각 node 의 역할 Add gate : gradient distributor add gate 를 기점으로, 각 입력(forward 방향의 입력)의 gradient로 local gradient 를 구하면 1이므로, 전달 되는 gradient 와 각각 1씩 곱해져 전달 되게 된다. 이 현상을 보게 되면 동등하게 나눠주는 역할을 하므로 gradient distributor 라고 볼 수 있다. Max gate : gradient router Max gate 는 gradient 를 한쪽에는 전체, 다른 쪽에는 0 을 준다. 해석적으로 보자면, max 연산을 통해 forward 방향에서 영향을 준 branch 에게 gradient 를 전달해 주는 것이 합적 $$max(x, y) = \\begin{cases} x \\quad\\quad if \\quad x &gt; y \\\\ y \\quad\\quad if \\quad x &lt; y \\end{cases}$$ 수식으로 보자면, x 에 대한 gradient, y 에 대한 gradient 가 각각 (1, 0), (0, 1) 로 local gradient 가 계산되기 때문이다. gradient 가 전달될 길을 결정해주는 면에서, 네트워크에서 path 를 설정해 주는 router의 기능과 비슷하다. Mul gate : gradient switcher + scaler 곱셈연산의 gradient 의 경우, x 에 대한 gradient 는 y 가 되므로, 서로 바꿔주는 역할을 한다. 이 때, forward 상에서의 결과 값으로 곱해주므로, scale 역할까지 함께 하게 된다. 2. Neural Networks강의에서는 Neural Network 에 대한 intuition 을 위해, biological neuron 과 비교하였다. 모델 architecture 로서의 neuron 과 biological neuron 의 공통점은 다음과 같다. input impulse input axon → dendrite (cell body)activation &amp; activation function output axon 이러한 비교는, 나의 개인적인 Neural Network에 대한 공부와 이해에 도움이 되지 않기에 큰 감동은 없다. 4강에 대한 핵심 사항은, Backpropagation 에 대한 수식적 이해와 그 이해를 통해 Backpropagation 이 gradient를 구함에 있어, 얼마나 편한 representation 인지이다. 공부하며 어려웠던 것은, backprop in vectorized 에서, Jacobian Matrix 의 표현이 scalar backprop 때와는 달리 한번에 머릿속으로 상상되지 않았기에, 손으로 써가며 따라 갔어야만 했다. 3. ReferenceLecture 4 | Introduction to Neural Networks Syllabus | CS 231N","link":"/2019/05/18/CS231n-Lecture04-Summary/"},{"title":"[CS231n]Lecture07-Training Neural Networks part2","text":"Lecture 07: Training Neural Networks part2 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. Fancier Optimization1-1. Problems with SGD Loss function 의 gradient 방향이 고르지 못해, 지그재그 형태로 업데이트가 일어나고, 그 업데이트가 느리게 발생된다. 고차원으로 갈수록, 자주 만날 수 있는 문제. Local Minima &amp; Saddle point: 높은 차원에서 생각해보면, Local Minima 는 고차원의 모든 gradient 방향에 대해 전부 loss 가 증가하는 방향이므로, 이 경우보다는 Saddle point 문제가 훨씬더 빈번하게 발생한다. Saddle point 의 경우 그 포인트 자체도 문제지만, 그 근처에서 gradient 가 매우 작기 때문에 update 를 해도 매우 느린 문제가 있다. SGD 의 S - stochastic! : 미니 배치의 loss 를 가지고 전체 training loss 를 추정하는 것이므로, 노이즈가 포함된 추정일 수 밖에없다. 1-2. 다양한 최적화 알고리즘 기법최적화 알고리즘은 간단하게 정리한다. SGD, SGD + Momentum, Nesterov Momentum : 진행방향과 그 gradient (Momentum) 을 더해 실제로 업데이트 될 방향을 정한다. AdaGrad : 이전에 진행했던 gradient 를 제곱하여, 업데이트가 진행 될 수록 학습률을 작게해, 세밀하게 업데이트한다. RMSProp : AdaGrad 가 gradient 를 제곱할 때, 학습의 후반부에 갈수록 gradient 가 0 에 가까워 지게 되므로, 제곱하는 term 의 비율을 설정해 0으로 가는 것을 막는다. Adam : Momentum 방법과 AdaGrad를 합친 방법으로써, 학습률을 조절하면서, 속도를 조절하는 방법이다. 2. Regularization2-1. Dropout앞선 강의에서 다양한 Regularization 방법들을 살펴 보았었다.L2, L1, Elastic Net 등. 이번 강의에서는 신경망에서 자주 사용되는 dropout 방법을 살펴본다. Dropout 이란 forward 진행시, 일부 뉴런을 0으로 만들어 버리는 것을 말한다. dropout layer 는 우리가 그 dropout rate 을 설정하므로써, 어떤 확률로 꺼질지 설정할 수 있다. dropout 은 먼저 각 뉴런이 서로 동화되는 것을 방지함으로써 다양한 표현방식을 지닐 수 있게 한다. 또한, dropout 은 여러 sub model 을 앙상블 하는 효과를 낼 수 있다. Dropout Layer 를 사용할 시 주의 할 점은 evaluation, inference 시, 네트워크의 출력에 dropout 확률을 곱해주어야 한다는 점이다. 혹은 test 시에는 기존 출력을 그대로 사용하고, train 할 때 dropout확률로 나눠주는 방법이다. (keras 에는 후자로 구현되어있는 듯 하다.) p.s. Batch Normalization 에 regularization 의 효과가 있으므로, 일반적으로 BN 과 dropout 을 같이 사용하지는 않는다. 하지만 dropout 에는 우리가 조절할 수 있는 dropout rate 이 있는 장점이 있다. 2-2. Data Augmentation신경망은 기본적으로 데이터가 많으면 많을 수록 학습에 유리한 이점이 있다. 따라서 우리가 가지고 있는 데이터를 조금씩 변형하여, 학습 데이터를 늘려주는 방법을 사용할 수 있다. horizontal flip이나, 사진에 일부분을 자르는 방법, Color jittering(이미지의 contrast, brightness 를 변형해준다) 2-3. Drop ConnectActivation 이 아닌, weight 을 확률적으로 0을 만들어 주는 방법 2-4. Fractional Max Pooling고정된 Pooling window 를 랜덤으로 정하는 방법. 자주 사용되지는 않는다. 3. Transfer Learning유명한 모델과 깊은 네트워크는 대부분 많은량의 데이터와 많은 하드웨어 Resource와 시간이 투입되서 구축된다. 우리가 이 모델구조를 가져와 우리의 목적에 맞게 사용하려고 보면, 우리가 가지고 있는 데이터와 자원의 한계로 학습에 실패하거나 그 시간이 매우 오래 걸리곤 한다. Transfer Learning 은 기존에 학습되어있는 모델 구조와 그 weight 을 가져와, 마지막 Fully Connected Layer 를 우리의 task 에 맞게 수정하고 이 부분만 학습하는 개념을 말한다. 4. ReferenceLecture 7 | Training Neural Networks II Syllabus | CS 231N","link":"/2019/07/22/CS231n-Lecture07-Summary/"},{"title":"[CS231n]Lecture09-CNN Architectures","text":"Lecture 09: CNN Architectures 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 이번 강의에서는 최초의 CNN 모델부터, ImageNet 대회에서 역대 좋은 스코어를 기록한 유명한 convnet 구조에 대해 살펴본다. 강의 summary 는 강의 내용 중심과 수업 중 궁금한 사항을 따로 찾아본 내용 위주로 정리한다. 해당 모델의 자세한 내용은 논문을 참조했다. 1. AlexNetLenet 이후, 가장 처음으로 나온 large scale convnet model 이다. Architecture 는 다음과 같다. 수업시간에는 CONV1 과 Max Pooling layer 에 대해서만 각 layer 의 output volume size 와 parameter 갯수를 확인했으나, 연습과 공부를 위해 전체 layer 에 대해 계산해본다. Architecture CONV1 MAX POOL1 NORM1 CONV2 MAX POOL2 NORM 2 CONV3 CONV4 CONV5 MAX POOL3 FC6 FC7 FC8 Input 이 227 x 227 x 3 image 가 들어갈 때, CONV1 (96 11x11 filter, stride 4)의 output volume size ? (227 - 11) / 4 + 1 = 55 이므로, 55 x 55 x 96 Conv1 의 weight 갯수 ? filter size 11 * 11 * 3 (channel) * 96 (filter 갯수) = 34,849개 MAX POOL (3 x 3 filter, stride 2) output volume size? (55 - 3) / 2 + 1 = 27 27 x 27 x 96 MAX POOL 의 weight 갯수? Pooling 은 weight 이 없으니까, 낚이지말자. (낚일 수 없는 낚시지..) NORM Layer 는? Normalization 만 해주는 것이므로 output size 는 27 x 27 x 96 으로 동일 CONV2 layer (256 5 x 5 filters, stride 1, padding 2) 의 output volume size 와 parameter 갯수? (27 - 5 + 4) / 1 + 1 = 27 output volume size : 27 x 27 x 256 parameter 갯수: 5 * 5 * 96 (channel) * 256 (filter 갯수 ) = 614,400 MAX POOL2 ( 3 x 3 filters, stride 2) output volume size? (27 - 3) / 2 + 1 = 13 output volume size: 13 x 13 x 256 CONV3 layer ( 384 3 x 3 filters, stride 1, padding 1)의 output volume size 와 parameter 갯수? (13-3+2) / 1 + 1 = 13 ouput volume size = 13 x 13 x 384 parameters: 13 * 13 * 256(channels) * 384 = 16,613,376 CONV4 layer (384 3x3 filters stride 1, padding 1) (13 - 3 + 2) / 1 + 1 = 13 output volume size = 13 x 13 x 384 parameters: 13 * 13 * 384(channels) * 384 = 24,920,064 CONV5 layer (256 3x3 filters stride 1 , padding 1) (13 - 3 + 2) / 1 + 1 = 13 output volume size = 13 x 13 x 256 parameters: 13 * 13 * 384 * 256 = 16,613,376 MAX POOL3 (3 x 3 filters strides 2) (13 - 3) / 2 + 1 = 6 output volume size: 6 x 6 x 256 AlexNet 의 특이점 당시 GPU 메모리의 부족으로, CONV1 layer 의 경우 depth 가 96이었으나, 48 개씩 (반반) 다른 GPU 에 올려져 계산이 진행되었다. 이는 서로가 데이터를 바라볼수 없다는 것을 의미한다. 마찬가지 의미로, CONV2, CONV4, CONV5의 경우 서로 다른 gpu 상에 올라가 있는 feature map 을 볼수 없다. 반면, CONV3, FC6, FC7, FC8 에서 서로 cross 됨으로써 feature map 을 바라볼수 없는 문제를 완화하였다 2. VGGVGG 는 AlexNet 과 비교하여, 조금더 깊은 convnet 을 가지고 있다. 이는 filter size 를 작게 가져감으로써 얻을 수 있는 이익이었다. 7x7 conv layer 1개와 3x3 conv layer 가 3개를 비교해본다. 7x7 conv layer 1개의 원본 이미지로부터 얻는 receptive field 는 7x7 영역이다. 3개의 3 x 3 conv layer가 쌓였을 때, 3번째 layer 입장에서, 원본이미지의 7 x 7 만큼의 receptive field, 즉 같은 양의 receptive field 를 얻을 수 있다. 같은 receptive field 영역을 커버하지만, layer 의 수가 증가함으로써, 더 많은 non-linearity feature map을 얻을 수 있다. 또한, 각 layer 의 parameter 수를 살펴 보면, 7x7 conv layer 는 C(이전 layer 의 channel 수)77C = 49 * C^2만큼의 parameter를 가지고 있고, 3 layer 3x3 conv layer 는 333CC = 9 * C^2 만큼의 parameter 를 가지고 있다. 작은 filter size로 layer 수를 늘리는 것이 *parameter 의 갯수** 관점에서도 큰 이득을 가져다 준다. cf) 네트워크가 깊어질수록 computation양을 일정하게 유지하기 위해 각 레이어의 입력을 downsampling 한다. &amp; Spatial Area 가 작아질수록 filter 의 depth 를 조금씩 늘려준다. 3. GoogLeNetGoogLeNet 의 특이점GoogleNet 은 깊은 신경망 모델에 대해, 계산량의 이점을 가져다주기 위해 고안되었다. 이는 Inception Module 을 설계하여, 이를 연속해 쌓는 방식의 구조이다. Inception Module 의 모양은 다음과 같다. 다음과 같이 구성할 때, conv layer 를 거친 output 을 depth 방향으로 concatenate 한다. spatial dimension 은 stride 등을 조절한다. feature map 을 뽑기 위해 각 layer 의 filter 갯수를 조절할텐데, 효과적인 feature map 을 뽑기 위해 filter 갯수를 늘리게되면, 전체 Inception Module 이 쌓이면 쌓일 수록, parameter 수가 지수배 증가하는 단점이 발생한다. 여기서 GoogleNet 의 핵심 idea가 등장하는 듯 하다. 1x1 convolution layer! 1x1 convolution layer를 통해 spatial dimension 은 보존하면서, depth 는 줄인다. 즉 이 convolution layer 를 bottle neck 이 되는 conv layer 앞단에 구성하여, 입력을 더 낮은 차원으로 보낸다. 또한 google net 은 parameter 가 많이 필요한 fc layer 를 제거하므로써 computational 이득을 취했다. 또한 Inception Module 이 쌓이면서, 깊게 쌓일 수록 loss 의 grdient 전파가 소실 되는 효과를 보완하기 위해 추가적인 classifier 를 곁가지에 닮으로써 gradient 를 추가적으로 update 한다. 4. ResNetResNet 은 conv layer 를 깊게 추가하는 것이 성능에 이득을 주는지에 대한 의문으로 시작한다. 즉, 답은 깊게 쌓는 것이 성능이 좋아지지 않는다는 것이다. 위 그림의 test error 그래프를 보더라도, 성능면에서 conv layer 의 증가가 좋은 성능을 가져다 주지 않는 것이 아니다. test error 그래프만 본다면, overfitting 된 것이 아닌가? 라는 생각을 할 수 있으나, training error 그래프를 보면, 학습 조차 잘 되지 않았음을 확인 할 수 있다. 즉, 깊이가 깊은 모델이 어느 순간부터는 얕은 모델보다 성능이 안좋아 질 수 있는 문제가 발생한다. 이 문제를 degradation 문제라고 한다. ResNet 의 특이점ResNet의 구조적 특이점은 바로 skip connection 이다. skip connection 은 이렇게, layer 의 입력과 출력이 더해져, 다음 layer 에 대한 입력으로 이루어 지는 구조를 의미한다.기존의 Neural Net 의 구조를 remind 해본다. 기존 뉴럴넷은 입력 x 가 들어갈 때, 출력 y 를 얻기 위한 H(x)를 찾아내는 과정이다. H(x)가 y 에 최대한 가깝게 하기 위한 즉, H(x) - y 가 0 이 될 수 있도록 최소화 과정을 거쳐 H(x) 를 찾아낸다. 이에 반해 ResNet 은 layer 를 거친 F(x) 와 x 가 더해진 F(x) + x 를 H(x)로 보고 이를 H(x) - x 를 최소화한다. 이는 residual 로 볼 수 있는 F(x)를 최소화한다는 의미이다. 5. ReferenceLecture 9 | CNN Architectures Syllabus | CS 231N","link":"/2019/07/26/CS231n-Lecture09-Summary/"},{"title":"[CS231n]Lecture06-Training Neural Networks part1","text":"Lecture 06: Training Neural Networks part1 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다. 1. One time setup1-1. Activation FunctionsActivation Function 마다 각 특성과 trade off 가 있다. 많이 사용되는 activation function 이 있지만, LU 계열의 activation function 은 모두 실험의 parameter가 될 수 있다. (1) Sigmoid $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ sigmoid function 의 경우, 수식의 특성상 실수 space 에 있는 값들을 [0, 1] 범위 내로 좁혀 주는 기능을 해, 역사적으로 오래된 activation function 이다. 하지만 다음과 같은 단점이 있다. 단점 일정 범위(Saturated 되는 범위)부터 gradient가 0에 가까워진다. sigmoid 의 출력 결과가 zero-centered 가 되지 않는다. minor 한 단점이지만, exponential 의 계산이 비효율적이다.(비싸다고 표현) 왜 zero-centered 가 중요한가? $$f(\\sum_iw_ix_i+b)$$ 강의에서 설명해준 직관적인 방법이 매우 도움이 되었다. Backpropagation 을 생각해 보게 되면, Neuron(Node) 안에서 local gradient 와 loss 에서 부터 오는 upstream gradient 가 곱해지게 된다. 위 식에서 w 에 대해 local gradient 를 구하면 x_i 가 되는 것을 확인 할 수 있다. 만약 xi 가 모두 양수라고 가정한다면, f()의 gradient 는 항상 양수 또는 음수이고, 이는 w 가 모두 같은 방향으로 움직인다는 것을 의미한다. 즉, 비효율적인 gradient update 라고 할 수 있다. (2) hyper tangent : Tanh(x) $$tanh(x)=\\frac{e^{2x}-1}{e^{2x}+1}$$ 위와 같은 zero-centered problem 을 해결하기 위해 tanh(x) 를 사용할 수 도 있다. (추가적으로, 수업시간에 언급은 없었으나, tanh의 등장배경을 설명하는 또다른 한가지는 sigmoid 와 tanh 의 gradient의 최댓값이 4배 차이가 나므로, backpropagation 에서 gradient vanishing 현상을 방지하는 기대효과로 설명하기도 한다.)하지만, 이 역시 sigmoid 처럼 saturate 되는 부분에서 gradient 가 0이 되는 현상이 아직 남아있다. (3) ReLU $$f(x) = max(0, x)$$ ReLU 의 경우, 가장 우리가 많이 볼 수 있는 activation function 이다. 다음과 같은 장점이 있다고 설명한다. 장점 양수인 구간에서는 gradient 가 0이 되지 않고 계산이 효율적이다. sigmoid 와 tanh 에 비해 실제로 6배 빠른 converge 성능을 보여준다. 단점 zero-centered output이 아니다. 0보다 작은 구간에서는 gradient 가 0이 된다. (4) Leaky ReLU $$f(x) = max(0.01x, x)$$ 0보다 작은 구간에서 ReLU 처럼 Saturate 되는 단점을 없애고자, 0보다 작은 구간에서 작은 gradient 를 주는 것이 Leary Relu 이다. 장점 양수 구간 뿐만아니라 음수 구간에서 gradient 를 작게 주어 gradient 가 0 이 되지 않게 한다. ReLU function 과 마찬가지로, 계산이 효율적이고 sigmoid 와 tanh 에 비해 빠르게 수렴한다. Parametric Rectifier(PReLU) 라는 이름으로, 좀더 generalize 된 형태도 사용한다. $max(\\alpha x, x)$ 형태로써, alpha 를 고정시키지 않고 학습시키는 형태이다. (5) Exponential Linear Units (ELU) $$f(x)=\\begin{cases}x \\quad\\quad\\quad\\quad if \\quad x&gt;0 \\\\alpha(exp(x) -1) \\quad if \\quad x\\leq 0 \\end{cases}$$ (6) Maxout Neuron ReLU function 의 Generalize 된 형태라고 생각할 수 있다. $$max(w_1^{T}x + b_1, w_2^Tx + b_2)$$ 하지만, 이 형태는 각 뉴런마다 두 배의 parameter를 가지고 그 output 값 간의 비교를 하므로, 연산량이 두배가 많아지는 단점이 있다. (6) Summary ReLU 를 사용한다! Leaky ReLU, Maxout, ELU 를 실험해볼 수 있다. tanh 도 실험할 수 있지만, 큰 효능을 기대하기 힘들다 Don’t Use sigmoid 1-2. Data Preprocessing앞서 살펴 보았던 것 처럼, 입력 데이터에 있어서 zero-centered 가 매우 중요한 preprocessing key 라고 생각 할 수 있다. 머신러닝에서 처럼 다양한 normalized 기법과 whitening 기법들이 있지만, 뉴럴넷, 특히 이미지 데이터에 대해서는 zero-centered 까지만 전처리 해준다. 이는 모든 차원의 데이터가 같은 범위안에 있게 함으로써 각 차원이 equally contribute 하게 하기 위함이다. 1-3. Weight Initialization우리가 설정하는 각 layer 의 weight 을 어떻게 초기화 해줄 것인가의 문제도 뉴럴넷의 학습과 성능에 영향이 있을 수 있다. 작은 gaussian random number 로 모든 weight 을 초기화 해 줄 경우, layer 를 지날 수록 activation function 을 거친 작은 output 과 초기화 된 작은 w 가 곱해져, 점차 그 분산이 작아 지는 것을 확인 할 수 있다. 따라서, 이런 가우시안 랜덤으로 초기화 해주는 것이 아닌 다른 초기화 방법이 등장하였다. Xavier Initialization &amp; He Initialization 개인적으로 Xavier Initialization 과 He Initialization 을 간단하게 실험해 본 내용은 나의 github에 올려두었다. 비교군 설정에 있어, 미흡하지만 직관적으로 이해하기 쉽게 실험해 본 내용이다. 간단한 api 를 통해 구현하였기에, 자세한 수식과 개념은 해당 논문을 읽어보고, 정리해봐야겠다. github: Xavier vs He experiment 1-4. Batch Normalization결국 우리는 모든 activation 이 unit gaussian form 이길 원한다. 위에서도 살펴 보았듯이, activation function 에 들어가기 전에 모든 fully connected layer 의 출력이 saturate 구간에 있게 하고 싶지 않은 것이다. 따라서, 주로 Batch Normalization 은 Fully Connected Layer 이후 비선형함수 전 또는 Convolution Layer 다음 에 들어가게 된다. 이 때, 주의 해야할 점은 Convolution Layer 다음에 들어가는 batch normalize 는 activation map 으로 나온 channel 별로 수행해주게 된다. batch mean 과 variance 는 각 차원별로 계산해 준다. Normalize $$\\hat{x}^{(k)} = \\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$ 앞서, batch normalization 의 목적은 network의 forward, backward pass 시 saturation 이 일어나지 않게 하기 위함이었습니다. 하지만 반면에, 이렇게 batch normalization 을 해준 이후에도 우리는 network 이 얼마나 해당 activation 을 얼마나 saturate 시킬지까지 학습 할 수 있다면 얼마나 좋을까요? 따라서 우리는 normalize 이후에 scaling factor 와 shifting factor 를 추가시켜, 얼마나 saturate 시킬지까지 학습할 수 있는 parameter 를 추가합니다. $$y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{k}$$ Batch Noramlization 에 대한 pseudo 알고리즘이다. 또한, 우리는 이렇게 학습한 batch mean 과 variance 를 학습 때 사용하며, testing (inference) 시에는 다시 계산하지 않는 것을 유의해야한다. testing 시에는 training 시 running average 등의 방식으로 고정된 mean과 variance 등을 사용할 수 있다. 2. Training Dynamics2-1. Babysitting the Learning Process Preprocess the data Build Model Sanity check for model (e.g. weigh이 작을ㄹ 때, loss가 negative log likelihood 와 비슷한지, regularization term이 추가될때 loss 가 증가하는지 등) (regularization term 을 사용하지 않고) 매우 작은 데이터에 대해서, train 을 돌렸을 때, loss 가 떨어지고, 금방 overfitting 되는지 확인 여기까지가 sanity check 이라면, 이제 본격적인 training!! 간단한 몇가지 실험을 통해 learning rate 을 정한다. 큰 값, 작은 값을 넣어보고, epoch 을 10까지 정도로 주었을 때, loss 가 주는 지 확인하여 대략적인 범위를 정한다. Coarse search: learning rate 과 다른 hyper parameter 를 uniform 등과 같은 distribution 을 통해 random search 한다. 강의에서 말했던, 주의사항 : 범위의 양 끝에 가장 좋은 score 가 뿌려져 있다면, 다시 범위를 설정해야한다. 내가 처음 설정한 범위 끝단에 존재한다면 그 주변에서 다시 최적의 값이 존재 할 수 있기 때문이다. Finer search: 최적의 값을 찾기 위해 세세하게 튜닝한다. 2-2. Hyperparameter Optimization 많은 양의 Cross Validation 을 통해 성능을 비교, 검증하며 최적의 hyper parameter 를 찾아야한다! Grid Search &lt;&lt; Random Search Random Sampling 을 통해 좀더 중요한 parameter 의 분포를 찾아 낼 수 있어, Random search 가 효과적 3. ReferenceLecture 6 | Training Neural Networks I Syllabus | CS 231N","link":"/2019/07/18/CS231n-Lecuture06-Summary/"},{"title":"Classification Metrics","text":"Classification Metrics: 분류 성능 지표Kaggle Classification 의 Evaluation 에 자주 사용되는 ROC-AUC를 정리해보면서, 이번 기회에 분류모델에서 자주 사용되는 성능지표(Metric)을 간단하게 정리해봅니다. Confusion Matrix 에서 비롯되는 Metric 들은 이를 이미지로 기억하는 것이 효율적입니다. Confusion Matrixconfusion matrix 는 해당 데이터의 정답 클래스(y_true) 와 모델의 예측 클래스(y_pred)의 일치 여부를 갯수로 센 표입니다. 주로, 정답 클래스는 행으로(row), 예측 클래스는 열로(columns) 표현합니다. (정의하기에 따라 다르지만) 일반적으로, class 1 을 positive로, class 0 을 negative 로 표기합니다. 우리가 예측한 클래스를 기준으로 Positive 와 Negative 를 구분합니다. 그리고 정답과 맞았는지, 틀렸는지를 알려주기 위해 True 와 False 를 각각 붙여줍니다. Accuracy: 정확도 전체 데이터에 대해 맞게 예측한 비율 $$\\frac{TP+TN}{TP+FN+FP+TN}$$ Precision: 정밀도 class 1 이라고 예측한 데이터 중, 실제로 class 1 인 데이터의 비율 $$\\frac{TP}{TP+FP}$$ 우리의 모델이 기본적인 decision이 class 0 이라고 생각할 때, class 1 은 특별한 경우를 detect 한 경우 일 것입니다. 이 때 class 1 이라고 알람을 울리는 우리의 모델이 얼마나 세밀하게 class를 구분 할 수 있는지의 정도를 수치화 한 것입니다. Recall: 재현율 실제로 class 1 인 데이터 중에 class 1 이라고 예측한 비율 = Sensivity = TPR $$\\frac{TP}{TP+FN}$$ 제가 기억하는 방식은, 자동차에 결함이 발견되서 recall 이 되어야 하는데 (실제 고장 데이터중) 얼마나 recall 됐는지로 생각합니다. Fall-out: 위양성율 실제로 class 1 이 아닌 데이터 중에 class 1이라고 예측한 비율 낮을 수록 좋음 = FPR = 1 - Specificity Specificity = 1 - Fall out $$\\frac{FP}{FP+TN}$$ 실제로 양성데이터가 아닌 데이터에 대해서 우리의 모델이 양성이라고 잘못 예측한 비율을 말합니다. 억울한 데이터의 정도를 측정했다고 생각 할 수 있습니다. 각 Metric 간의 상관관계우리 모델의 decision function 을 f(x) 라 할 때, 우리는 f(x)의 결과와 threshold (decision point)를 기준으로 class를 구분합니다. Recall vs Fall-out : 양의 상관관계 Recall은 위의 정의에 의하듯이, 실제로 positive 인 클래스에 대해 얼마나 positve 라고 예측했는지의 비율입니다. 우리가 Recall 을 높이기 위해서는 고정되어있는 실제 positive 데이터 수에 대해 예측하는 positive 데이터의 갯수 threshold 를 낮춰 늘리면 됩니다. 이에 반해 threshold 를 낮추게 되면, 실제로 positive 가 아닌 데이터에 대해 positive 라고 예측하는 억울한 데이터가 많아지므로 Fall-out 은 커지게 되고 둘은 양의 상관관계를 갖게 됩니다. Recall vs Precision : 대략적인 음의 상관관계 위에 설명한 것처럼 threshold 를 낮춰 우리가 예측하는 positive 클래스의 숫자를 늘리게 되면, recall 은 높아지는 반면, 예측한 positive 데이터 중 실제 positive 데이터의 비율은 작아 질 수 있습니다. F-beta score precision 과 recall의 가중 조화평균 $$(\\frac{1}{1+\\beta^2}\\frac{1}{precision} + \\frac{\\beta^2}{1+\\beta^2}\\frac{1}{recall})^{-1}$$ 이처럼, 다양한 Metric 에 대해 우리가 초점을 맞추는 것에 따라 모델의 성능은 다르게 바라 볼 수 있습니다. 따라서 모델에 대해 성능을 평가하고 최종 모델을 선택함에 있어, 서로 다른 Metric 을 동시에 비교해야합니다. 이를 위해 precision 과 recall 을 precision 에 beta^2 만큼 가중하여 바라보는 스코어가 F beta score 입니다. 이 중 beta=1 일 때, score 가 우리가 자주 보는 f1 score 입니다. $$F_1=\\frac{2precisionrecall}{precision+recall}$$ ROC Curve: Receiver Operator Characteristic Curve Recall vs Fallout 의 plot (TPR vs FPR) 위의 예시 처럼, 우리가 클래스를 판별하는 기준이 되는 threshold (decision point) 를 올리거나 내리면서, recall 과 fall out 은 바뀌게 됩니다. 이렇게 threshhold 를 변화 해 가면서, recall 과 fall out 을 plotting 한 것이 ROC curve 입니다. sklearn.metrics.roc_curve() 의 documentation (ROC-)AUC: Area Under Curve 위에서 그린 ROC Curve 의 넓이를 점수로써 사용하는 것이 AUC 입니다. AUC 의 유의미한 범위는 class 를 50%의 확률로 random 하게 예측한 넓이인 0.5 보다는 클 것이고, 가장 최대의 AUC 의 넓이는 1 일 것이므로 0.5≤AUC≤1 의 범위를 갖는 score 입니다. ROC 커브와 AUC score 를 보고 모델에 대한 성능을 평가 하기 위해서, ROC 는 같은 Fall out 에 대해 Recall 은 더 높길 바라고, 같은 Recall 에 대해서는, Recall 이 더 작길 바랍니다. 결국, 그래프가 왼쪽 위로 그려지고, AUC 즉 curve 의 넓이는 커지는 것이 더 좋은 성능의 모델이라고 볼 수 있습니다.","link":"/2019/06/03/Classification-Metrics/"},{"title":"[Book] Summary of Digital Image Processing Chapter 03","text":"Chapter 3: Intensity Transformations and Spatial FilteringRafael C.Gonzalez and Richard E.Woods. Digital Image Processing. PEARSON 을 다시 한번 읽어보며, 개인적으로 정리한 글입니다. 3.1 Background3.1.1 The Basics of Intensity Transformations and Spatial Filtering 모든 spatial domain process 에서 operator 와 input image f(x,y), output image g(x, y) 로 표현된다. $$g(x, y) = T[f(x, y)]$$ 3.2 Some Basic Intensity Transformation Functions3.2.1 Image Negatives Negative transformation 이미지 반전 효과 $$s = L-1-r$$ 3.2.2 Log Transormations$$s=c*log(1+r)$$ c : constant, assumed r ≥ 0 low intensity value of input → wider range of output levels 3.2.3 Power-Law(Gamma) Transformations$$s = cr^\\gamma=c(r+\\epsilon)^\\gamma$$ c, gamma: positive constants gamma 의 값에 따라서, intensity의 어떤 부분이 강조되는지가 다르다. Original Image 와 monitor 에 비추는 이미지간의 차이를 중요하게 다룰 때, gamma correction 을 사용하게 된다. 일반적인 contrast를 다룰 때 중요하게 사용되기도 한다. MRI 사진 sample 예 3.2.4 Piecewise-Linear Transformation Functions1. Contrast stretching Low-contrast image 은 sensing 면에서 떨어질 수 있으므로, intensity range를 연장하여, 큰 범위의 intensity 를 사용할 수 있도록 하는 방법 (input_intensity, output_intensity)→ (r1, s1) 과 (r2, s2)의 관계를 조절하여 contrast transform 형태를 조절한다. 극단적으로 r1=r2, s1=0, s2=L-1 이면, binary image 를 생성한다.(thresholding function) 2. Intensity-level slicing 관심있는 영역의 Intensity-level 외에는 모두 0으로 처리하거나, 관심있는 영역은 특정 intensity level 로 두고, 나머지 level 은 그대로 두는 형태등이 있을 수 있다. 3. Bit-plane slicing 8bit 의 이미지 슬라이드 중, significant order의 bit slide 중 특정 부분을 slicing 3.3 Histogram Processing3.3.1 Histogram Equalization Assume monotonic transformation: one-to-one mapping or many-to-one mapping output pdf ps, input pdf pr $$p_s(s) = p_r(r)|\\frac{dr}{ds}|$$ 이 식은? $$s=T(r)=(L-1)\\int_{0}^{r}{p_r(w)dw}$$ histogram equalization, histogram linearization $$s_k=T(r_k)=(L-1)\\sum_{j=0}^{k}p_r(r_j) = \\frac{L-1}{MN}\\sum_{j=0}^{k}{n_j}$$ output image 의 p_s, distribution 이 uniform 되게 하는 형태 결과적으로, 같은 이미지 형태이지만, 밝기와 contrast 가 모두 다르더라도 일정한 historgram 이 되도록 transform 해준다. 3.3.2 Histogram Matching(Specifiaction) Uniform historgram이 항상 좋은 것은 아니다. histogram matching, histogram specification : The method used to generate a processed image that has specified histogram (1) histogram p_r(r) 을 계산, s_k 를 구하기 위해 histogram equalization transformation 식을 계산 (2) 원하는 p_z(z) 를 이용하여, function G 를 계산 (3)(2)에서 계산한 G를 이용해 z_k (k=0, 1, 2, …L-1) 까지 계산 (4) inverse of G 계산 (5) r → z trasnformation 계산 3.3.3 Local Histogram Processing 이 전 두가지 histogram processing model 은 global 한 영역에서 진행, 즉 전체 이미지에 대해 intensity distribution 을 확인하고, 이를 이용해 transformation 을 진행 하였다. 이러한 modeling 스킬을 Local enhancement 에 사용가능 Local enhancement 는 neighborhood 를 정하고, 중심을 이동해가며, neighborhood 안에서의 histogram equalization 이나 histogram specification transformation 등을 이용할 수 있다. 계산을 줄이기 위해, nonoverlapping region 을 사용하여 위 방법을 동일하게 사용할 수 있으나, blocky effect 를 발생시킬 수 있음 blocky effect (p.161) 3.3.4 Using Histogram Statistics for Image Enhancement3.4 Fundamentals of Spatial Filtering3.4.1 The mechanics of Spatial FIltering Neighborhood (typically a small rectangle) 2. predefined operation 3.4.2 Spatial Correlation and Convolution correlation, convolution: 180 degree 반전 3.4.3 Vector Representation of Linear Filttering$$R=\\sum_{k=1}^{9}w_kz_k=\\boldsymbol{w}^T\\boldsymbol{z}$$ 3.4.4 Generating Spatial Filter Masks mn mask Average value of masked values → image smoothing 위치에 따른 gaussian filter 예 (standard deviation?(p.173)) 3.5 Smoothing Spatial Filters smoothing for blurring, noise reduction blurring : removal of small details from an image prior to object extraction, bridging of small gaps in lines or curves noise reduction: blurring by linear or non-linear filtering 3.5.1 Smoothing Linear Filters averaging filters == lowpass filters Replace the value of every pixel in an image by the average of the intensity levels in neighborhood defined by the filter mask → reduced “sharp” transitions in intensity box filter : all coefficient are euqal in filter 3.5.2 Order-Statistic(Nonlinear) Filters median filter (popular): effective in impulse noise-reduction (salt-pepper noise) implement sort values of neighborhood determine their median assign the value to filtered image 3.6 Sharpening Spatial Filters highlight transitions 3.6.1 Foundation3.6.2 Using the Second Derivative for Image Sharpeing—The Laplacian capture intensity discontinuities in an image and deemphasize $$g(x, y)=f(x, y) + c[\\triangledown^2f(x,y)]$$ 3.6.3 Unsharp Masking and Highboost Filtering unsharp masking Blur the original image Subtract the blurred image from the original (the resulting difference is called the mask) Add the mask to the original $$g_{mask}(x,y)=f(x,y)-\\bar{f}(x,y)$$ $$g(x,y) = f(x, y) + k*g_{mask}(x, y), \\quad k&gt;=0$$ k&gt;1 , highboost filtering k&lt;1, de-emphasizes unsharp mask 3.6.4 Using First-Order Derivatives for (Nonlinear) Image Sharpening—The gradient Using the magnitude of the gradient, magnitude M(x, y) $$M(x, y)=mag(\\triangledown f)=\\sqrt{g_x^2+g_y^2}=|g_x|+|g_y|$$","link":"/2019/08/26/DIP-chapter3/"},{"title":"Generative Model","text":"확률론적 생성모형 (Generative Model)확률론적 생성모형의 기본 개념 우리가 궁금한 것은 $ x $ (features) 들이 있을 떄, $ y $가 어떤 Class 인지 맞추는 것이다. 맞추기 위해서는 training set 으로 부터 각 Class 마다 $ P(y \\mid x) $의 모형을 알아내야 한다. 베이즈 정리를 이용하여, $ y=C_k $일 때의 조건부 확률을 구할 수 있다.$$P(y = C_k \\mid x) = \\dfrac{P(x \\mid y = C_k); P(y = C_k)}{P(x)}$$ 이 때, $ P(x \\mid y = C_k) $== Likelihood== y가 k라는 클래스 일 때, x의 확률을 구하는 것이 관건 Likelihood 추정의 알고리즘 $ P(x \\mid y = C_k) $ 가 ** 어떤 ** 확률분포를 따를 것이다라고 가정 x의 특성에 따라 우리가 알고 있는 특정 확률분포를 따른다! 라고 가정 Class k 를 만드는 data들을 통해 이 확률분포의 모수값을 구한다. 모수값을 안다 == $ P(x \\mid y = C_k) $ 의 pdf 를 알고 있다. test set x 가 들어오면 $ P(x \\mid y = C_k) $ 를 구할 수 있다.","link":"/2018/12/05/Generative-Model/"},{"title":"Linear Model with Pytorch","text":"Linear Model with Pytorch 이 글의 목적은, 지난 Linear Regression 에서 좀더 나아가서, 다양한 Regression 예제들을 Linear Model (WX) 형태로 pytorch 를 이용해 풀어 보는 것입니다. Pytorch 를 사용하여 Modeling 과 loss function 등을 class 형태, 내장 loss 함수등을 사용해보겠습니다. 12345678910import torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Fimport numpy as npimport warningswarnings.filterwarnings(\"ignore\")%config InlineBackend.figure_format = 'retina'%matplotlib inline 1. Quadratic Regression Model$$f(x) = w_0 + w_1x + w_2x^2$$ 12345678x = np.linspace(-10, 10, 100)y = x**2 + 0.7 * x + 3.0 + 20 * np.random.rand(len(x))plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 12345x_train = torch.FloatTensor([[each_x**2, each_x, 1] for each_x in x])y_train = torch.FloatTensor(y)print(\"x_train shape: \", x_train.shape)print(\"y_train shape: \", y_train.shape) x_train shape: torch.Size([100, 3]) y_train shape: torch.Size([100])1234567891011121314151617W = torch.zeros(3, requires_grad=True)optimizer = optim.SGD([W], lr=0.0001)epochs = 10000for epoch in range(1, epochs + 1): hypothesis = x_train.matmul(W) loss = torch.mean((hypothesis - y_train) ** 2) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch: {} -- Parameters: W: {} -- loss {}\".format(epoch, W.data, loss.data)) epoch: 1000 -- Parameters: W: tensor([1.1738, 0.4943, 1.0699]) -- loss 85.30189514160156 epoch: 2000 -- Parameters: W: tensor([1.1581, 0.4949, 2.0311]) -- loss 76.05414581298828 epoch: 3000 -- Parameters: W: tensor([1.1437, 0.4949, 2.9105]) -- loss 68.31205749511719 epoch: 4000 -- Parameters: W: tensor([1.1305, 0.4949, 3.7151]) -- loss 61.83049011230469 epoch: 5000 -- Parameters: W: tensor([1.1185, 0.4949, 4.4514]) -- loss 56.4041862487793 epoch: 6000 -- Parameters: W: tensor([1.1075, 0.4949, 5.1250]) -- loss 51.86140060424805 epoch: 7000 -- Parameters: W: tensor([1.0974, 0.4949, 5.7414]) -- loss 48.058231353759766 epoch: 8000 -- Parameters: W: tensor([1.0882, 0.4949, 6.3054]) -- loss 44.8742790222168 epoch: 9000 -- Parameters: W: tensor([1.0798, 0.4949, 6.8214]) -- loss 42.20869445800781 epoch: 10000 -- Parameters: W: tensor([1.0721, 0.4949, 7.2935]) -- loss 39.9770965576171912345plt.plot(x, y, 'o', label='train data')plt.plot(x, (x_train.data.matmul(W.data).numpy()), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show() 2. Cubic Regression Model$$f(x) = w_0 + w_1x + w_2x^2 + w_3x^3$$ 2.1 Generate Toy data 100개의 data 를 생성합니다. 12x = np.linspace(-1, 1, 100)y = 3*x**3 - 0.2 * x ** 2 + 0.7 * x + 3 + 0.5 * np.random.rand(len(x)) 12345plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 2.2 Define Model x_train과 y_train 을 만들어줍니다. 123x_train = torch.FloatTensor([[xval**3, xval**2, xval, 1]for xval in x])y_train = torch.FloatTensor([y]).view(100, -1)y_train.shape torch.Size([100, 1]) 이번에 Model을 nn.Module 추상 클래스를 상속 받아, class 형태로 모델링 해보겠습니다. 1234567class CubicModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4, 1) def forward(self, x): return self.linear(x) 1model = CubicModel() train 시킬 때, loss 역시 nn.functional 에 있는 내장 mse loss 를 사용하여 보겠습니다. 1234567891011121314151617optimizer = optim.SGD(model.parameters(), lr=0.001)epochs = 15000for epoch in range(1, epochs + 1): hypothesis = model(x_train) # define loss loss = F.mse_loss(hypothesis, y_train) # Backprop &amp; update parameters optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1500 == 0: print(\"epoch: {} -- loss {}\".format(epoch, loss.data)) epoch: 1500 -- loss 0.22306101024150848 epoch: 3000 -- loss 0.11560291796922684 epoch: 4500 -- loss 0.09848319739103317 epoch: 6000 -- loss 0.08879078179597855 epoch: 7500 -- loss 0.08104882389307022 epoch: 9000 -- loss 0.07452096790075302 epoch: 10500 -- loss 0.06889640539884567 epoch: 12000 -- loss 0.06398065388202667 epoch: 13500 -- loss 0.05964164435863495 epoch: 15000 -- loss 0.05578556656837463412345plt.plot(x, y, 'o', label='train data')plt.plot(x, model(x_train).data.numpy(), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show() 3. Exponential Regression Model$$f(x) = e^{w_0x}$$ $$g(x) = \\ln f(x) = w_0x$$ Exponential 의 경우, Linear Model 형태를 만들어 주기 위해, log 를 씌워 주워 train 을 시킨후, 다시 exponential 을 양변에 취해주는 형태로 modeling 을 하여야 한다. 3.1 Generate Toy data123np.random.seed(20190505)x = np.linspace(-1, 1, 50)y = np.exp(2 * x) + 0.2 * (2 * np.random.rand(len(x)) - 1) 12345plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 3.2 Define Model123x_train = torch.FloatTensor([[xval, 1] for xval in x])y_train = torch.FloatTensor([np.log(y)]).view(50, -1)y_train.shape torch.Size([50, 1])1234567class ExpModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(2, 1) def forward(self, x): return self.linear(x) 이번에는 optimize algorithm 중 Adam 을 사용해 보겠습니다. Adam 은 adaptive 하게 learning rate 를 조정해 주는 algorithm 입니다. 1234567891011121314151617model = ExpModel()optimizer = optim.Adam(model.parameters())epochs = 15000for epoch in range(1, epochs + 1): hypothesis = model(x_train) loss = F.mse_loss(hypothesis, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch: {} -- loss {}\".format(epoch, loss.data)) epoch: 1000 -- loss 1.4807509183883667 epoch: 2000 -- loss 0.6568859815597534 epoch: 3000 -- loss 0.2930431365966797 epoch: 4000 -- loss 0.1839657723903656 epoch: 5000 -- loss 0.1683545857667923 epoch: 6000 -- loss 0.16775406897068024 epoch: 7000 -- loss 0.16775156557559967 epoch: 8000 -- loss 0.16775153577327728 epoch: 9000 -- loss 0.16775155067443848 epoch: 10000 -- loss 0.16775155067443848 epoch: 11000 -- loss 0.16775155067443848 epoch: 12000 -- loss 0.16775155067443848 epoch: 13000 -- loss 0.16775153577327728 epoch: 14000 -- loss 0.1677515208721161 epoch: 15000 -- loss 0.167751520872116112345plt.plot(x, y, 'o', label='train data')plt.plot(x, np.exp(model(x_train).data.numpy()), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show() 4. Sine &amp; Cosine Regression$$f(x) = w_0\\cos(\\pi x) + w_1\\sin(\\pi x)$$ 4.1 Generate Toy data12x = np.linspace(-2, 2, 100)y = 2 * np.cos(np.pi * x) + 1.5 * np.sin(np.pi * x) + 2 * np.random.rand(len(x)) - 1 12345plt.plot(x, y, 'o')plt.grid()plt.xlabel('x')plt.ylabel('y')plt.show() 4.2 Modeling123x_train = torch.FloatTensor([[np.cos(np.pi*xval), np.sin(np.pi*xval), 1] for xval in x])y_train = torch.FloatTensor(y).view(100, -1)y_train.shape torch.Size([100, 1])1234567class SinCosModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3, 1) def forward(self, x): return self.linear(x) 1234567891011121314151617model = SinCosModel()optimizer = optim.Adam(model.parameters())epochs = 10000for epoch in range(1, epochs + 1): hypothesis = model(x_train) loss = F.mse_loss(hypothesis, y_train) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 1000 == 0: print(\"epoch: {} -- loss {}\".format(epoch, loss.data)) epoch: 1000 -- loss 1.1333037614822388 epoch: 2000 -- loss 0.45972707867622375 epoch: 3000 -- loss 0.36056602001190186 epoch: 4000 -- loss 0.3566252291202545 epoch: 5000 -- loss 0.3566077649593353 epoch: 6000 -- loss 0.3566077947616577 epoch: 7000 -- loss 0.3566077649593353 epoch: 8000 -- loss 0.3566077947616577 epoch: 9000 -- loss 0.3566077947616577 epoch: 10000 -- loss 0.356607764959335312345plt.plot(x, y, 'o', label='train data')plt.plot(x, model(x_train).data.numpy(), '-r', linewidth=3, label='fitted')plt.grid()plt.legend()plt.show()","link":"/2019/05/04/Linear-Model-with-Pytorch/"},{"title":"[Lecture] 딥러닝을 이용한 자연어 처리 Section A, B","text":"Section A, B - Summary이 글은 edwith(https://www.edwith.org/)의 조경현 교수님의 딥러닝을 이용한 자연어 처리 (https://www.edwith.org/deepnlp/joinLectures/17363)강의를 듣고 정리한 글입니다. Section A. Introduction 알고리즘의 정의 : 문제를 해결하기 위한 instruction 의 sequence Machine Learning Algorithm 문제 정의 (optional) : 문제를 specific 하게 정의 하는 것 자체가 쉽지 않다. ex) 얼굴을 detection 할 때, 어떤 범위까지 얼굴이라고 정의할 것인지 Example들이 주어진다 → 데이터들 문제를 해결 할 수 있는 Train된 Machine Learning Model Section B. Basic Machine Learning: Supervised Learning0. Supervised Learning 제공되는 것들: N 개의 pair 로 된 training set $$D = {(x_1, y_1), …, (x_N, y_N)}$$ Data 별 loss function Loss function 은 필요에 따라서 우리가 디자인 해야 할 때도 있다. $$l(M(x), y) \\geq 0$$ Evaluation sets: Validation set과 test set 기존에 보지 못한 dataset 에도 trained model 이 잘 작동 하는지 확인하는 것이 필수 우리가 결정해야 하는 것들: Hypothesis sets: H1, H2 … : 모델, hyper parameter들이 다른 모델, 여러가지 실험 해보고 싶은 것들이 될 수 있다. 가설을 잘 설정하는 것이 최종의 모델을 결정하는데 중요한 기초 작업 Optimization Algorithm 어떤 방법론으로 최적화를 진행 할지 역시 매우 중요한 문제 결국, 우리가 해야 하는 것은 주어진 Training set 에서, hypothesis set 안의 각 Hm 마다 가장 좋은 모델을 찾는다. Trained Hm 중에서, Validation set 에서 가장 좋은 한 가지 모델을 결정한다. Reporting 을 위해 test set 에서 얼마나 좋은 성능을 나타내는지 확인한다. [Three points to Consider both in research and in practice]1. 어떻게 Hypothesis set 을 설정하는가? Hypothesis Set 자체가 infinite 하다는 문제 Neural Network 에서 국한되서 보자면, 어떤 Network Architecture 를 사용하여 모델을 구성할 것인지, 각 모델마다 hyper parameter 를 어떻게 설정할 것인지 등 hypothesis set 이 매우 다양하다. 이 중, 좋은 한가지 모델을 한가지 찾는 방법이 어렵다. 강의 표현 중 이를 찾는 것은 Science ——— Magic 사이에 어느 한 점인, 거의 Art 에 가깝다고 하셔서 매우 웃겼다. 개인적으로 이 부분이 가장 공부하면서도 어렵고, 그 모델을 찾는 것이 매우 추상적인 느낌이다. 그리고 개인적으로 진행하는 프로젝트에서 과연 내가 찾은 모델보다 더 좋은 성능을 가지는 모델혹은 파라미터는 없을까(무조건 있을 것인데..라고 생각하는 경우가 훨씬 많지만..)라고 생각하며 분석과 모델링의 열정을 높이곤한다. Network Architectures Neural Network 는 Directed Acyclic Graph이다!! Inference : Forward Computaion 만으로, 쉽게 trained neural network를 사용할 수 있다. 이를 구성하는데 있어, high-level 로 abstraction 된 라이브러리를 oop , functional programming 을 활용해 쉽게 구현할 수 있다. (pytorch, tensorflow…) 2. Loss Function 관점의 이동: 어떤 주어진 data x 에 대하여 y 는 무엇일까? 를 생각하는 모델이 아니라, $$f_\\theta(x) = ? $$ 주어진 x 에 대해 y가 어떤 case 혹은 값일 확률로 생각한다. $$p(y=y’|x) = ?$$ Distribution based loss functions Binary Classification : Bernoulli distribution → Sigmoid Multiclass Classification : Categorical distribution → Softmax Linear Regression : Gaussian distribution Multimodal linear Regression : Mixture of Gaussians 결국 Loss Function 은 다음과 같이 표현될 수 있다. Maximize logp $$argmax_\\theta \\sum_{n=1}^{N} logp_\\theta(y_n|x_n)$$ = minimize L(theta): $$L(\\theta) = \\sum_{n=1}^{N} l(M_\\theta(x_n), y_n)= -\\sum_{n=1}^{N} logp_\\theta(y_n|x_n)$$ 3. Optimization Optimization 방법 Optimization 방법에는 GD, SGD, Newton Method 등 다양한 방법이 있지만, Nerual Network 에서는 Gradient Descent 방법을 위주로 사용한다. 이 강의에서는 다루지 않았지만, Gradient Descent 방법 외의 다른 알고리즘들은 전제되는 가정들이 많고, Nerual Network 의 고차원에서는 그 가정을 만족하기가 쉽지 않다. (예를 들어, Newton Method 에서의 Hessian 행렬이 구해지기 위한 가정, computation 양 또한 매우 많다.) 이런 이유에서 Gradient Descent 방법을 사용한다. Backward Computation : Backpropagation Loss function 의 gradient 를 구하는 방법은 쉽지 않다. Neural Network는 Automatic differentiation (Autograd) 를 사용하여, weight 과 bias term 에 대해 쉽게(?) gradient 값을 구할 수 있다. library 덕분에 우리가 loss function 의 gradient 를 직접 구하지 않아도 된다.","link":"/2019/05/03/Lecture-딥러닝을-이용한-자연어-처리-Section-A-B/"},{"title":"Linear Regression with Pytorch","text":"Linear Regression through Pytorch 이번 포스트의 목적은 Linear Model을 Pytorch을 통해 구현해보며, 개인적으로 Pytorch의 사용을 연습하며 적응력을 높여보는 것입니다. Import Library12345678910import torchimport torch.optim as optimimport matplotlib.pyplot as pltimport numpy as npimport warningswarnings.filterwarnings(\"ignore\")%config InlineBackend.figure_format = 'retina'%matplotlib inline Generate Toy Data$ y = \\frac{1}{3} x + 5 $ 와 약간의 noise 를 합쳐 100 개의 toy data를 만들겠습니다. 12345# Target Functionf = lambda x: 1.0/3.0 * x + 5.0x = np.linspace(-40, 60, 100)fx = f(x) 123plt.plot(x, fx)plt.grid()plt.show() 123456# y_train data with little noisey = fx + 10 * np.random.rand(len(x))plt.plot(x, y, 'o')plt.grid()plt.show() 1. Gradient Descent Model (hypothesis) 를 설정합니다.(여기선, Linear Regression 이므로, $y = Wx + b$ 형태를 사용합니다.) Loss Function 을 정의합니다. (여기선, MSE loss 를 사용하겠습니다.) gradient 를 계산합니다.(여기선, Gradient Descent 방법으로 optimize 를 할 것이므로, optim.SGD() 를 사용합니다.) parameter 를 update 합니다. 1234x_train = torch.FloatTensor(x)y_train = torch.FloatTensor(y)print(\"x_train Tensor shape: \", x_train.shape)print(\"y_train Tensor shape: \", y_train.shape) x_train Tensor shape: torch.Size([100]) y_train Tensor shape: torch.Size([100])1234567891011121314151617181920212223242526# train code# parameter setting &amp; initializeW = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)# optimizer settingoptimizer = optim.SGD([W, b], lr=0.001)# total epochsepochs = 3000for epoch in range(1, epochs + 1): # decide model(hypothesis) model = W * x_train + b # loss function -&gt; MSE loss = torch.mean((model - y_train)**2) optimizer.zero_grad() loss.backward() optimizer.step() # 10 epoch 마다 train loss 를 출력합니다. if epoch % 500 == 0: print(\"epoch: {} -- Parameters: W: {} b: {} -- loss {}\".format(epoch, W.data, b.data, loss.data)) epoch: 500 -- Parameters: W: tensor([0.3709]) b: tensor([5.6408]) -- loss 22.728315353393555 epoch: 1000 -- Parameters: W: tensor([0.3467]) b: tensor([7.9427]) -- loss 11.399767875671387 epoch: 1500 -- Parameters: W: tensor([0.3368]) b: tensor([8.8829]) -- loss 9.51008415222168 epoch: 2000 -- Parameters: W: tensor([0.3327]) b: tensor([9.2669]) -- loss 9.194862365722656 epoch: 2500 -- Parameters: W: tensor([0.3311]) b: tensor([9.4237]) -- loss 9.142287254333496 epoch: 3000 -- Parameters: W: tensor([0.3304]) b: tensor([9.4878]) -- loss 9.13351631164550812345plt.plot(x, y, 'o', label=\"train data\")plt.plot(x_train.data.numpy(), W.data.numpy()*x + b.data.numpy(), label='fitted')plt.grid()plt.legend()plt.show() 2. Stochastic Gradient Descent Model (hypothesis) Setting Loss Function Setting 최적화 알고리즘 선택 shuffle train data mini-batch 마다 W, b 업데이트 12345678910111213141516# batch 를 generate 해주는 함수def generate_batch(batch_size, x_train, y_train): assert len(x_train) == len(y_train) result_batches = [] x_size = len(x_train) shuffled_id = np.arange(x_size) np.random.shuffle(shuffled_id) shuffled_x_train = x_train[shuffled_id] shuffled_y_train = y_train[shuffled_id] for start_idx in range(0, x_size, batch_size): end_idx = start_idx + batch_size batch = [shuffled_x_train[start_idx:end_idx], shuffled_y_train[start_idx:end_idx]] result_batches.append(batch) return result_batches 12345678910111213141516171819# trainW = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)optimizer = optim.SGD([W, b], lr=0.001)epochs = 10000for epoch in range(1, epochs + 1): for x_batch, y_batch in generate_batch(10, x_train, y_train): model = W * x_batch + b loss = torch.mean((model - y_batch)**2) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(\"epoch: {} -- Parameters: W: {} b: {} -- loss {}\".format(epoch, W.data, b.data, loss.data)) epoch: 500 -- Parameters: W: tensor([0.0890]) b: tensor([9.5399]) -- loss 162.1055450439453 epoch: 1000 -- Parameters: W: tensor([0.3672]) b: tensor([9.5366]) -- loss 12.424881935119629 epoch: 1500 -- Parameters: W: tensor([0.3560]) b: tensor([9.5097]) -- loss 7.826609134674072 epoch: 2000 -- Parameters: W: tensor([0.3375]) b: tensor([9.5556]) -- loss 13.15934944152832 epoch: 2500 -- Parameters: W: tensor([0.2462]) b: tensor([9.5157]) -- loss 11.582895278930664 epoch: 3000 -- Parameters: W: tensor([0.3097]) b: tensor([9.5111]) -- loss 9.991677284240723 epoch: 3500 -- Parameters: W: tensor([0.2497]) b: tensor([9.5532]) -- loss 20.481367111206055 epoch: 4000 -- Parameters: W: tensor([0.4388]) b: tensor([9.5390]) -- loss 20.827198028564453 epoch: 4500 -- Parameters: W: tensor([0.1080]) b: tensor([9.4959]) -- loss 140.0277862548828 epoch: 5000 -- Parameters: W: tensor([0.3188]) b: tensor([9.4829]) -- loss 6.635367393493652 epoch: 5500 -- Parameters: W: tensor([0.2553]) b: tensor([9.5017]) -- loss 25.45773696899414 epoch: 6000 -- Parameters: W: tensor([0.2490]) b: tensor([9.5489]) -- loss 9.580666542053223 epoch: 6500 -- Parameters: W: tensor([0.3189]) b: tensor([9.5347]) -- loss 12.585128784179688 epoch: 7000 -- Parameters: W: tensor([0.3026]) b: tensor([9.4874]) -- loss 8.298829078674316 epoch: 7500 -- Parameters: W: tensor([0.3507]) b: tensor([9.6815]) -- loss 13.348054885864258 epoch: 8000 -- Parameters: W: tensor([0.1423]) b: tensor([9.5220]) -- loss 32.567440032958984 epoch: 8500 -- Parameters: W: tensor([0.7147]) b: tensor([9.5182]) -- loss 75.97190856933594 epoch: 9000 -- Parameters: W: tensor([0.5170]) b: tensor([9.5289]) -- loss 39.07848358154297 epoch: 9500 -- Parameters: W: tensor([0.3748]) b: tensor([9.5590]) -- loss 10.358983993530273 epoch: 10000 -- Parameters: W: tensor([0.2958]) b: tensor([9.6088]) -- loss 7.410649299621582 Stochasitic 하게 loss의 gradient 를 계산하여, parameter update를 하므로, loss 가 굉장히 oscilation 이 나타나며 감소하는 것을 볼 수 있다. 12345plt.plot(x, y, 'o', label=\"train data\")plt.plot(x_train.data.numpy(), W.data.numpy()*x + b.data.numpy(), label='fitted')plt.grid()plt.legend()plt.show() 12","link":"/2019/05/03/Linear-Regression-with-Pytorch/"},{"title":"Mysql","text":"MySQL1. Install MySQL https://dev.mysql.com/downloads/mysql/5.7.html#downloads 에서DMG 파일 다운로드 시스템 환경설정에서 MySQL -&gt; Start MySQL Server : server 시작 1$ cd usr/local/mysql/bin 위 경로에서 MySQL 서버에 접속1$ sudo ./mysql -p 1mysql &gt; 패스워드 변경123mysql&gt;ALTER USER 'root'@'localhost' IDENTIFIED BY '바꾸고싶은 비밀번호';mysql&gt;FLUSH PRIVILEGES;mysql&gt;quit; 2. MySQL Shell Command(1) DATABASE 생성, 접속, 삭제 현재 상태 보기 1mysql&gt; STATUS DB 목록 보기 1mysql&gt; SHOW DATABASES; DB 만들기 1mysql&gt; CREATE DATABASE DBNAME DB 접속하기 1mysql&gt; USE DBNAME; 현재 접속중인 DB 확인하기 1mysql&gt; SELECT DATABASE(); DB 지우기 1mysql&gt; DROP DATABASE DBNAME; (2) TABLE 생성, 추가, 삭제 table 만들기 1234567CREATE TABLE table_name( column_name_1 column_data_type_1 column_constraint_1, column_name_2 column_data_type_2 column_constraint_2, . . .) column_constraint 는 Optional 이다. (unique 와 같은 제약조건) user 라는 table에 name, email, age 컬럼 생성 example1 : constraint 가 없을 떄, 12345mysql&gt; CREATE TABLE user( name CHAR(20), email CHAR(40), age INT(3)) example2 : constraint가 있을 때, 1234567mysql&gt; CREATE TABLE user2( user_id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20) NOT NULL, email VARCHAR(30) UNIQUE NOT NULL, age INT(3) DEFAULT'30', rdate TIMESTAMP) ## (3) 수정 ### (3)-1. DATABASE 수정 - DATABASE 의 **encoding** 수정 - 현재 문자열 encoding 확인 1mysql&gt; SHOW VARIABLES LIKE \"CHARACTER_SET_DATABASE\"; mydb 데이터베이스의 문자열 인코딩을 utf8 로 변경 1mysql&gt; ALTER DATABASE mydb CHARACTER_SET = utf8; user 데이터베이스의 문자열 인코딩을 ascii 로 변경 1mysql&gt; ALTER DATABASE user CHARACTER_SET=ascii (3)-2. TABLE 수정 user 테이블에 tmp라는 컬럼명, TEXT 데이터 타입 컬럼을 추가1mysql&gt; ALTER TABLE user ADD tmp TEXT;","link":"/2018/11/01/MySQL/"},{"title":"NGINX","text":"Nginx0. What is nginx? client 가 외부 IP와 그 포트를 통해 server 에 접근하면, 그 때부터는 request 를 내부 IP 와 port 로 연결을 해주어야 한다. 일종의 proxy server 역할을 해주는 것이 nginx 1. Install Nginx12$ sudo apt-get update$ sudo apt-get install nginx 2. Manage the Nginx Process nginx Process1234567891011121314#start Nginx$ sudo systemctl start nginx#stop nginx$ sudo systemctl stop nginx#restart nginx$ sudo systemctl restart nginx#check status nginx$ sudo systemctl status nginx# reload nginx$ sudo systemctl reload nginx 3. Structure /etc/nginx Nginx의 설정에 관련된 directory /etc/nginx/nginx.conf nginx의 기본설정 파일, global설정은 이 파일에서 /etc/nginx/sites-available/ 포트 접속시 개별 설정하는 directory 여기 안에 default 파일 변경 4. Nginx Configuration4.1 static file serving/etc/nginx/sites-available/default 파일 수정 123456789server { location / { root /path/to/html ; } location /images/ { root /path/to/image; }} 4.2 proxy server port 별로 설정이 가능하다 12345678server { # default 는 80 port (http default) # 8080 port 에 대해서 listen 8080; location / { proxy_pass http://localhost:8080; }} 여러개의 포트를 연결 server { listen 8080; listen 80; location / { proxy_pass http://localhost:8080; } }","link":"/2018/11/29/NGINX/"},{"title":"Pillow","text":"Pillow 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Pillow 는 Python에서 이미지를 핸들링 하기위한 라이브러리이다. 스크린 샷, 이미지 크롭, 썸네일등을 만들 수 있다. 또한, 다양한 이미지 확장자를 다룰수 있다. (예, jpg, png) Pillow install1pip install Pillow 1. Import Convention Pillow 를 사용하기에 앞서, Import 해야한다.1from PIL import Image 2. Pillow 메소드(1) open() 이름 그대로, 경로와 파일 이름을 적어주면, 해당 이미지리턴하여 Variable 에 할당한다. 1image = Image.open(\"imagefile_name\") (2) crop() 이름 그대로, image 를 잘라준다. 이 때 Parameter 는 box형태로 들어가게 된다. box 는 (left, upper, right, lower) 로 들어가게 된든데, pixel 단위로 들어가면 된다 이미지의 가장 왼쪽과 위쪽 라인을 기준으로, left px 만큼 upper px 만큼 (왼쪽을 기준으로) right px 만큼 (위쪽을 기준으로) bottom px 만큼 잘라, box 로 만들어 준다.123box = (10, 10, 110, 110)image.crop(box)### 이렇게 되면 가로세로 100px 만큼의 이미지가 만들어진다. (3) thumbnail() 썸네일을 만들어준다. 썸네일의 활용 방안은 한 가지 사진을 어플리케이션 곳곳에서 사용한다고 할 때, 데이터 사이즈가 큰 원본 사진을 계속 해서 들고 다니며 사용하면 어플리케이션에 따라 메모리 낭비, 서버용량 낭비, 트래픽 낭비 등으로 이어질 수 있다. 따라서, 이미지의 데이터 크기와 해상도를 낮춰 사용할 수 있다. 12image.thumbnail((pixel, pixel))#thumbnail 메소드를 사용하여, 원하는 pixel 수로 줄일 수 있다.","link":"/2018/10/25/Pillow/"},{"title":"Provision","text":"Server Provisioning &amp; Terraform Provisioning 이란, 한정된 자원을 최적의 효율을 위해 제공하는 기술적 개념을 말한다. 유저의 요청에 맞게 자원을 미리 세팅해두고, 유저의 요청에 따라 준비된 자원들을 목적과 효율에 맞게 제공하는 개념이다. 특정 분야에서 한정되어 사용하는 개념이 아니라 다양한 분야에서 응용되어지는 주제이다. (IT 분야만으로 한정되지도 않는다) IT 분야의 Provisioning의 예시로는, Server Provisioning, Storage Provisioning, Telecommunication Provisioning 등이 있다. 여기서는 Terraform 을 활용한 AWS 서버 프로비져닝에 관해 다룬다. ## AWS EC2 AWS EC2를 활용하기 위해서는 3가지의 기본적인 세팅이 필요하다. 키페어 (Key pair) 보안그룹 (Security Group) 인스턴스 (Instance) 이 세가지를 Terraform 을 활용해 생성하는 코드를 정리한다. 1. 키페어 생성 (Key pair)1-1. Key 만들기- 자신의 email로 ssh key 를 생성하여, key_name 이름으로 .ssh 폴더에 저장ssh-keygen -t rsa -b 4096 -C “email” -f “$HOME/.ssh/key_name” -N “” - 이렇게 생성된 key 는 /key_name/ 과 /key_name.pub/로 private key 와 public key가 생성된다. 1-2. Key pair 생성- `main.tf` 파일 생성123456789`provider “aws” { # 이 region 은 seoul 을 의미한다 region = “ap-northeast-2”}Resource “aws_key_pair” “resource_name” { # keygen 으로 생성한 key_name key_name = &quot;key_name&quot; public_key = &quot;${file(&quot;~/.ssh/key_name.pub&quot;)}&quot;} - apply 를 실행해, aws 키페어를 생성123$ terraform init$ terraform plan$ terraform apply - destroy 를 실행해, aws 키페어를 삭제$ terraform destroy 2. 보안그룹 생성 (Security Group)- 보안그룹은 생성될 인스턴스의 정책을 설정하는 부분이다. 가장 대표적인 기능은 인바운드와 아웃바운드 port 를 설정할 수 있다. - 필요에 따라 port number 를 열어주면 된다. - `ingress` 는 인바운드, `egress` 는 아웃바운드 태그이다. - `from_port` 와 `to_port` 는 말 그대로 from 부터 to 까지의 번호를 지정한다. - 대표적인 포트번호에 관한 설명 - 22 : ssh 접속을 위한 포트 - 80 : http의 기본포트 - 8888 : Jupiter notebook 사용을 위한 포트 - 27017 : MongoDB 사용을 위한 포트 - 3306 : MySQL 사용을 위한 포트 ( /여기서는 DB의 경우, 다른 서버를 두고 사용하고 있으므로, 열어주지 않는다/ ) - `main.tf` 파일 생성1234567891011121314151617181920212223242526provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot;}resource &quot;aws_security_group&quot; &quot;resource_name&quot; { name = &quot;보안그룹 이름&quot; description = &quot;보안그룹의 설명&quot; ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { from_port = 80 to_port = 80 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { from_port = 8888 to_port = 8888 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] }} 마찬가지로, init, plan, apply 를 활용하여 보안그룹을 생성하고, destroy 로 제거한다. 3. 인스턴스 생성- 대망의 인스턴스 생성! - 여기선, EC2 중, linux ubuntu 18.04 버젼 을 활용한다. AWS 내에서 다른 종류의 인스턴스를 사용할 경우, ami 를 다른 값으로 사용하면 된다. - 또한, 다양한 인스턴스 유형중, t2.nano를 사용한다. 인스턴스 유형도 필요에 따라 다르게 설정하면 된다. - `main.tf` 파일 생성12345678910111213141516171819provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot;}data &quot;aws_security_group&quot; &quot;resource1&quot; { name = &quot;security_group_name&quot;}resource &quot;aws_instance&quot; &quot;resource2&quot; { ami = &quot;ami-06e7b9c5e0c4dd014&quot; instance_type = &quot;t2.nano&quot; key_name = &quot;key_name&quot; vpc_security_group_ids = [ &quot;${data.aws_security_group.resource1.id}&quot; ] tags { Name = &quot;dss_instance&quot; }} - `init`, `plan`, `apply` 를 활용하여 인스턴스를 생성하고, `destroy` 로 제거한다.4. 생성된 인스턴스 확인ssh -I ~/.ssh/key_name ubuntu@외부ip 로 생성된 인스턴스를 확인하고, 활용할 수 있다.","link":"/2019/01/10/Provision/"},{"title":"Queue","text":"Queue 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Queue의 특징 내가 버스정류장에 서있다고 생각해보면, 버스 전용차선으로 버스가 줄지어 들어온다. 아무리 뒷차가 손님을 다 태웠다고해서, 앞에 버스가 아직 손님을 태우고 있으면 뒷 버스는 출발 하지 못한다. 이것이 바로 QUEUE FIFO : First In First Out == 선입선출 or 후입후출 front와 rear 라는 index가 각각 구조의 맨 앞과 뒤를 가리키고 있다. Data는 rear로 들어가고, front 에서 나온다. ADT empty() 라는 메소드로 Queue 에 data가 있는지 없는지 확인하게 한다. empty 이면 True, not empty 이면, False 를 return 한다. Queue.empty() returns Boolean enaueue(data) 메소드로 Queue의 rear 가 가리키고 있는 data 뒤에 data를 넣는다. Queue.enqueue (data) returns None dequeue() 메소드로 Queue의 front가 가리키고 있는 data를 반환하면서 삭제 된다. Queue.dequeue() returns data peek() 메소드로 Queue의 front가 가리키고 있는 data를 반환한다. peek은 어디까지나 확인하는 메소드 이므로, data가 삭제되지 않는다. Queue.peek() returns data 구현 1 : by python list Queue 구조를 Python에 내장 되어 있는 list를 container 로 구현한다. 123456789101112131415161718192021class Queue: def __init__(self): self.container=list() def empty(self): # ADT 를 따라서, 비어 있으면 return True 비어있지 않으면 return False if not self.container: return True return False def enqueue(self, data): # list 를 활용했으므로, list.append(data)를 활용할 수 있다. self.container.append(data) def dequeue(self): # pop 역시 list.pop()을 활용할 수 있다. return self.container.pop(0) def peek(self): # peek은 단지 front 에 어떤 데이터가 있는지 확인 하는 것 뿐이므로, 현재 리스트의 가장 앞 부분에 있는 data 를 확인하면 된다. return self.container[0] 구현 2 :","link":"/2018/10/22/Queue/"},{"title":"Requests","text":"WEB CRAWLING 1 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Requests Requests 패키지는 크롤링 하고자 하는 페이지를 url 값을 통해 가져와 객체로 변환 해주는 기능을 가지고 있다. 1. Installation Requests 를 이용하기 위해 package 를 설치해준다.1$ pip3 install requests - request를 이용하면서, json 형식으로의 크롤링과, html 형식으로 css selecting 을 위한 크롤링을 실습해 볼 것이므로, BeautifulSoup Package 역시 같이 설치해준다. (BeautifulSoup은 html parser 를 이용해보기 위함이다.) - python-forecastio 는 dark sky 의 api를 활용해 날씨 데이터를 받아올때 사용해보기 위해 설치한다. 12$ pip3 install bs4$ pip3 install python-forecastio import 할 것들1234import requestsimport forecastiofrom bs4 import BeautifulSoupfrom pandas.io.json import json_normalize 2. [ jSON ] Dark Sky api 활용 날씨정보 가져오기 DarkSky api 는 위도와 경도를 입력하면, 날씨 정보를 주는 api 이다. https://darsky.net/dev/ 에 가입 후, TOKEN 을 받는다. 위에서 받은 개인의 TOKEN 을 활용해 url 을 먼저, formating을 해준다. requests의 get 메소드를 활용해 url 의 정보를 받아온다. 받아온 정보를 requests 의 json 메소드로 json 형식으로 변환해준다. json 을 확인하고 원하는 정보를 return 해준다. 12345def forecast(lat, lng): url = \"https://api.darksky.net/forecast/{}/{},{}\".format(TOKEN, lat, lng) response = requests.get(url) json_obj = response.json() return json_obj[\"hourly\"][\"summary\"] 3. [ html ] BS4 활용 html selecting 해서 가져오기 네이버의 실시간 검색순위 부분의 text 를 크롤링 해보자 html 파일을 BS4 를 활용해 받아온뒤, CSS selecting 으로 원하는 text data 를 가져온다. 123456789101112131415from bs4 import BeautifulSoupdef naver(): url = \"https://www.naver.com\" df = pd.DataFrame(columns=[\"rank\", \"keyword\"]) response = requests.get(url) dom = BeautifulSoup(response.content, \"html.parser\") # BeautifulSoup(markup, features, builder, parse_only, from_encoding, exclude_encodings) for keyword in keywords: df.loc[len(df)] = { \"rank\":keyword.select_one('.ah_r').text, \"keyword\":keyword.select_one('.ah_k').text } return df print(response.content)의 모양을 보자. 따라서, BeautifulSoup 으로 html 형식으로 parsing 해주고, css 를 활용해 selecting 해준다. .select 는 여러개의 엘리먼트를 선택 -&gt; 결과가 리스트 .select_one은 하나의 엘리먼트를 선택 -&gt; 결과가 하나의 객체","link":"/2018/10/24/Requests/"},{"title":"SPARK BASIC 1","text":"Apache Spark Basic 1 SPARK 를 공부하면서 실습 과정을 정리해서 남깁니다. 실습환경 CentOS Spark 2.4.3 Hadoop 2.7 1. Spark-shell1-1. Introductionshell의 spark home directory 에서 다음 명령어를 통해 spark shell 을 진입할 수 있습니다. $ cd /spark_home_directory/ $ ./bin/spark-shell sc : spark context spark : spark session spark context 와 spark session 의 경우, spark shell에 띄우면서 내부적으로 선언된 변수명이다. $ jps // jps 명령어를 통해 현재 돌고 있는 spark process 를 확인할 수 있다. // spark processor 가 jvm 을 바탕으로 돌기 때문에, jvm 프로세스가 도는 것을 확인하므로써 // 확인 할 수 있는 것이다.http://localhost:4040, 즉 해당 서버의 ip:4040 포트를 통해서 드라이버에서 제공되는 웹 UI 를 확인할 수 있다. 이 웹 UI 를 통해 현재 작동하는 프로세서와 클러스터들을 관리 할 수 있다. 1-2. RDDspark는 data를 처리할 때, RDD 와 Spark SQL 을 통해서 data object 를 생성하고 이를 바탕으로 다양한 pipeline 으로 동작 할 수 있다. RDD 를 처음 접해보는 실습. //scala shell val data = 1 to 10000 val distData = sc.parallelize(data) distData.filter(_ &lt; 10).collect() data 가 RDD sc.parallelize의 return 형 역시 parallelize 된 RDD, 즉 distData 도 RDD 마지막 command line 은 10보다 작은 data 에 대해 filtering 하고 각 executor 에서 실행된 자료를 collect() spark 의 특징은 .collect() 와 같은 action api 가 실행될 때 모든 것이 실행되는 Lazy Evaluation (RDD)으로 동작한다. 드라이버 웹 UI 를 통해 이를 확인 할 수 있다. 이전 command line 에서는 아무 동작도 일어나지 않다가 collect() action api 수행을 통해 실제로 command들이 수행되는 것을 확인 할 수 있다. local 에서 default 로 동작하기 때문에 2개의 partition 으로 동작하며, 어떤 shuffling 도 일어나지 않았기 때문에 1개의 stage 임을 확인 할 수 있다. // scala shell // sc.textFile 을 통해 textfile, md 파일등을 읽어드릴 수 있다. val data = sc.textFile(&quot;file_name&quot;) // rdd 의 .map api 를 통해서 rdd 의 element 마다 val distData = data.map(r =&gt; r + &quot;_experiment!!&quot;) // 앞선 map 이 수행되고, 각 element(data) 갯수를 세개 된다. distData.count여기서는 .count 가 action api 이므로, .count 가 수행될 때, 앞선 command 들이 수행되게 된다. sc.textFile() 의 경우 ‘\\n’, newline 을 기준으로 element 를 RDD 에 담게 된다. RDD.toDebugString 를 통해 해당 RDD 의 Lineage 를 확인 할 수 있다. 가장 왼쪽에 있는 | 를 통해 stage 정보 역시 확인 할 수 있다. shuffle 이 일어나게 되면, stage가 바뀌므로, 서로 다른 stage 에 있는 command 의 경우, 다른 indent에 있게 된다. RDD.getNumPartitions 를 통해 해당 RDD의 Partition 갯수 (=Task 의 갯수), 즉 병렬화 수준을 확인 할 수 있다. Shuffle!!suffle 이 일어나는 경우는 api 마다 다양할 수 있다. 가장 기본적으로, 우리가 default partition 갯수를 변경하므로써 shuffle 이 일어나는 것을 확인 할 수 있다. val data = sc.textFile(&quot;file_name&quot;) data.getNumPartitions // Partition 의 숫자를 확인해보면, default 이므로 2 인 것을 확인 할 수 있다. val newData = data.repartition(10) newData.getNumPartitions // Partition 갯수가 10로 변경된 것을 확인 할 수 있다. newData.toDebugString // newData 의 Lineage 를 확인하면, repartition 이 일어나면서 shuffle 이 되고, // shuffle 로 인해 stage 가 2개가 되는 것을 확인 할 수 있다.(indentation) newData.count // action api 를 수행하여 앞선 command 를 모두 수행 위 command line 에 대한 DAG 를 웹 UI 를 통해 확인하면, 다음과 같이 stage 가 repartition을 기점으로 나누어 지는 것을 확인 할 수 있다. 총 Partition의 갯수 (Task의 갯수)를 확인해 보면, default 로 수행된 partition 2 개와, 우리가 설정해준 Partition 의 갯수인 10개를 합하여 12개인 것을 확인 할 수 있다. 여기서 한 스텝을 더 들어가보면, spark 만의 특이한 특징을 확인 할 수 있다. // 위 코드에 이어서, newData 에 대해 // newData RDD를 collect 해서 cli에 찍는 command 를 수행해보자. newData.collect.foreach(println)collect api 와 RDD의 element를 print 를 하는 action api 를 수행할 때, 지금까지 공부한 것으로 생각해 보면, text를 읽어서, 2개의 Partition 을 나누고, 다시 10개의 Partition 을 나누는 작업으로 이전의 12 개의 Task 와 다를게 없을 것 같은 느낌이다. 하지만 UI 를 통해 확인해보면, 10개의 Partition 으로 2개가 skipped 되었다고 확인할 수 있다. DAG 에서도, skipped 된 stage에 대해서 회색으로 확인된다. 이는 spark 에서 이 커맨드라인을 수행할 때, process 간 통신이 file 을 기반으로한 통신을 했기 때문이다. 제일 처음 newData 에 대해서 수행 될 때, 첫 stage 에서 shuffle 이 수행 될 때, 해당 파일을 각 executor 에서 shuffle write 을 하고 저장해두었다가, 두번째 stage 에서 shuffle 이 수행 될때, shuffle read 를 하는 방식으로 file을 기반으로 processor 가 통신하게 된다. 따라서, spark 가 같은 command line 을 수행하게 되면 미리 shuffle write 된 file 을 읽기만 함으로써 앞선 stage 의 동일한 반복 작업을 수행하지 않게 되는 것이다. UI 를 확인 해보아도, shuffle read 만 수행 되었다. SaveFile!!!RDD.saveAsTextFile(&quot;directory_name&quot;) api 를 활용하여, 어떤 처리가 끝난 RDD 를 저장할 수 있다. 이 때 주의 할 점은 parameter 에 들어 가는 것이 directory_name 이라는 것이다. 또한 partition 별로 파일이 저장된다. (e.g. 10개의 partition 이라면, 10개의 file이 저장된다.) Cache!!!spark가 자랑하는 가장 큰 특징은, data(RDD) 를 memory에 cache 함으로써 처리의 속도가 매우 빠르다는 점이다. RDD.cache api 를 통해 memory 에 캐시할 수 있다. // distData RDD 에 이름을 부여 distData.name = &quot;myData&quot; // cache! distData.cache // action : 5 개의 data 를 가져옴 distData.take(5) // action : collect distData.collectdistData.take(5) 까지 한 결과를 UI 에서 cache 를 살펴보면, 다음과 같다. 우리가 설정 한 것 처럼, RDD 의 이름이 myData 로 들어간것을 확인 할 수 있고 cache 역시 확인 할 수 있다. 하지만, Cached 된 비율을 확인하면 전체 RDD 에서 50% 만 된 것을 확인 할 수 있다. 반면에, distData.collect action 을 취하게 되면, Fraction Cached 가 100% 가 된 것을 확인 할 수 있다. 이는 우리의 action 에 따라 cache 할 용량이 달라 질 수 있기 때문이다. spark 입장에서 take(5) api 는 전체 RDD 중 5개의 element data 만 가져오면되고, 이 때 2개의 Partition 중 하나의 Partition 만 cache 해도 충분하기 때문에 Fraction Cached가 50%라고 나오는 것이다. 반면 collect api 는 collect 자체가 각 executor 에 있는 data 를 driver 로 모두 가져오는 것이므로 100% cache 하게 된다. Cache 에서 중요한 것은, 각 executor 의 cache 를 위한 가용 메모리 공간이 해당 Partition의 용량보다 작을 경우, 저장 할 수 있는 용량만큼 저장되는 것이 아니라, 해당 Partition 은 아예 저장이 안되게 된다. 이 점은 Cache를 할 때, Partition 의 용량과 해당 Executor 의 가용 메모리 공간을 미리 파악하여, 설계해야 한다. Word Count 예제!!!우리가 데이터 분석을 할 때, 가장 basic 한 방법은 해당 데이터의 갯수를 세어 보는 것이다. 본 예제에서는 텍스트 파일을 읽어, 띄어쓰기를 바탕으로 word token을 나누고, 이를 세어보자. WordCount 예제는 매우 basic 한 코드이므로, 어떤 로직으로 돌아가는지 완벽한 이해와 코드작성이 필수라고 생각한다. val originalDataRDD = sc.textFile(&quot;text-file&quot;) val wordcountRDD = originalDataRDD.flatMap(line =&gt; line.split(&quot; &quot;)) .map(word =&gt; (word, 1)).reduceByKey(_ + _) wordcountRDD.collect.foreach(println) originalDataRDD 에서 text-file을 읽고, line 마다 띄어쓰기를 기준으로 split 하고 이를 .flatMap 을 통해, flatten 하게 됩니다. 그리고 .map 을 통해 (word, 1) tuple 형태로 mapping 합니다. .reduceByKey 를 통해 같은 word 에 대해 그 counting 갯수를 더하게 된 것을 RDD 로 return 하게 됩니다.","link":"/2019/05/31/SPARK-BASIC-1/"},{"title":"Scrapy","text":"Scrapy 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Scrapy 라이브러리는 파이썬에서 제공하는 라이브러리로써, 대량의 페이지들의 Crawling을 손쉽게 해주는 라이브러리이다. 1. Install 파이썬의 라이브러리 이므로 pip 으로 설치 할 수 있다.1pip3 install scrapy 2. 실습 실습을 위해 import 할 것들 123import scrapyimport requestsfrom scrapy.http import TextResponse requests 를 통해 url 정보를 받아온다. TextResponse 를 통해 받아온 html 파일을 encoding 과 text형식으로 return 12req = requests.get(\"url_name\")response = TextResponse(req.url, body=req.text, encoding=\"utf-8\") 123456a = response.xpath('xpath')# xpath 로 지정한 엘리먼트를 가져온다.a_text = reponse.xpath('xpath/text()')# 엘리먼트의 text data 를 가져온다.a_text.extract()# 엘리먼트의 text data들을 말그대로 extract 하여, list 형태로 return 해준다 3. Scrapy 사용하기(1) scrapy 프로젝트 생성 shell command1scrapy startproject crawler 1!scrapy startproject crawler New Scrapy project &apos;crawler&apos;, using template directory &apos;/Users/emjayahn/.pyenv/versions/3.7.0/envs/dss/lib/python3.7/site-packages/scrapy/templates/project&apos;, created in: /Users/emjayahn/Dev/DSS/TIL(markdown)/crawler You can start your first spider with: cd crawler scrapy genspider example example.com1!tree crawler crawler ├── crawler │ ├── __init__.py │ ├── __pycache__ │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── __pycache__ └── scrapy.cfg 4 directories, 7 files(2) Scrapy 기본 구조 Spider 크롤링 절차 정하기 어떤 웹사이트들을 어떻게 크롤링 할 것인지 선언 각각의 웹페이지의 어떤 부분을 스크래핑 할 것 인지 명시하는 클래스 items.py spider 가 크롤링한 data 들을 저장할 때, 사용자 정의 자료구조 클래스 MVC : 중 Model 부분에 해당 Feature 라고 생각 pipeline.py 스크래핑한 데이터를 어떻게 처리할지 정의 데이터에 한글이 포함되어 있을 때는 encoding=’utf-8’ utf-8인코딩이 필요 settings.py Spider, item, pipeline 의 세부 사항을 설정 (예) 크롤링 빈도 등 (예) robots.txt - ROBOTSTXT_OBEY=True","link":"/2018/10/26/Scrapy/"},{"title":"Selenium","text":"Selenium 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 자동화 할 수 있는 프로그램 [ Install Selenium ] chrome driver 다운로드 1$ mv ~/Download/chromedriver /usr/local/bin Selenium Python Package 설치 1$ sudo pip3 install selenium 1. 셀레니움 사용해보기 import 1from selenium import webdriver Open Browser (Chrome Driver) 1driver = webdriver.Chrome() 페이지 이동 1driver.get(url) 브라우져 상에서 자바 스크립트 실행 1driver.execute_script(\"window.scrollTo(300, 400)\") 지금 control하고 있는 window 객체 확인 123main_window = driver.current_window_handlemain_window#현재 control 하고 있는 window 의 객체를 리턴한다. 열려 있는 전체 window 탭 모두 확인 123windows = driver.window_handleswindows#현재 열려있는 모든 탭의 객체를 리스트 형태로 리턴한다. 열려있는 window 탭 중, control 대상 window 로 바꾸기 123driver.switch_to_window(windows[0])#다시 원래 열었던 창으로 돌아가기driver.switch_to_window(main_window) 2. Alert 와 Confirm 다루기 웹을 자동화해서 다니다보면, Alert 창이나 Confirm 창이 의도치 않게 나올 수 있다. 이를 다루는 방법은 다음과 같다. Alert와 Confirm 창의 차이는, Alert 는 확인 창 하나만 있고, Confirm은 확인과 취소가 같이 있는 창이다. (1) Alert Alert 띄우기 12drive.switch_to_window(main_window)drive.execute_script(\"alert('이게 alert 로 뜰겁니다.');\") Alert 확인 누르기 12alert = driver.switch_to.alertalert.accept() Alert 창이 있으면 확인을 누르고, 없으면 없다고 리턴하기 (예외처리) 12345try: alert = driver.switch_to.alert alert.accept()except: print(\"alert 가 없습니다.\") (2) Confirm Confirm 띄우기 1driver.execute_script(\"confirm('이게 Confirm 창입니다.');\") Confirm 창 확인 누르기 or 취소 누르기 12345confirm = driver.switch_to.alert# 확인# confirm.accept()# 취소confirm.dismiss() 3. 입력창에 글씨 입력하기 이제부터 Selenium을 통해 특정 html 의 element 에 액션을 주려면,각종 Selector 를 사용하여 html 상의 element을 셀렉팅 하고, 해당 element 에게 액션을 주어야한다. 1driver.find_element_by_css_selector(nameof cssselector).send_keys(\"입력할 내용\") .find_element_by_css_selector 와 .find_elements_ by_css_selector 는 다르다.element 는 하나의 selector 만 선택하는 반면, elements 는 여러가지 element 를 셀렉팅 해서, 리스트 형식으로 Return 한다. 따라서 이렇게 리스트 형식으로 Return 이 된 경우, List[0] 등과 같이 그 엘리먼트를 뒤에 지정해줘야된다. 즉,1234driver.find_element_by_css_selector(nameof cssselector)#위는 바로 selecting 이 된것이지만,driver.find_elements_by_css_selector(nameof cssselector)[0]#위는 뒤에 selecting 위해 list 의 요소를 선택 해주어야한다. 4. 버튼 클릭하기12# 위의 방법처럼, element 를 선택해준다.driver.find_element_by_css_selector(\"name of css selector\").click() 5. id, class, 그 외의 attribute 값을 selecting 하는 법1234driver.find_element_by_css_selector(\"name of css selector.\").click()# id 의 경우 : #idname# class 의 경우 : .classname# 다른 attribute 의 경우 : [attribute = 'value'] 6. selecting 한 element 의 attribute value 를 얻는 방법 CSS로 선택한 element에 html 태그에는 다양한 attribute 들이 있을 수 있다. 이 중attribute의 value에 접근 하고 싶을 때는 .get_attribute()메소드를 사용할 수 있다.1driver.find_element_by_css_selector(\"name of css selector\").get_attribute(\"attribute_name\") 7. element 의 위치와, 사이즈 구하기 스크롤을 내리거나, 엘리먼트의 위치와 크기를 알고 싶을 때 사용할 수 있다.12345element = driver.find_element_by_css_selector(\"name of css selector\")element.location#element 의 좌측 상단의 위치가 pixel 단위로 x, y 값의 dictionary로 보여준다.element.size#element 의 size 를 height, width를 pixel 단위로 dictionary로 보여준다.","link":"/2018/10/24/Selenium/"},{"title":"Stack","text":"Stack 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Stack 의 특징 접시를 쌓듯이 데이터를 쌓아 올리는 모양의 데이터 구조 LIFO : Last In First Out == 후입선출 or 선입후출 top index 가 항상 Stack의 가장 윗부분을 가리키고 있어, 우리는 top 위치만 볼 수 있다. 단점 : stack 의 top 이 외에 밑에 쌓여져있는 데이터의 Search 가 안된다. ADT empty() 라는 메소드로 Stack 에 data가 있는지 없는지 확인하게 한다. empty 이면 True, not empty 이면, False 를 return 한다. Stack.empty() returns Boolean Stack의 top 위치에 데이터를 쌓는다. Stack.push(data) returns None Stack의 top 위치에 있는 데이터를 삭제하면서 반환한다. Stack.pop() returns data Stack의 top 위치에 있는 데이터를 반환하지만, 삭제하지 않는다. 어떤 데이터가 있는지 just 확인. Stack.peek() returns data 구현 1 : by python list Stack 구조를 Python에 내장 되어 있는 list를 container 로 삼아 구현하게 되면, 그 구현은 매우매우 쉽다. Python의 List 자료형의 대단함을 그만큼 느낀다. 이 때 List 의 index가 큰 쪽이 top 방향이다.12345678910111213141516171819202122class Stack: def __init__(self): self.container=list() def empty(self): # ADT 를 따라서, 비어 있으면 return True 비어있지 않으면 return False if not self.container: return True return False def push(self, data): # list 를 활용했으므로, list.append(data)를 활용할 수 있다. self.container.append(data) def pop(self): # pop 역시 list.pop()을 활용할 수 있다. return self.container.pop() def peek(self): # peek은 단지 top 에 어떤 데이터가 있는지 확인 하는 것 뿐이므로, 현재 리스트의 가장 마지막에 append 되어 있는 data를 확인하면 된다. return self.container[-1] 구현 2 :","link":"/2018/10/22/Stack/"},{"title":"Thread","text":"Thread 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 파이썬 기본 : Single Thread (Main Thread) threading 모듈 : main thread에서 subthread 를 생성하여 진행하는 방식 multiprocessing 모듈 : double cpu ThreadPoolExecutor : API - 멀티스레드와 멀티프로세스 동일한 형태로 디자인(Pool 클래스만 변경하면됨) threading.Thread() arguement12345Thread(group=, target=, args= , kwargs=, *, daemon=None)#target= : 실제 스레드에서 돌아가게 될 함수#args= : tuple 로 target 함수에 들어가게될 argument#kwargs= : dictionary로 target 함수에 들어가게될 argument#daemon : 데몬 스레드로 돌아갈지 여부 12345#Thread 의 메소드start(): #스레드의 실행, self 의 run() 메소드를 호출run(): #스레드가 실제로 수행하게될 작업name : #스레드의 이름threading.locals() : #해당 스레드 내부에서 사용할 로컬 변수 지정","link":"/2018/10/24/Thread/"},{"title":"[논문읽기] Implementation of Vanilla GAN","text":"A pytorch implementation of Vanilla GAN using MNIST digits data(https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) 실험 결과와 코드는 https://github.com/EmjayAhn/GAN-pytorch 에서 확인 할 수 있습니다. 0. 목표GAN 은 최근 인기를 끌고 있는 generative model 중 하나입니다. 다양한 모델이 쏟아져 나오고 있기에, 이 트렌트를 따라가기 위해서, 가장 기본적인 vanilla gan 을 구현하고, 이해하는 것이 목표입니다. 이번 예제에서는 MNIST digit 을 사용하였으나, 이미지 외에도 다양한 데이터를 활용할 수 있습니다. 1. Model 구조GAN 의 아버지 Ian Goodfellow가 제안한 이 모델은 두가지 신경망으로 구성 되어 있습니다. 먼저, 우리가 가지고 있는 데이터와 비슷하게 데이터를 생성하는 것을 학습하는 Generator 와 Generator 가 생성한 데이터(fake)와 실제 우리가 가지고 있는 데이터(real)를 fake 인지 real 인지 구분하는 Discriminator 를 구분하는 두 신경망으로 구성되어 있습니다. 1-1. Generator Generator 는 Gaussian Random Noise (mean=0, std=1) 를 입력으로 받아, 이 noise로 부터 data 를 생성해냅니다. 이번 구현에서 Generator는 다음과 같이 작성하였습니다. Dense layer 만 사용했으며, gradient vanishing 현상을 막기 위해 activation 을 거친 후, Batch Normalization 을 추가하였습니다. 1-2. Discriminator Discriminator 는 Generator 가 생성해낸 데이터와 기존에 가지고 있는 진짜 데이터를 입력으로 받아, 진짜 데이터를 1, 가짜데이터를 0으로 학습하는 classifier 입니다. 아래의 Loss Function 을 확인 하겠지만, 진짜 데이터에 대해서는 그 확률 값을 높게 하고, 가짜데이터에서는 그 확률 값을 0에 가깝게 하는 것이 이 모델의 optimize 목표입니다. 이번 구현에서 Discriminator를 다음과 같이 작성하였습니다. 2. Loss Function2-1. Loss Function 의 해석$$\\underset{G}{\\text{min}}\\underset{D}{\\text{max}}V(D, G)=E_{xp_{data(x)}}[logD(x)]+E_{zp_{z}(z)}[log(1-D(G(z)))]$$ Vanilla GAN 의 Loss function 은 위와 같습니다. Loss Function 의 구조 자체는 min-max 최적화로써, Discriminator와 Generator 의 loss 함수를 각각 최적화 해 나아가면서 위 식의 균형 향해 다가가는 것입니다. 먼저, Discriminator에 대한 max 부터 살펴 보면, Real을 입력으로 넣었을 때는, log(D(x))의 기댓값이 최대가 되게 하고, G(z) 즉, 가짜를 가짜라고 할 확률 1-D(G(z))는 최대가 되게끔 학습을 하는 것입니다. Generator 에 대한 min 을 살펴보면, G(z) 는 가우시안 랜덤 변수를 받아 생성된 데이터를 D(G(z)), discriminator에 넣었을 때, 1-D(G(z)), 즉 가짜라고 할 확률을 최소화하게끔 학습하는 것입니다. 이는 결국, Discriminator를 속이기 위해 generator의 최적화가 실행된다는 의미입니다. 이 식에서 중요한 점은, Discriminator 는 학습할 때, Generator 가 생성한 데이터와 진짜 데이터 모두를 보며 학습하지만, Generator 는 그 어디에서도 진짜 데이터가 어떻게 생겼는지는 확인하지 않습니다. 오로지 Discriminator 를 속이기 위해 학습하는 것이지만, 그 결과 우리가 가지고 있는 진짜 데이터와 비슷하게 만들수 있는 모델을 획득하게 될 수 있는 것이라는 점에서 매우 획기적인 모델입니다. 2-2. 실제 구현에서의 변형Generator 에 대한 loss function 은 log(1-(D(G(z)))를 최소화 하는 것입니다. 하지만, log(1-x) 형태의 식은 x가 0일 때, 그 gradient 가 매우 작아, 학습이 매우 오래 걸리는 문제가 있습니다. 이를 해결하기 위해, log(1-D(G(z)))를 G에 대해 최소화 하는 것은 결국, -log(D(G(z)))를 최소화 하는 것과 같고, 이는 log(D(G(z)))를 최대화 하는 것과 같습니다. 따라서 실제 구현에서의 criterion 은 Discriminator 가 사용하는 criterion(여기선, binary cross enntropy loss)을 동일하게 사용합니다. 3. 학습 및 모델 결과학습 parameter 는 다음과 같습니다. Total epoch: 300 batch size : 128 z dimension : 100 Adam optimizer : lr=0.0002, weight_decay=8e-9 다음은 generator가 학습이 되가면서, 같은 가우시안 랜덤 노이즈에 대해 mnist 와 닮은 데이터를 생성해 나가는 과정입니다. (1) 0 epoch 약 400개의 배치를 학습한 후, 찍은 사진이기에 가운데에 아주 미세한 형태는 보이지만, 가우시안 노이즈임을 확인 할 수 있습니다. (2) 20 epoch 20 epoch 만 되더라도, (마치 뱃속의 아가처럼(?)) 가운데에 어떤 형태가 생성되기 시작하는 것을 확인 할 수 있습니다. (3) 100 epoch 9, 3, 8, (horizontal flipped) 3, 1.. 아주 힘들게 MNIST 와 비슷해 보이는 숫자를 확인 할 수 있습니다. 4. 결론이번 구현의 목표는 나의 첫 vanilla gan 을 논문과 여러 자료를 공부해보며, 구현해 보는 것에 있었기에, 이를 완수하고, 실제로 학습 시켰을 때, generator 로써 기능을 할 수 있다는 점에서 유의미 하였습니다. 다양한 repository 에서 서로 다른 framework 를 사용하여, gan 을 구현하는 것을 참조 할 수 있습니다. 하지만, 직접 공부해보고, loss function의 의미를 해석하여 직접 구현해보며 많은 것을 배울 수 있었습니다. 실제로 loss function 위 처럼 바꾸지 않았을 때는 1000 epoch 를 학습하더라도 generator가 학습 되지 않는 실패 경험을 통해, 자세한 논문 리딩과 분석은 구현에 있어 필수적임을 느낄 수 있었습니다. 5. Futher Study자원의 제약으로 작은 모델 구조와 hyper parameter tuning을 더 하지 못한게 아쉽습니다. generator 가 생성해내는 모양이 조금더 세밀하게 할 수 있는 것을 더 해보고 싶고, MNIST 데이터 뿐만아니라 본 논문의 참조사진처럼 CIFAR10 이나, TFD 데이터에 대해서도 실험해보고 싶습니다.","link":"/2019/08/13/Vanilla-GAN/"},{"title":"Xpath","text":"xpath 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 1. xpath란? xpath = XML Path Language XML 문서의 element나 attribute의 value에 접근하기 위한 언어 XML 문서를 해석하기 위한 언어이므로, 기본적으로 path expression 이 바탕이 되어있다. 연산, 문자열 처리를 위한 라이브러리를 내장하고 있다. 2. Location Path element 를 찾으러 가는 것이므로, location을 나타내기 위한 연산자는 다음과 같다. element_name : element_name 과 일치하는 모든 element를 선택한다. / : 가장 처음 쓰는 /는 절대경로를 의미한다. Path 중간에 쓰는 /는 조건에 맞는 바로 다음 하위 엘리먼트를 검색한다. (css selector에서 &gt;와 같다) // : 가장 상위 엘리먼트 . : 현재 엘리먼트 * : 조건에 맞는 전체 하위 엘리먼트를 검색한다(css selector 에서 한칸 띄우는 것 과 같다) element[조건] : 엘리먼트에 조건에 해당되는 것을 검색한다 (예) p[2] : p 엘리먼트 중 두번째 엘리먼트를 선택 *주의:1부터 시작, 0이 아님 * (예) [@(attribute_key=&quot;attribute_value&quot;)] : 속성값으로 엘리먼트를 선택[@id=&quot;main&quot;] : “main” 이라는 id 를 선택[@class=&quot;pp&quot;] : “pp” 라는 class 를 선택 not(조건) : 조건이 아닌 엘리먼트를 찾는다. /text() : 해당 엘리먼트의 text 데이터를 가져온다 /@attribute_name : 해당 엘리먼트의 attribute_name 에 할당된 value 값을 가져옴 /@href : 해당 엘리먼트의 href의 value 값을 가져옴","link":"/2018/10/26/Xpath/"},{"title":"[Python] 쉽게 쓰여진 Decorator","text":"오픈소스나, 다른 사람들이 만든 코드를 재수정한 코드, 제가 짠 코드에 대해 다양한 디버깅과 좀 더 다른 기능을 추가하고 싶을 때, 우리는 Decorator 를 자주 접하게 됩니다. Python 실력을 한층 업그레이드 하기 위함과, 코딩의 또 다른 목적이이 귀차니즘의 해결 이라고 생각할 때 Decorator 에 대한 이해는 그 시작 관문이 됩니다. 이번 글에서는 이 Decorator 에 대한 개념을 쉽게 설명하려고 노력하였습니다. 많은 책들에서 Decorator 의 설명을 본격적으로 들어가기에 앞서, python에서의 변수의 범위와 전역변수, 지역변수, 자유변수 등을 설명하고, 자칫 정신을 혼미하게 만들 수 있는 클로져(closure)를 설명한 뒤에 Decorator 를 만나게 됩니다. 물론 모두 Decorator 에서만아니라 파이썬을 다루고, 컴퓨터 과학을 공부하며, 필수적으로 알아야하는 중요한 개념이긴 하나, 이번 글에서는 예제들을 통해 Decorator 를 짜는 방법과 어떻게 구동이 되는지 실용적인 개념에 대해 요약 정리하려고 합니다. 1. Decorator란? Decorator : 호출 가능한 객체 로써, 호출 가능한 객체를 입력으로 받아, 호출 가능한 객체 를 반환하는 함수 Decorator 를 만들 때, 가장 첫번째로 성립해야하는 구조가 위처럼, Decorator 기능을 하게 될 함수가 호출 가능해야하고, 입력에 호출 가능한 객체를 받아, 반환하는 객체도 호출 가능한 객체로 반환 해주어야 합니다.위의 정의를 만족하는 세상에서 가장 간단한 Decorator 를 다음의 예제코드로 만들 수 있습니다. 위의 간단한 코드의 동작 순서를 살펴봅니다. 123456(3) 에서 say_hi 라는 함수 객체가 dumb_decorator 함수의 입력으로 들어가, dumb_decorator() 함수를 호출합니다.(1) 에서 dumb_decorator는 입력받은 함수 객체(say_hi) 자체를 반환합니다.dumb_decorator 가 반환한 함수객체를 (3)의 decorated 에 저장합니다. 3.에서 decorated는 함수 객체 자체입니다. (4) 에서 decorated() 로, 함수 객체를 실행한 결과를 text 에 저장합니다. 이 코드에서 결국 decorated 함수 객체가 실행되면, say_hi 함수가 실행되고, say_hi함수의 반환값인 &quot;Hi, Hi&quot; 가 text 에 저장되어,(5)에서 출력됩니다. 위의 코드의 결과는 Hi, Hi 가 출력되는 아무 기능이 없는 기본 함수(say_hi)와 Decorator 입니다. 위와 같은 결과이지만, Decorator 문법인 @를 사용한 코드는 다음과 같습니다. 위의 두 예제 코드는 같은 결과를 내지만, 앞서 정의된 dumb_decorator 가 첫번째 예제에서는 꾸미려고하는 say_hi 함수를 dumb_decorator의 입력으로 넣어주어 코드를 실행 시켜 주어야 했습니다. 두번째 예제에서는 꾸미려고하는 say_hi 함수의 선언시 @dumb_decorator 를 먼저 적어주고, 꾸며진 함수 say_hi() 를 실행하는 점에서 차이가 있습니다.앞서 처음에 등장한 구조적 정의를 위 예제에 대입하여 살펴봅니다. Decorator : 호출 가능한 객체 로써, 호출 가능한 객체를 입력으로 받아, 호출 가능한 객체 를 반환하는 함수 호출 가능한 객체로써 (dumb_decorator), 호출 가능한 객체(say_hi)를 입력으로 받아, 호출 가능한 객체((1)에 return func)를 반환하는 함수 일 때, @decorator로 동작 할 수 있습니다. 2. Decorator를 Decorator 처럼1에서 Decorator의 핵심적인 구조를 살펴보았습니다. 이제 본격적으로 Decorator를 사용되기 위해선, decorator 내부에 입력함수의 기능을 꾸며주는 wrapper 함수가 필요합니다. 예제로 살펴보겠습니다. 첫번째 예제와 다른 점이라곤, 우리가 꾸미려고 하는 decorator 함수 안에 내부 wrapper 함수를 추가해줌으로써, 우리가 꾸미려는 내용을 선언해주었습니다. 3. Decorator 의 장점?위에 본 예제를 극단적인 case 로 몰고 가보죠. 우리가 꾸며야할 함수가 많다고 상상해 보겠습니다. 기존에 잘 동작하던 프로그램이 있을 때, 프로그램의 로그가 궁금하다던가, 신경망을 학습시키는 코드에 대해 각 layer 의 gradient 나 출력값을 보고 싶을 수 있습니다. 우리가 decorator 를 사용하지 않는다면, 첫번째 코드 예시 처럼 일일히 다 실행시켜주어야 하는 문제가 발생합니다. 우리는 사용하려고 하는 함수를 선언할때 @decorator 를 붙여 줌으로써, 이러한 귀차니즘을 해결할 수 있습니다. 예제를 통해 살펴보겠습니다.기존에도 잘 돌아가는 프로그램 3개의 log 를 찍어야 하는 순간이 찾아왔다고 가정합니다. generator 를 사용하는 것과 그렇지 않은 것을 비교해봄으로써 generator 의 고마움을 느껴볼 수 있습니다. 이번 예제에서는 간단하게 그 log 를 프로그램 return 형의 글자(character) 수로 생각해보죠. 기존에 존재하던 프로그램이 program_1, program_2, program_3 이라고 생각해보고, 추가적으로 우리가 my_decorator 라는 코드를 통해 각 프로그램의 로그(여기선, charater 수)를 찍어야 하는 task 가 주어졌습니다. 우리가 decorator 를 알기 전이라면, 위와 같이 코드를 작성해 준 후,실행부분에서 각 프로그램을 decorator 에 넣어주어, 꾸며진 결과 객체를 가지고, 이를 다시 실행해주는 행동을 반복해 주어야 합니다. 이제 decorator 의 고마움을 느껴볼 차례입니다. 4. 마치며간단한 예제를 통해 Decorator 가 어떻게 동작하는지, 어떻게 구성해야하는지 핵심적인 부분이 이해됐길 바랍니다. 조금더 공부할 수 있는 키워드는 클로져 (closure), free variable 등이 있을 수 있습니다. 위 키워드의 공부를 통해 조금더 자유로운 generator 설계를 할 수 있을 것으로 믿습니다.","link":"/2019/07/27/decorator/"},{"title":"[Python] 볼 때마다 헷갈리는 Iterable, Iterator, Generator 정리하기","text":"Iterable vs Iterator vs Generator 다른 분들의 코드를 읽을 때마다, 내가 사용할 때마다, 헷갈리는 Iterable, Iterator, Generator를 이번 글을 작성해보면서, 마지막으로! (라는 다짐으로) 정리해봅니다. 잘 알고 있는 개념이라고 생각했지만, 다른 사람들로부터의 질문을 받았을 때, 나의 설명이 만족스럽지 못해 ‘아 내가 더 정확히 알아야 한다’ 는 메타인지로부터 출발하는 글입니다. 파이썬의 장점으로 꼽히는, '사용하기 쉬운 데이터 구조'들 덕분에 우리는 루프를 돌아야하는 알고리즘에 대해 손쉽게 코드를 작성 할 수 있습니다. 하지만, 때로는 나만의 객체(class)를 만들고 그 객체가 파이썬에 내장 되어있는 데이터 구조 처럼, 동작하기를 바랍니다. 요즘 들어, 제가 짜는 코드에서 이 욕구는 신경망 모델링을 할 때, 신경망 모델 객체에 데이터의 배치를 feeding 하는 객체를 생성하고 싶을 때, 넘쳐나게 됩니다. 이럴 때, 파이썬에 대한 기본 개념이 잘 잡혀있지 않은 상태에서 복잡한 코드를 짜려고 하는 시도를 하니, 신경망 모델 자체의 구조에 대해서도 복잡한데 이런 루프를 돌면서 반복적으로 일정 데이터를 넘겨주기 위한 간단한 기능을 가진 코드에 대해서도 비효율적으로 작성하게 됩니다. 이런 제 자신의 문제를 해결하기 위해 이번 기회에 Iterator 와 Generator 에 대해 확실하게 정리해보려고 합니다. (feat. 예제코드) 1. 이터러블: Iterable Iterable 객체란? : 객체 안에 있는 원소(element)를 하나씩 반환 가능한 객체 파이썬이 제공하는 대부분의 내장 데이터 구조는 이터러블(Iterable)한 객체 입니다. 뿐만 아니라 우리가 만든 객체(class)도 Iterable 객체가 될 수 있습니다. 이터러블(Iterable)객체는 for 문과 같은 루프 뿐만 아니라, zip이나 map 과 같은 순서대로 처리할 입력이 필요한 곳에서도 사용 될 수 있습니다.이터러블(Iterable) 객체는 iter() 라는 함수의 입력으로 들어갑니다. iter() 라는 함수는 다음에 설명될 이터레이터(Iterator)를 반환합니다. 2. 이터레이터: Iterator Iterator 객체란? : Iterator의 __next__() 나 내장 함수인 next()를 부르면서, 원소(element)를 순차적으로 반환 할 숫 있는 객체 앞서, 설명드린대로 이터레이터(Iterator)는 iter()라는 함수가 반환하는 객체입니다. 그리고, 이터레이터(Iterator)는 반복적으로 __next__()나, next() 함수의 입력으로 들어가 호출하여, next()의 return 값인 원소(element)를 최종적으로 반환합니다.이터레이터(Iterator)가 다음 원소를 계속 반환하다가, 끝에 다달아 반환할 원소가 없을 경우 예외문인 StopIteration이 발생하게 됩니다.즉, 정리하면, Iterable 객체 A → iter(A) → Iterator 객체 B → next(B) → element (data) Question? 우리는 list 같은 이터러블(Iterable) 객체와 for 문을 쓰면서 한번도 iter() 나 next()를 보지 못했는뎁쇼????파이썬의 for 문은 이터러블(Iterable) 객체를 만나, 내부적으로 iter() 함수를 호출하여, 이터레이터(Iterator)를 생성합니다. 이 생성된 이터레이터(Iterator)가 루프가 실행되면서 next() 를 호출하며 반복적인 데이터를 뽑아 낼 수 있게 되는 것입니다. 그리고 모든 원소가 뽑아지고 난 뒤에는 StopIteration 이 발생하며, for 문이 종료 됩니다. 위 설명을 코드로 풀어쓰면, 다음과 같습니다. 우리가 다음과 같은 for 문을 사용하면, 12for element in iterable_object: print(element) 위 코드는 다음과 같이 파이썬 내부적으로 다음과 같이 동작하게 됩니다. 123456789101112# iter() 함수를 호출해, iterator 를 생성하고,iterator_object = iter(iterable_object)while True: # next() 함수를 호출해, element 를 받아옵니다. try: element = next(iterator_object) print(element) # element 가 없을 시, StopIteration Exception 발생 except: StopIteration: break 3. 이터러블(Iterable), 이터레이터(Iterator)와 친해지기3-1. 간단 버전파이썬 이터러블(Iterable) 내장 데이터 구조인 list 를 활용하여, 실습해 봅니다. 123456789101112131415161718&gt;&gt;&gt; iterable_object = [1, 2, 3, 4, 5]&gt;&gt;&gt; iterator_object = iter(iterable_object)&gt;&gt;&gt; iterator_object&lt;list_iterator object at 0x104fe2278&gt;&gt;&gt;&gt; next(iterator_object)1&gt;&gt;&gt; next(iterator_object)2&gt;&gt;&gt; next(iterator_object)3&gt;&gt;&gt; next(iterator_object)4&gt;&gt;&gt; next(iterator_object)5&gt;&gt;&gt; next(iterator_object)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration 3-2. 나만의 iterable, iterator객체다음의 코드는 이름과 나이를 받는 데이터 객체입니다. 루프를 돌면서 각각 순서에 맞는 (이름, 나이)형태로 데이터를 반환하는 Iterable 객체입니다. 같은 기능을 하는 더욱 효율적인 코드를 짤 수 있겠지만, 공부한 iterable 과 iterator 를 적용해보는 class 입니다. 4. 제너레이터: Generator 제너레이터(Generator)란?: 특이한 (공식 문서에 ~ iterator) 이터레이터(Iterator) 정의에서도 보다시피, 이터레이터(Iterator)입니다. 즉, next() 함수를 만나 동작하는 함수입니다. 이 때, 제너레이터(Generator) 를 일반 함수와 다르게 하는 것이 yield 라는 문법입니다. yield 는 일반 함수의 return과 같이 값을 반환 하지만, return과 다르게 해당 함수(generator)가 종료하지 않고, 그대로 유지됩니다. 다음 순서의 제너레이터(Generator)가 호출되면, 멈추었던 yield 자리에서 다시 함수가 동작하게 됩니다. 요약하자면, 제너레이터(Generator) → next(제너레이터) → 제너레이터 함수 실행 → yield를 만나 next(제너레이터가) 호출된 곳으로 값을 반환 (제너레이터 종료 ❌) → 다시 next(제너레이터) → 멈추었던 yield부터 재실행 Question? 이런 특이하고, 어렵고, 처음엔 익숙하지 않은 제너레이터(Generator)를 왜 때문에 쓰는 겁죠???제너레이터는 메모리를 아끼기 위해서 사용합니다!!! 다음의 코드에서 메모리 사용량의 비교를 통해 살펴 보겠습니다. 5. 제너레이터 (Generator)와 친해지기제너레이터를 사용한 코드가 메모리 사용량 측면에서 얼마나 효율적인지 확인해보겠습니다. 다음의 코드는 999999까지의 숫자를 제곱하여 return 해주는 함수입니다. 첫번째, 일반적인 함수는 end 숫자까지 for 문을 돌면서 그 결과를 저장한 뒤에 return 해줍니다. 두번째 generator 는 for 문이 돌 때, yield 에서 해당 데이터만 리턴하게 되므로, 메모리 사용에서 효율적입니다. 다음의 실험에서도 쉽게 비교할 수 있습니다. 위 코드 실행결과: 12Memory Usage when program start: 8.80859375Memory Usage when program end: 56.0390625 위 코드 실행결과: 12Memory Usage when program start: 8.78515625Memory Usage when program end: 8.78515625 6. 마무리이번 짧은 글에서, Iterable, Iterator, Generator 를 비교해보면서 그 개념과 사용 예제를 간단하게 살펴 보았습니다. 이름에서부터 헷갈릴 수 있는 각 객체에 대한 내용을 이 글을 통해 조금이나마 정리해볼 수 있는 기회가 되셨으면 좋겠으며, 작은 도움이 되셨으면 합니다. 7. Reference Python documentation: generator Python documentation: iterable Python documentation: iterator","link":"/2019/07/15/iterator-generator/"},{"title":"[Linux] 자고있을 때도, 알아서.. 리눅스 Crontab","text":"데이터를 모으기 위해 크롤링을 진행하거나, 머신러닝, 딥러닝 실험을 할 때 Linux 환경의 머신에서 정해진 시간과 주기에 맞추어 크롤링을 실행하고, 학습을 해준다면, 수많은 작업들을 미리 설정해둔 내용을 바탕으로 편하게 작업을 자동화 할 수 있습니다. 1. Crontab 스케줄 작성, 삭제, 목록 확인1-1. Crontab 스케줄 작성하기 1$ crontab -e -e (edit) 옵션으로 Crontab 의 스케쥴을 설정해 줄 수 있습니다. 작성할 스케쥴은 리눅스의 디폴트 에디터인 vi 에디터를 이용합니다. 1-2. Crontab 스케줄 지우기 1$ crontab -r -r (remove) 옵션으로 Crontab 에 등록된 스케줄을 삭제해 줄 수 있습니다. 1-3. Crontab 스케줄 목록 확인하기 1$ crontab -l -l (list) 옵션으로 Crontab 에 등록된 스케줄 리스트를 확인 할 수 있습니다. 2. Crontab 주기항상 사용할 때마다, 헷갈리고 잊어버리는 설정 주기 순서입니다. 마지막의 요일 부분은 0, 7: 일요일, 1: 월요일 ~ 6: 토요일 12* * * * *분 시간 일 월 요일 예를 들어, 매주 월요일에 crawling.py 를 실행한다면, 아래와 같이 crontab edit 창에서 작성하고 저장하면 됩니다. 1* * * * 5 python3 /directory/crawling.py 이제부터는 조금 복잡한 주기를 설정할 수도 있습니다.2-1. 반복 매시 25분, 45분에 실행하고 싶을 때 125,45 * * * * python3 /directory/crawling.py 매 20분마다 실행하고 싶을 때 1*/20 * * * * python3 /directory/crawling.py 2-2. 범위 매주 수요일에서 금요일까지 1시 30분마다 실행 시킬 때130 1 * * 3-5 python3 /directory/crawling.py 3. 예제2분마다 “THIS IS CRONTAAAB”을 test.txt 에 기록해보자. “THIS IS CRONTAAAB”을 test.txt 파일에 기록하는 shell command 를 작성합니다. 12# program.sh파일에 다음과 같이 작성합니다.echo &quot;THIS IS CRONTAAAB&quot; &gt;&gt; ./test.txt&quot; crontab -l 명령어를 통해 다음과 같이 작성합니다 1*/2 * * * * /directory/program.sh","link":"/2019/08/06/linux-crontab/"},{"title":"[GCP] Computing Engine 환경설정","text":"Computing Engine 환경설정 이번 글은, Google Cloud Platform의 Computing Engine 인스턴스의 기본적인 환경설정 방법을 소개하는 글입니다. 직접 작업환경을 세팅하면서, 정리 하는 목적으로 작성하는 글입니다. 다음과 같은 환경을 설정합니다. 인스턴스 생성 및 네트워크 설정 접속을 위한 ssh 생성 및 접속 python3, pip3 설치 CUDA 설치 cuDNN 설치 Pytorch 설치 Jupyter 설치 및 환경설정 1. 인스턴스 생성 및 네트워크 설정Compute Engine에서 자신의 목적에 맞는 리소스를 정해, 인스턴스를 생성해줍니다. Jupyter notebook 을 사용하기 위해, VPC 네트워크 → 방화벽 규칙 탭에서 방화벽 규칙을 만들어 줍니다. 후에 Tensorboard 와 다른 기타 환경들을 사용할 때 그에 맞는 포트 규칙을 동일한 방법으로 열어주면 됩니다. 아래의 4가지를 설정해주고 ‘만들기’ 클릭 이름 : jupyter 소스 IP 범위: 0.0.0.0/0 대상 태그: jupyter tcp: 8888 http, https: 체크 2. 접속을 위한 RSA key pair 생성 및 ssh 접속 gcp 에서 제공하는 gcloud로도 접속 할 수 있습니다. 위에서 생성한 인스턴스에 접속하기 위해, RSA key pair 를 이를 통해 접속 해 봅니다. 아래의 명령어를 이용해 키페어를 생성해 줍니다. 이 때, USERNAME 은 gcp에 등록한 이메일로 설정합니다. $ ssh-keygen -t rsa -f ~/.ssh/[KEYFILE_NAME] -C “[USERNAME]” example) $ ssh-keygen -t rsa -f ~/.ssh/gcp-key -C “myemail@mail.com“ 앞으로 사용할 Password 를 입력하고, 생성된 키페어는 .ssh 폴더 안에서 확인 할 수 있습니다. .ssh/[KEYFILE_NAME].pub 를 확인해 볼 수 있습니다. 생성한 키페어를 gcp 의 메타데이터 탭 → SSH 키에 등록합니다. ssh 접속 $ ssh -i ~/.ssh/[KEYFILE_NAME] [USERNAME]@[GCP외부IP] 3. Python3, PIP 설치위에서 서버에 접속했다면, 서버의 개발환경을 설정해 주기만 하면 됩니다. python3 와 pip 부터 설치해 봅니다. locale 설정123456$ sudo apt install language-pack-ko$ sudo locale-gen ko_KR.UTF-8$ export LC_ALL=\"en_US.UTF-8\"$ export LC_CTYPE=\"en_US.UTF-8\"$ sudo dpkg-reconfigure locales en_US.UTF-8이 [*] 로 체크 되어 있는지까지 확인합니다. Python3, PIP 설치12$ sudo apt update$ sudo apt install python3-pip 설치 후, python3 --version 으로 python 이 잘 설치 되었는지 확인합니다. 4. CUDA 설치우리가 가장 원하는 리소스인 gpu를 활용한 연산을 위해 CUDA 를 설치 해 줍니다. 먼저, 설치 파일을 다운로드 해줍니다. 12// 루트에서$ wget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64 ls 로 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb 이 잘 다운로드 되었는지 확인 해 줍니다. 다운로드 결과 .deb 확장자가 되어있지 않다면, 파일명에 .deb 를 뒤에 붙여 줍니다. mv cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb 설치12345$ sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb$ sudo apt-key add /var/cuda-repo-&lt;version&gt;/7fa2af80.pub### $ sudo apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pub$ sudo apt update$ sudo apt install cuda CUDA 가 정상적으로 설치되었다면, $ nvidia-smi 를 통해 자신의 gpu 상태를 확인 할 수 있습니다. 또한, /usr/local/cuda/version.txt 에 설치한 CUDA, 현재는 10.0의 version 을 확인 할 수 있습니다. NVIDA tool-kit 설치1sudo apt install nvidia-cuda-toolkit 5. cuDNN 설치다음의 명령어를 통해 cuDNN 을 설치 할 수 있습니다. 123$ sudo sh -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" &gt;&gt; /etc/apt/sources.list.d/cuda.list'$ sudo apt update$ sudo apt install libcudnn7-dev 6. Pytorch 설치 우리는 서버가 https 프로토콜을 사용한다고 체크했으므로 -H flag 를 주어 sudo pip3 를 활용해야합니다.12$ sudo -H pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl$ sudo -H pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl 7. Jupyter Notebook 설치 및 환경설정 Jupyter를 설치합니다.1$ sudo -H pip3 install jupyter 설치 후, $ jupyter notebook 으로 주피터 커널이 켜지는지 확인합니다. 주피터 config 파일을 생성합니다. 1$ jupyter notebook --generate-config 비밀번호를 생성합니다. 이 비밀번호는 지금 설치한 주피터 환경에 들어가기 위한 비밀번호 입니다. 12345$ ipythonfrom notebook.auth import passwdpasswd()Enter Password: 사용할 비밀번호 입력Verify Password: 사용할 비밀번호 입력 출력된 비밀번호 해쉬 sha1: ~~~ 를 복사해 둡니다. 우리의 인스턴스는 https 프로토콜을 사용하므로, SSL 키파일을 생성해야 합니다.1$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout cert.pem -out cert.pem 위 명령어를 입력하고, 뒤따라 나오는 정보들을 입력하여, .pem 파일을 생성합니다. 에서 생성한 config 파일 수정1$ vi /home/[USERNAME]/.jupyter/jupyter_notebook_config.py 123456// config 파일에 다음의 내용을 추가합니다.c = get_config()c.NotebookApp.ip = &apos;내부아이피주소&apos;c.NotebookApp.open_browser=Falsec.NotebookApp.password=&apos;3에서 생성한 비밀번호 해쉬 sha1:~&apos;c.Notebook.certfile=&apos;4에서 생성한 .pem 파일 경로 /home/USERNAME/cert.pem&apos; 위까지 완료하게 되었으면, 주피터 서버를 열고, https://외부아이피:8888 로 주피터를 접속 하는 것을 확인하시면 되겠습니다.","link":"/2019/06/17/gcp-setting/"},{"title":"math_cheatsheet","text":"이산확률분포베르누이 분포 (bernoulli distribution) 동전 생각하기 결과가 두가지 중 하나 확률변수를 1 or 0 으루 주었을 때,$$\\text{Bern}(x;\\mu) = \\mu^x(1-\\mu)^{(1-x)}$$ 확률변수를 1 or -1 로 주었을 때,$$\\text{Bern}(x; \\mu) = \\mu^{(1+x)/2} (1-\\mu)^{(1-x)/2}$$ 1234#예제코드#베르누이 분포 SciPy#100개 samplingsp.stats.bernoulli(mu).rvs(100, random_state=0) 이항분포 (binomial distribution) 동전을 N번 던졌다 성공확률이 $\\mu$ 인 베르누이 시도를 $N$ 번 반복 할 때, 성공 횟수를 $X$ 라 하면, $$ X \\sim \\text{Bin}(x;N,\\mu) $$ $$ \\text{Bin}(x;N,\\mu) = \\binom N x \\mu^x(1-\\mu)^{N-x} $$ 1sp.stats.binom(N, mu).rvs(100, random_state=0) 카테고리 분포 (Categorical distribution) 주사위 생각해! $K$ 개의 카테고리가 있을 때,$$ x = (x_1, x_2, x_3, x_4, … , x_k) $$ 각 원소 x_i 가 1이 나올 수 있는 확률 ($i$번쨰 원소의 성공확률)을 $\\mu_i$ 라 하면, $$ \\text{Cat}(x;\\mu) = \\mu_1^{x_1} \\mu_2^{x_2} \\cdots \\mu_K^{x_K} = \\prod_{k=1}^K \\mu_k^{x_k} $$ 이 표현은 One-Hot-Encoding 으로 카테고리 분포를 표현 했기에 가능하다. 단, $$ 0 \\leq \\mu_i \\leq 1 $$ $$ \\sum_{k=1}^K \\mu_k = 1 $$ 123#카테고리 분포는 SciPy 의 메소드가 없으므로, 다항분포의 N을 1로 준다.#mu 는 각 term 이 나올 수 있는 확률 vectorsp.stats.multinomial(1, mu) 다항 분포 (Multinomial distribution) 주사위를 여러번 던졌다. 카테고리 시도를 $N$ 번 반복하여 $k$ $(k=1,…,K)$ 가 $x_k$ 번 나올 확률분포. 123#N : number of trial#mu 는 각 term 이 나올 수 있는 확률 vectorsp.stats.multinomial(N, mu)","link":"/2018/11/23/math-cheatsheet/"},{"title":"numpy_cheatsheet","text":"np.arange(start, end) start 에서 end-1 까지의 행렬(array) return np.linspace(start, end, number_of_dots) number_of_dots : 범위 안의 점 갯수 np.zeros_like(x) 0으로 채운 x 와 같은 크기의 행렬","link":"/2018/11/23/numpy-cheatsheet/"},{"title":"[Metrics] PSNR & SSIM","text":"PSNR &amp; SSIMImage Reconstruction을 수행하는 중, Model Selection 을 위한 비교 metric 중 하나인 PSNR, SSIM 을 정리한 글입니다. 1. PSNR1-1. 정의 및 특징Peak Signal-to-Noise Ratio 영상 정보, 화질을 평가할 때 사용되는 Metric. 현재 나는 Image Reconstruction Task 를 수행하며, 다양한 모델의 비교를 위해 사용하기 위해 공부하는 내용이다. 품질이 좋은 이미지는 큰 PSNR 값을 가지며, 품질이 좋지 않은 이미지는 작은 PSNR 값을 가지게 된다. Peak Signal-to-Noise Ratio $$PSNR = 10log_{10}(\\frac{MAX_I^2}{MSE})=20log_{10}(\\frac{MAX_I}{\\sqrt{MSE}}))=20log_{10}(MAX_I)-10log_{10}(MSE)$$ MAXI 는 해당 영상의 최댓값, 해당 채널의 최댓값에서 최솟값을 빼서 구함 e.g. 8-bit gray scale 의 경우, 255-0 = 255 단위: db(log scale) 무손실 영상의 경우 MSE 가 0이기 때문에, PSNR 은 정의 되지 않는다. $$MSE = \\frac{1}{mn}\\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1}[I(i,j)-K(i, j)]^2$$ I : mxn 사이즈의 grayscale image K: I 에 잡음이 포함된 이미지 (왜곡된 이미지) 1-2. 한계PSNR은 intensity 의 값들을 위 식에 의해 종합하여 평가하는 방식이기 때문에, 실제로 사람이 봤을 때 느끼는 것과 다른 점수를 뱉어낼 때가 있다. 즉, 사람의 지각품질을 제대로 반영하지 못하기 때문에 이를 보완하기 위해 PSNR-HVS, PSNR-HVS-M, SSIM, VIF 등의 Metric 이 개발되었다. 2. SSIM2-1. 정의 및 특징위 PSNR 의 한계를 극복하기 위해, 개발된 metric 으로써, Structural Similarity 의 줄임말이다. 사람의 지각 능력과 metric 을 일치시키는 목적에 개발되었다. 사람은 영상에서 구조 정보를 반영하여 영상을 바라보게 되는데, 영상이 얼마나 그 구조 정보를 변화시키지 않았는가를 살펴보는 metric 이다. 즉, 원본이미지(x)와 왜곡이미지(y)의 Luminance(l), Contrast(c), Structure(s)를 비교한다. contrast: 이미지의 표준편차값 structure: (이미지-평균밝기) / 표준편차 $$l(x, y)=\\frac{2\\mu_x \\mu_y+c_1}{\\mu_x^2+\\mu_y^2+c_1}$$ $$c(x,y)=\\frac{2\\sigma_x\\sigma_y+c_2}{\\sigma_x^2+\\sigma_y^2+c_2}$$ $$s(x,y)=\\frac{\\sigma_{xy}+c_3}{\\sigma_x\\sigma_y+c_3}$$ $$SSIM(x,y)=[l(x,y)^{\\alpha}c(x,y)^{\\beta}s(x,y)^{\\gamma}]$$ $$c_3=c_2/2$$ 위에 c3 와 c2 의 조건을 추가하면 SSIM 은 다음과 같이 축약된다. 3. Reference https://kr.mathworks.com/help/images/ref/ssim.html https://bskyvision.com/ https://en.wikipedia.org/wiki/Structural_similarity https://ko.wikipedia.org/wiki/","link":"/2019/09/01/psnr-ssim/"},{"title":"[세미나요약] 데이터 분석, 의심에서 전달까지","text":"2021년 10월 8일, 한국에너지기술연구원 이제현 박사님이 진행하신 ‘데이터 분석, 의심에서 전달까지’ 제목으로 진행된 세미나를 참석했습니다. 본 글은 세미나 내용의 요약입니다. [세미나 발표 PDF 자료]세미나 발표 pdf 자료 1. 데이터 의심하기1-1. 레퍼런스의 중요성 분석결과의 신뢰도, 결과의 책임을 위해 데이터의 출처가 명확해야한다. 출처가 분명하지 않은 데이터는 사용하지 않는 것이 최선 1-2. 데이터의 건정성 데이터 자체의 건정성 결측치 : 데이터가 없음 → 데이터가 없는 것도 가끔은 중요한 메시지가 될 수 있다. (예: 이미지 데이터 내의 까만 이미지 → 빛이 없다고 해석 가능) 결측치에 대한 해석은 결국, 데이터의 도메인 관점에서 생각 중복데이터 : 같은 데이터가 여러개 → Key feature 를 중심으로 논리적으로 판단해야한다. 중복데이터에 대해 일괄적인 처리 이전에 고민해보아야 할 문제 (예: 같은 자리에 건물이 겹쳐 있음 : 비정상적인 데이터, 같은 시간 같은 가게에 손님이 두명 : 정상적인 데이터) 이상치 : (사전적의미) 정상적인 범위에서 벗어나는 데이터 → 통계 분석을 통해 이상치 후보군을 추리고, 도메인 접근을 통해 진짜 이상치인지 판별해야한다. (예: 대학 중퇴자가 왜 소득이 높지? → 빌게이츠, 스티브잡스, 마크 저커버그, 잭도시 등) 이상치로 볼수 있는 데이터가 있다 하더라도, 내가 하는 분석의 목적에 따라서 데이터가 이상치 일수도, 아닐수도 있다. 기계적인 처리는 하면 안됨 1-3. 너무 믿지 말아야할 데이터 너무 믿지 말아야할 데이터 : (예: 영화 장르데이터 → 해리포터 1, 2, 3,4 … 의 장르가 모두 다름. 어떤 절대적 기준에 의해 장르 구분이 된 것이 아니라, 누군가의 주관적 관점으로 새긴 데이터) 1-4. 필요 데이터 본격적인 프로젝트 시행 이전에, 필요한 데이터를 빠르게 계획하고, 살펴본뒤 레퍼런스 체크를 시행해야한다. 1-5. 데이터 파악 데이터 파악 : 통계치는 같은데 그림을 그리면 다르게 해석된다. 반드시 데이터는 그려보아야 한다. (예: 데이터 사우르스, 통계가 얼마나 눈을 가리는지 보여주는 예시) 데이터를 제대로 의심하는 방법 : Exploratory Data Analysis. 데이터를 받았을 때 이렇게 저렇게 찾아보는 과정. (예: 장님이 코끼리를 만져보는 과정) 한명의 장님(나) 여러 방법으로 그려보고 살펴보면서 데이터를 3d 다양한 관점에서 바라보아야 한다. 데이터에 대해 그림을 그릴 때마다 가설을 세우고, 다음 그림을 그릴 때 내 가설을 보완해나아가야함. 마치 셀프 강화학습 처럼 결국 EDA + Hypothesis + Graph = self-강화학습 2. 분석 방법 의심하기 사장님이 감자를 잘라달라 감자 썰기의 관건: 무엇을 만들 것인가 요리의 목적에 따라 다양한 감자 형태, 다양한 조리방식이 있을 수 있다 데이터 분석: 무엇을 할 것인가? 데이터 분석의 관건: 무엇을 위한 분석인가 현황 분석(현황 내용전달), 대안 제시 (설득력: 대안의 장점과 단점), 예측모델개발 (신뢰성: 검증결과, 예상오차) 등 목적에 따라 데이터 뿐만 아니라 그에 따른 다양한 데이터 분석 방법이 필요하다 망치와 모루 전략 (Hammer and Anvil Tactic) 모루가 버티는 동안 망치가 때린다 (마케도니아 알렉산더 대왕) 모루가 중요하다: 지지않아야하고, 이겨한다 → 수학적 엄밀함 (다양한 통계분석방법, T 검정, 층화 추출, 다양한 매트릭, 정규화, 카이제곱, 교차검증 등) 통계학은 의심의 학문이다 : 내가 전체 데이터를 모두 볼 수 없기 때문에 부분만 보고, 부분이 전체에 적용이 될지 → 이 불안을 해소하기 위한 것이 통계학, 수학 망치 : 나만의 인사이트 → 아무도 못한 생각을 통해 전쟁에서 이기자 인사이트 도출 방법: 데이터 자르기 (Segment) Airbnb “국내, 300마일” 인사이트 도출방법: 독창적 시각화 (Visualization) 나이팅게일 → 다쳐서가 아니라 더러워서 사람이 죽는다. Hans Rosling, Ted, 2007 → Animated bubble chart : 세상은 점점 나아지고 있다, 경향성을 새롭게 보여줌 Danny Dorling → Slow Down 가속 성장의 시대는 끝났다. 송강호 → 배우는 오담을 가져온다. 알고보면 그 오답이 진짜 정답이다. 데이터를 받으면, 공부한대로, 정해진 루틴대로 분석하고 visualization 을 진행함.. 남들도 그렇게 하고 있다. 세상에 없는 방법? 남들보다 더 많은 시간과 정성 쏟기 + 스스로 생각하기 3. 고객에게 잘 전달하기 결과를 보고하는 데이터 분석가의 주의사항 내 업무 시간 순이 아니라 상대방 논리에 따라 보고하기 결론 없이 사실만 나열하면 안됨 경영 용어가 아닌 통계 용어를 남발하면 안됨 분석을 원하는 사람들의 진짜 원하는 것 찾아내기 자신들 조차 원하는게 뭔지 정확히 모른다 다양한 질문들과 상황을 통해 그들이 원하는 것이 무엇인지 파악 데이터 vs 도메인 영문과 교수예시: 우리 영문과는 영어가 모국어처럼 입에 붙어야 비로소 국문과랑 같은 출발선에 서는거다. 초벌 데이터 분석으로 알아내는 것 = 도메인에서는 모두 알고 있는 것 데이터를 분석해서 실무자에게 공유해야 하는 내용 + 흥미를 보이는 지점, 애매하게 파악하고 있는 부분 캐치 + 심층 분석 4. 끊임없는 의심 내 분석결과를 살표본다 : Insight 내 아이디어를 빠르게 적용 : Agile, 코딩잘하기 호기심 : 흥미요소 5. 경계하는 자세 생명에 대한 예의 국가별 코로나 19 사망자 데이터 분석 안좋은 소식에 대한 분석결과는 tone &amp; manner 를 지켜서.. 데이터 밖의 세계 컴퓨터, 데이터 속에서만 사는가? vs 어차피, 결국 숫자일 뿐이야? 식사를 마친 손님 : 얼마나 좋은 재료, 어떤 기법을 사용한 요리? X → 음식이 제때 나오고, 맛있게 배부른 곳 → 음식을 어떻게 만들었는지는 그 다음 문제 데이터 분석 : 원래 목적을 잘 파악해서 다가가는게 중요 → 어떤 분석방법, 시각화는 나중, 남들이 잘 하지 않는 시도를 많이 해보기 세미나 발표 pdf 자료","link":"/2021/10/10/summary-seminar-EDA/"},{"title":"[논문읽기] Summary of Vision and Rain","text":"Presentation for summary papers Initial Model for removing rain and snow from Image system. Summary of the paper 'detection and removal of rain from video, vision and rain' from ssuser47e145","link":"/2019/08/20/summary-vision-and-rain/"}],"tags":[{"name":"TWIL","slug":"TWIL","link":"/tags/TWIL/"},{"name":"CS224n","slug":"CS224n","link":"/tags/CS224n/"},{"name":"summary","slug":"summary","link":"/tags/summary/"},{"name":"CS231n","slug":"CS231n","link":"/tags/CS231n/"},{"name":"Classification, 확률론적생성모형, Generative, 생성모형, 분류모델","slug":"Classification-확률론적생성모형-Generative-생성모형-분류모델","link":"/tags/Classification-확률론적생성모형-Generative-생성모형-분류모델/"},{"name":"mysql, MYSQL, database","slug":"mysql-MYSQL-database","link":"/tags/mysql-MYSQL-database/"},{"name":"nginx, proxy","slug":"nginx-proxy","link":"/tags/nginx-proxy/"},{"name":"pillow, Pillow, image","slug":"pillow-Pillow-image","link":"/tags/pillow-Pillow-image/"},{"name":"datastructure","slug":"datastructure","link":"/tags/datastructure/"},{"name":"Crawling, Web, Requests","slug":"Crawling-Web-Requests","link":"/tags/Crawling-Web-Requests/"},{"name":"scrapy, crawling, web","slug":"scrapy-crawling-web","link":"/tags/scrapy-crawling-web/"},{"name":"자동화, selenium, webdriver","slug":"자동화-selenium-webdriver","link":"/tags/자동화-selenium-webdriver/"},{"name":"datastructure, stack","slug":"datastructure-stack","link":"/tags/datastructure-stack/"},{"name":"Thread","slug":"Thread","link":"/tags/Thread/"},{"name":"Gan","slug":"Gan","link":"/tags/Gan/"},{"name":"Vanilla Gan","slug":"Vanilla-Gan","link":"/tags/Vanilla-Gan/"},{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"논문","slug":"논문","link":"/tags/논문/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"decorator","slug":"decorator","link":"/tags/decorator/"},{"name":"데코레이터","slug":"데코레이터","link":"/tags/데코레이터/"},{"name":"iterator","slug":"iterator","link":"/tags/iterator/"},{"name":"generator","slug":"generator","link":"/tags/generator/"},{"name":"iterable","slug":"iterable","link":"/tags/iterable/"},{"name":"이터레이터","slug":"이터레이터","link":"/tags/이터레이터/"},{"name":"제너레이터","slug":"제너레이터","link":"/tags/제너레이터/"},{"name":"이터러블","slug":"이터러블","link":"/tags/이터러블/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"리눅스","slug":"리눅스","link":"/tags/리눅스/"},{"name":"crontab","slug":"crontab","link":"/tags/crontab/"},{"name":"크론탭","slug":"크론탭","link":"/tags/크론탭/"},{"name":"Google Cloud Platform","slug":"Google-Cloud-Platform","link":"/tags/Google-Cloud-Platform/"},{"name":"Data Analysis","slug":"Data-Analysis","link":"/tags/Data-Analysis/"},{"name":"Data Science","slug":"Data-Science","link":"/tags/Data-Science/"},{"name":"Seminar","slug":"Seminar","link":"/tags/Seminar/"}],"categories":[{"name":"MachineLearning","slug":"MachineLearning","link":"/categories/MachineLearning/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"},{"name":"Pytorch","slug":"MachineLearning/Pytorch","link":"/categories/MachineLearning/Pytorch/"},{"name":"Lecture","slug":"Lecture","link":"/categories/Lecture/"},{"name":"CS231n","slug":"Lecture/CS231n","link":"/categories/Lecture/CS231n/"},{"name":"CS224n","slug":"Lecture/CS224n","link":"/categories/Lecture/CS224n/"},{"name":"Book","slug":"Book","link":"/categories/Book/"},{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"딥러닝을 이용한 자연어 처리","slug":"Lecture/딥러닝을-이용한-자연어-처리","link":"/categories/Lecture/딥러닝을-이용한-자연어-처리/"},{"name":"wiki","slug":"wiki","link":"/categories/wiki/"},{"name":"DataStructure","slug":"DataStructure","link":"/categories/DataStructure/"},{"name":"Apache Spark","slug":"MachineLearning/Apache-Spark","link":"/categories/MachineLearning/Apache-Spark/"},{"name":"Development","slug":"Development","link":"/categories/Development/"},{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Digital Image Processing","slug":"Book/Digital-Image-Processing","link":"/categories/Book/Digital-Image-Processing/"},{"name":"Google Cloud Platform","slug":"Google-Cloud-Platform","link":"/categories/Google-Cloud-Platform/"},{"name":"Vision","slug":"MachineLearning/Vision","link":"/categories/MachineLearning/Vision/"},{"name":"MySQL","slug":"wiki/MySQL","link":"/categories/wiki/MySQL/"},{"name":"Data Science","slug":"Data-Science","link":"/categories/Data-Science/"},{"name":"NGINX","slug":"wiki/NGINX","link":"/categories/wiki/NGINX/"},{"name":"Pillow","slug":"wiki/Pillow","link":"/categories/wiki/Pillow/"},{"name":"Provision","slug":"wiki/Provision","link":"/categories/wiki/Provision/"},{"name":"Requests","slug":"wiki/Requests","link":"/categories/wiki/Requests/"},{"name":"Scrapy","slug":"wiki/Scrapy","link":"/categories/wiki/Scrapy/"},{"name":"Selenium","slug":"wiki/Selenium","link":"/categories/wiki/Selenium/"},{"name":"Python","slug":"Development/Python","link":"/categories/Development/Python/"},{"name":"Xpath","slug":"wiki/Xpath","link":"/categories/wiki/Xpath/"},{"name":"Linux","slug":"Development/Linux","link":"/categories/Development/Linux/"},{"name":"Seminar","slug":"Data-Science/Seminar","link":"/categories/Data-Science/Seminar/"}]}