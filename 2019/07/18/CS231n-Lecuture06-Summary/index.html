
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Emjay, 오늘도 새롭게">
    <title>[CS231n]Lecture06-Training Neural Networks part1 - Emjay, 오늘도 새롭게</title>
    <meta name="author" content="EmjayAhn(Minjae Ahn)">
    
        <meta name="keywords" content="데이터사이언스,머신러닝,딥러닝,개발,machine learning,deep learning,datascience,datascientist,">
    
    
    
        
            <link rel="alternate" type="application/rss+xml" title="RSS" href="/rss2.xml">
        
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"EmjayAhn(Minjae Ahn)","sameAs":["https://github.com/emjayahn","https://facebook.com/jjminjae","mailto"]},"articleBody":"Lecture 06: Training Neural Networks part1\n이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.\n\n\n1. One time setup1-1. Activation FunctionsActivation Function 마다 각 특성과 trade off 가 있다. 많이 사용되는 activation function 이 있지만, LU 계열의 activation function 은 모두 실험의 parameter가 될 수 있다.\n(1) Sigmoid\n\\sigma(x) = \\frac{1}{1+e^{-x}}\nsigmoid function 의 경우, 수식의 특성상 실수 space 에 있는 값들을 [0, 1] 범위 내로 좁혀 주는 기능을 해, 역사적으로 오래된 activation function 이다. 하지만 다음과 같은 단점이 있다.\n\n단점\n일정 범위(Saturated 되는 범위)부터 gradient가 0에 가까워진다.\nsigmoid 의 출력 결과가 zero-centered 가 되지 않는다.\nminor 한 단점이지만, exponential 의 계산이 비효율적이다.(비싸다고 표현)\n\n\n\n왜 zero-centered 가 중요한가?\nf(\\sum_iw_ix_i+b)강의에서 설명해준 직관적인 방법이 매우 도움이 되었다.\nBackpropagation 을 생각해 보게 되면, Neuron(Node) 안에서 local gradient 와 loss 에서 부터 오는 upstream gradient 가 곱해지게 된다. 위 식에서 w 에 대해 local gradient 를 구하면 x_i 가 되는 것을 확인 할 수 있다. 만약 xi 가 모두 양수라고 가정한다면, f()의 gradient 는 항상 양수 또는 음수이고, 이는 w 가 모두 같은 방향으로 움직인다는 것을 의미한다. 즉, 비효율적인 gradient update 라고 할 수 있다. \n(2) hyper tangent : Tanh(x)\ntanh(x)=\\frac{e^{2x}-1}{e^{2x}+1}\n위와 같은 zero-centered problem 을 해결하기 위해 tanh(x) 를 사용할 수 도 있다. (추가적으로, 수업시간에 언급은 없었으나, tanh의 등장배경을 설명하는 또다른 한가지는 sigmoid  와 tanh 의 gradient의 최댓값이 4배 차이가 나므로, backpropagation 에서 gradient vanishing 현상을 방지하는 기대효과로 설명하기도 한다.)하지만, 이 역시 sigmoid 처럼 saturate 되는 부분에서 gradient 가 0이 되는 현상이 아직 남아있다.\n(3) ReLU\nf(x) = max(0, x)\nReLU 의 경우, 가장 우리가 많이 볼 수 있는 activation function 이다. 다음과 같은 장점이 있다고 설명한다.\n\n장점\n양수인 구간에서는 gradient 가 0이 되지 않고\n계산이 효율적이다.\nsigmoid 와 tanh 에 비해 실제로 6배 빠른 converge 성능을 보여준다.\n\n\n단점\nzero-centered output이 아니다.\n0보다 작은 구간에서는 gradient 가 0이 된다.\n\n\n\n(4) Leaky ReLU\nf(x) = max(0.01x, x)\n0보다 작은 구간에서 ReLU 처럼 Saturate 되는 단점을 없애고자, 0보다 작은 구간에서 작은 gradient 를 주는 것이 Leary Relu 이다. \n\n장점\n양수 구간 뿐만아니라 음수 구간에서 gradient 를 작게 주어 gradient 가 0 이 되지 않게 한다.\nReLU function 과 마찬가지로, 계산이 효율적이고\nsigmoid 와 tanh 에 비해 빠르게 수렴한다.\n\n\n\nParametric Rectifier(PReLU) 라는 이름으로, 좀더 generalize 된 형태도 사용한다. $max(\\alpha x, x)$ 형태로써, alpha 를 고정시키지 않고 학습시키는 형태이다.\n(5) Exponential Linear Units (ELU)\nf(x)=\\begin{cases}x \\quad\\quad\\quad\\quad if \\quad x>0 \\\\ \n\\alpha(exp(x) -1) \\quad if \\quad x\\leq 0 \\end{cases}\n(6) Maxout Neuron\nReLU function 의 Generalize 된 형태라고 생각할 수 있다.\nmax(w_1^{T}x + b_1, w_2^Tx + b_2)하지만, 이 형태는 각 뉴런마다 두 배의 parameter를 가지고 그 output 값 간의 비교를 하므로, 연산량이 두배가 많아지는 단점이 있다.\n(6) Summary\n\nReLU 를 사용한다!\nLeaky ReLU, Maxout, ELU 를 실험해볼 수 있다.\ntanh 도 실험할 수 있지만, 큰 효능을 기대하기 힘들다\nDon’t Use sigmoid\n\n1-2. Data Preprocessing앞서 살펴 보았던 것 처럼, 입력 데이터에 있어서 zero-centered 가 매우 중요한 preprocessing key 라고 생각 할 수 있다. 머신러닝에서 처럼 다양한 normalized 기법과 whitening 기법들이 있지만, 뉴럴넷, 특히 이미지 데이터에 대해서는 zero-centered 까지만 전처리 해준다. 이는 모든 차원의 데이터가 같은 범위안에 있게 함으로써 각 차원이 equally contribute 하게 하기 위함이다.\n\n1-3. Weight Initialization우리가 설정하는 각 layer 의 weight 을 어떻게 초기화 해줄 것인가의 문제도 뉴럴넷의 학습과 성능에 영향이 있을 수 있다. 작은 gaussian random number 로 모든 weight 을 초기화 해 줄 경우, layer 를 지날 수록 activation function 을 거친 작은 output 과 초기화 된  작은 w 가 곱해져, 점차 그 분산이 작아 지는 것을 확인 할 수 있다. \n\n따라서, 이런 가우시안 랜덤으로 초기화 해주는 것이 아닌 다른 초기화 방법이 등장하였다.\n\nXavier Initialization &amp; He Initialization\n\n개인적으로 Xavier Initialization 과 He Initialization 을 간단하게 실험해 본 내용은 나의 github에 올려두었다. 비교군 설정에 있어, 미흡하지만 직관적으로 이해하기 쉽게 실험해 본 내용이다. 간단한 api 를 통해 구현하였기에, 자세한 수식과 개념은 해당 논문을 읽어보고, 정리해봐야겠다.\ngithub: Xavier vs He experiment\n1-4. Batch Normalization결국 우리는 모든 activation 이 unit gaussian form  이길 원한다. 위에서도 살펴 보았듯이, activation function  에 들어가기 전에 모든 fully connected layer 의 출력이 saturate 구간에 있게 하고 싶지 않은 것이다. 따라서, 주로 Batch Normalization 은 Fully Connected Layer 이후 비선형함수 전 또는 Convolution Layer 다음 에 들어가게 된다.\n이 때, 주의 해야할 점은 Convolution Layer 다음에 들어가는 batch normalize 는 activation map 으로 나온 channel 별로 수행해주게 된다.\n\nbatch mean 과 variance 는 각 차원별로 계산해 준다.\nNormalize \n\n\\hat{x}^{(k)} = \\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}앞서, batch normalization 의 목적은 network의 forward, backward pass 시 saturation 이 일어나지 않게 하기 위함이었습니다. 하지만 반면에, 이렇게 batch normalization 을 해준 이후에도 우리는 network 이 얼마나 해당 activation 을 얼마나 saturate 시킬지까지 학습 할 수 있다면 얼마나 좋을까요? 따라서 우리는 normalize 이후에 scaling factor 와 shifting factor 를 추가시켜, 얼마나 saturate 시킬지까지 학습할 수 있는 parameter 를 추가합니다.\ny^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{k}Batch Noramlization 에 대한 pseudo 알고리즘이다.\n\n또한, 우리는 이렇게 학습한 batch mean 과 variance  를 학습 때 사용하며, testing (inference) 시에는 다시 계산하지 않는 것을 유의해야한다. testing 시에는 training 시 running average 등의 방식으로 고정된 mean과 variance 등을 사용할 수 있다.\n2. Training Dynamics2-1. Babysitting the Learning Process\nPreprocess the data\nBuild Model\nSanity check for model (e.g. weigh이 작을ㄹ 때, loss가 negative log likelihood 와 비슷한지, regularization term이 추가될때 loss 가 증가하는지 등)\n(regularization term 을 사용하지 않고) 매우 작은 데이터에 대해서, train  을 돌렸을 때, loss 가 떨어지고, 금방 overfitting 되는지 확인여기까지가 sanity check 이라면, 이제 본격적인 training!!\n간단한 몇가지 실험을 통해 learning rate 을 정한다. 큰 값, 작은 값을 넣어보고, epoch 을 10까지 정도로 주었을 때, loss 가 주는 지 확인하여 대략적인 범위를 정한다.\n\nCoarse search: learning rate 과 다른 hyper parameter 를 uniform 등과 같은 distribution 을 통해 random search 한다. 강의에서 말했던, 주의사항 : 범위의 양 끝에 가장 좋은 score 가 뿌려져 있다면, 다시 범위를 설정해야한다. 내가 처음 설정한 범위 끝단에 존재한다면 그 주변에서 다시 최적의 값이 존재 할 수 있기 때문이다.\n\nFiner search: 최적의 값을 찾기 위해 세세하게 튜닝한다.\n\n\n2-2. Hyperparameter Optimization\n많은 양의 Cross Validation 을 통해 성능을 비교, 검증하며 최적의 hyper parameter 를 찾아야한다!\nGrid Search &lt;&lt; Random Search\nRandom Sampling 을 통해 좀더 중요한 parameter 의 분포를 찾아 낼 수 있어, Random search 가 효과적\n\n\n\n3. ReferenceLecture 6 | Training Neural Networks I\nSyllabus | CS 231N\n","dateCreated":"2019-07-18T20:38:56+09:00","dateModified":"2019-07-18T20:55:58+09:00","datePublished":"2019-07-18T20:38:56+09:00","description":"Lecture 06: Training Neural Networks part1\n이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.\n","headline":"[CS231n]Lecture06-Training Neural Networks part1","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"},"publisher":{"@type":"Organization","name":"EmjayAhn(Minjae Ahn)","sameAs":["https://github.com/emjayahn","https://facebook.com/jjminjae","mailto"]},"url":"https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/","keywords":"summary, CS231n"}</script>
    <meta name="description" content="Lecture 06: Training Neural Networks part1 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.">
<meta name="keywords" content="summary,CS231n">
<meta property="og:type" content="blog">
<meta property="og:title" content="[CS231n]Lecture06-Training Neural Networks part1">
<meta property="og:url" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/index.html">
<meta property="og:site_name" content="Emjay, 오늘도 새롭게">
<meta property="og:description" content="Lecture 06: Training Neural Networks part1 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-f80c60bd-97fe-4db9-a0a3-3a8a8c38907c.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-c1d35d05-3ea2-49a8-9e8a-5c85bbc519a7.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-5850722c-4016-4335-b77a-d5237ed6d5b6.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-ed8a9855-d2ce-4756-93d5-e042e0a7b823.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-b527e985-4c1c-475f-a01a-6f8bdb006ceb.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-417f2e92-c3d6-4bc1-825e-7dfc79a52956.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-96d3f546-0140-4ccf-b6b1-925eb79dba94.png">
<meta property="og:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-83e0b3e7-d764-4a19-9c28-9fe815096b28.png">
<meta property="og:updated_time" content="2019-07-18T11:55:58.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[CS231n]Lecture06-Training Neural Networks part1">
<meta name="twitter:description" content="Lecture 06: Training Neural Networks part1 이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.">
<meta name="twitter:image" content="https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/Untitled-f80c60bd-97fe-4db9-a0a3-3a8a8c38907c.png">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/all.css">
    <link rel="stylesheet" href="/assets/css/jquery.fancybox.css">
    <link rel="stylesheet" href="/assets/css/thumbs.css">
    <link rel="stylesheet" href="/assets/css/tranquilpeak.css">
    <!--STYLES END-->
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-128251719-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-128251719-1');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    

    
        
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Emjay, 오늘도 새롭게
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link open-algolia-search"
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/emjayahn"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://facebook.com/jjminjae"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Facebook"
                        >
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/mailto"
                            
                            rel="noopener"
                            title="Mail"
                        >
                        <i class="sidebar-button-icon fa fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/atom.xml"
                            
                            rel="noopener"
                            title="RSS"
                        >
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            [CS231n]Lecture06-Training Neural Networks part1
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-07-18T20:38:56+09:00">
	
		    Jul 18, 2019
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Lecture/">Lecture</a>, <a class="category-link" href="/categories/Lecture/CS231n/">CS231n</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="Lecture-06-Training-Neural-Networks-part1"><a href="#Lecture-06-Training-Neural-Networks-part1" class="headerlink" title="Lecture 06: Training Neural Networks part1"></a>Lecture 06: Training Neural Networks part1</h1><ul>
<li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li>
</ul>
<a id="more"></a>
<h2 id="1-One-time-setup"><a href="#1-One-time-setup" class="headerlink" title="1. One time setup"></a>1. One time setup</h2><h3 id="1-1-Activation-Functions"><a href="#1-1-Activation-Functions" class="headerlink" title="1-1. Activation Functions"></a>1-1. Activation Functions</h3><p>Activation Function 마다 각 특성과 trade off 가 있다. 많이 사용되는 activation function 이 있지만, LU 계열의 activation function 은 모두 실험의 parameter가 될 수 있다.</p>
<p>(1) Sigmoid</p>
<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><p><img src="Untitled-f80c60bd-97fe-4db9-a0a3-3a8a8c38907c.png" alt></p>
<p>sigmoid function 의 경우, 수식의 특성상 실수 space 에 있는 값들을 [0, 1] 범위 내로 좁혀 주는 기능을 해, 역사적으로 오래된 activation function 이다. 하지만 다음과 같은 단점이 있다.</p>
<ul>
<li>단점<ul>
<li>일정 범위(Saturated 되는 범위)부터 gradient가 0에 가까워진다.</li>
<li>sigmoid 의 출력 결과가 zero-centered 가 되지 않는다.</li>
<li>minor 한 단점이지만, exponential 의 계산이 비효율적이다.(비싸다고 표현)</li>
</ul>
</li>
</ul>
<p>왜 <strong>zero-centered</strong> 가 중요한가?</p>
<script type="math/tex; mode=display">f(\sum_iw_ix_i+b)</script><p>강의에서 설명해준 직관적인 방법이 매우 도움이 되었다.</p>
<p>Backpropagation 을 생각해 보게 되면, Neuron(Node) 안에서 local gradient 와 loss 에서 부터 오는 upstream gradient 가 곱해지게 된다. 위 식에서 w 에 대해 local gradient 를 구하면 x_i 가 되는 것을 확인 할 수 있다. 만약 xi 가 모두 양수라고 가정한다면, f()의 gradient 는 항상 양수 또는 음수이고, 이는 w 가 모두 같은 방향으로 움직인다는 것을 의미한다. 즉, 비효율적인 gradient update 라고 할 수 있다. </p>
<p>(2) hyper tangent : Tanh(x)</p>
<script type="math/tex; mode=display">tanh(x)=\frac{e^{2x}-1}{e^{2x}+1}</script><p><img src="Untitled-c1d35d05-3ea2-49a8-9e8a-5c85bbc519a7.png" alt></p>
<p>위와 같은 zero-centered problem 을 해결하기 위해 tanh(x) 를 사용할 수 도 있다. (추가적으로, 수업시간에 언급은 없었으나, tanh의 등장배경을 설명하는 또다른 한가지는 sigmoid  와 tanh 의 gradient의 최댓값이 4배 차이가 나므로, backpropagation 에서 gradient vanishing 현상을 방지하는 기대효과로 설명하기도 한다.)하지만, 이 역시 sigmoid 처럼 saturate 되는 부분에서 gradient 가 0이 되는 현상이 아직 남아있다.</p>
<p>(3) ReLU</p>
<script type="math/tex; mode=display">f(x) = max(0, x)</script><p><img src="Untitled-5850722c-4016-4335-b77a-d5237ed6d5b6.png" alt></p>
<p>ReLU 의 경우, 가장 우리가 많이 볼 수 있는 activation function 이다. 다음과 같은 장점이 있다고 설명한다.</p>
<ul>
<li>장점<ul>
<li>양수인 구간에서는 gradient 가 0이 되지 않고</li>
<li>계산이 효율적이다.</li>
<li>sigmoid 와 tanh 에 비해 실제로 6배 빠른 converge 성능을 보여준다.</li>
</ul>
</li>
<li>단점<ul>
<li>zero-centered output이 아니다.</li>
<li>0보다 작은 구간에서는 gradient 가 0이 된다.</li>
</ul>
</li>
</ul>
<p>(4) Leaky ReLU</p>
<script type="math/tex; mode=display">f(x) = max(0.01x, x)</script><p><img src="Untitled-ed8a9855-d2ce-4756-93d5-e042e0a7b823.png" alt></p>
<p>0보다 작은 구간에서 ReLU 처럼 Saturate 되는 단점을 없애고자, 0보다 작은 구간에서 작은 gradient 를 주는 것이 Leary Relu 이다. </p>
<ul>
<li>장점<ul>
<li>양수 구간 뿐만아니라 음수 구간에서 gradient 를 작게 주어 gradient 가 0 이 되지 않게 한다.</li>
<li>ReLU function 과 마찬가지로, 계산이 효율적이고</li>
<li>sigmoid 와 tanh 에 비해 빠르게 수렴한다.</li>
</ul>
</li>
</ul>
<p>Parametric Rectifier(PReLU) 라는 이름으로, 좀더 generalize 된 형태도 사용한다. $max(\alpha x, x)$ 형태로써, alpha 를 고정시키지 않고 학습시키는 형태이다.</p>
<p>(5) Exponential Linear Units (ELU)</p>
<script type="math/tex; mode=display">f(x)=\begin{cases}x \quad\quad\quad\quad if \quad x>0 \\ 
\alpha(exp(x) -1) \quad if \quad x\leq 0 \end{cases}</script><p><img src="Untitled-b527e985-4c1c-475f-a01a-6f8bdb006ceb.png" alt></p>
<p>(6) Maxout Neuron</p>
<p>ReLU function 의 Generalize 된 형태라고 생각할 수 있다.</p>
<script type="math/tex; mode=display">max(w_1^{T}x + b_1, w_2^Tx + b_2)</script><p>하지만, 이 형태는 각 뉴런마다 두 배의 parameter를 가지고 그 output 값 간의 비교를 하므로, 연산량이 두배가 많아지는 단점이 있다.</p>
<p>(6) Summary</p>
<ul>
<li>ReLU 를 사용한다!</li>
<li>Leaky ReLU, Maxout, ELU 를 실험해볼 수 있다.</li>
<li>tanh 도 실험할 수 있지만, 큰 효능을 기대하기 힘들다</li>
<li><strong>Don’t Use sigmoid</strong></li>
</ul>
<h3 id="1-2-Data-Preprocessing"><a href="#1-2-Data-Preprocessing" class="headerlink" title="1-2. Data Preprocessing"></a>1-2. Data Preprocessing</h3><p>앞서 살펴 보았던 것 처럼, 입력 데이터에 있어서 zero-centered 가 매우 중요한 preprocessing key 라고 생각 할 수 있다. 머신러닝에서 처럼 다양한 normalized 기법과 whitening 기법들이 있지만, 뉴럴넷, 특히 이미지 데이터에 대해서는 zero-centered 까지만 전처리 해준다. 이는 모든 차원의 데이터가 같은 범위안에 있게 함으로써 각 차원이 equally contribute 하게 하기 위함이다.</p>
<p><img src="Untitled-417f2e92-c3d6-4bc1-825e-7dfc79a52956.png" alt></p>
<h3 id="1-3-Weight-Initialization"><a href="#1-3-Weight-Initialization" class="headerlink" title="1-3. Weight Initialization"></a>1-3. Weight Initialization</h3><p>우리가 설정하는 각 layer 의 weight 을 어떻게 초기화 해줄 것인가의 문제도 뉴럴넷의 학습과 성능에 영향이 있을 수 있다. 작은 gaussian random number 로 모든 weight 을 초기화 해 줄 경우, layer 를 지날 수록 activation function 을 거친 작은 output 과 초기화 된  작은 w 가 곱해져, 점차 그 분산이 작아 지는 것을 확인 할 수 있다. </p>
<p><img src="Untitled-96d3f546-0140-4ccf-b6b1-925eb79dba94.png" alt></p>
<p>따라서, 이런 가우시안 랜덤으로 초기화 해주는 것이 아닌 다른 초기화 방법이 등장하였다.</p>
<ul>
<li>Xavier Initialization &amp; He Initialization</li>
</ul>
<p>개인적으로 Xavier Initialization 과 He Initialization 을 간단하게 실험해 본 내용은 나의 github에 올려두었다. 비교군 설정에 있어, 미흡하지만 직관적으로 이해하기 쉽게 실험해 본 내용이다. 간단한 api 를 통해 구현하였기에, 자세한 수식과 개념은 해당 논문을 읽어보고, 정리해봐야겠다.</p>
<p>github: <a href="https://github.com/EmjayAhn/TIL/blob/master/03_Pytorch/02_xavier%26he.ipynb" target="_blank" rel="noopener">Xavier vs He experiment</a></p>
<h3 id="1-4-Batch-Normalization"><a href="#1-4-Batch-Normalization" class="headerlink" title="1-4. Batch Normalization"></a>1-4. Batch Normalization</h3><p>결국 우리는 모든 activation 이 unit gaussian form  이길 원한다. 위에서도 살펴 보았듯이, activation function  에 들어가기 전에 모든 fully connected layer 의 출력이 saturate 구간에 있게 하고 싶지 않은 것이다. 따라서, 주로 Batch Normalization 은 <strong>Fully Connected Layer 이후 비선형함수 전</strong> 또는 <strong>Convolution Layer 다음</strong> 에 들어가게 된다.</p>
<p>이 때, 주의 해야할 점은 Convolution Layer 다음에 들어가는 batch normalize 는 activation map 으로 나온 channel 별로 수행해주게 된다.</p>
<ol>
<li>batch mean 과 variance 는 각 차원별로 계산해 준다.</li>
<li>Normalize </li>
</ol>
<script type="math/tex; mode=display">\hat{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}</script><p>앞서, batch normalization 의 목적은 network의 forward, backward pass 시 saturation 이 일어나지 않게 하기 위함이었습니다. 하지만 반면에, 이렇게 batch normalization 을 해준 이후에도 우리는 network 이 얼마나 해당 activation 을 얼마나 saturate 시킬지까지 학습 할 수 있다면 얼마나 좋을까요? 따라서 우리는 normalize 이후에 scaling factor 와 shifting factor 를 추가시켜, 얼마나 saturate 시킬지까지 학습할 수 있는 parameter 를 추가합니다.</p>
<script type="math/tex; mode=display">y^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{k}</script><p>Batch Noramlization 에 대한 pseudo 알고리즘이다.</p>
<p><img src="Untitled-83e0b3e7-d764-4a19-9c28-9fe815096b28.png" alt></p>
<p>또한, 우리는 이렇게 학습한 batch mean 과 variance  를 학습 때 사용하며, testing (inference) 시에는 다시 계산하지 않는 것을 유의해야한다. testing 시에는 training 시 running average 등의 방식으로 고정된 mean과 variance 등을 사용할 수 있다.</p>
<h2 id="2-Training-Dynamics"><a href="#2-Training-Dynamics" class="headerlink" title="2. Training Dynamics"></a>2. Training Dynamics</h2><h3 id="2-1-Babysitting-the-Learning-Process"><a href="#2-1-Babysitting-the-Learning-Process" class="headerlink" title="2-1. Babysitting the Learning Process"></a>2-1. Babysitting the Learning Process</h3><ol>
<li>Preprocess the data</li>
<li>Build Model</li>
<li>Sanity check for model (e.g. weigh이 작을ㄹ 때, loss가 negative log likelihood 와 비슷한지, regularization term이 추가될때 loss 가 증가하는지 등)</li>
<li>(regularization term 을 사용하지 않고) 매우 작은 데이터에 대해서, train  을 돌렸을 때, loss 가 떨어지고, 금방 overfitting 되는지 확인<br><br><br>여기까지가 sanity check 이라면, 이제 본격적인 training!!<br><br></li>
<li><p>간단한 몇가지 실험을 통해 learning rate 을 정한다. 큰 값, 작은 값을 넣어보고, epoch 을 10까지 정도로 주었을 때, loss 가 주는 지 확인하여 대략적인 범위를 정한다.</p>
</li>
<li><p>Coarse search: learning rate 과 다른 hyper parameter 를 uniform 등과 같은 distribution 을 통해 random search 한다. 강의에서 말했던, 주의사항 : 범위의 양 끝에 가장 좋은 score 가 뿌려져 있다면, 다시 범위를 설정해야한다. 내가 처음 설정한 범위 끝단에 존재한다면 그 주변에서 다시 최적의 값이 존재 할 수 있기 때문이다.</p>
</li>
<li><p>Finer search: 최적의 값을 찾기 위해 세세하게 튜닝한다.</p>
</li>
</ol>
<h3 id="2-2-Hyperparameter-Optimization"><a href="#2-2-Hyperparameter-Optimization" class="headerlink" title="2-2. Hyperparameter Optimization"></a>2-2. Hyperparameter Optimization</h3><ul>
<li>많은 양의 Cross Validation 을 통해 성능을 비교, 검증하며 최적의 hyper parameter 를 찾아야한다!</li>
<li>Grid Search &lt;&lt; Random Search<ul>
<li>Random Sampling 을 통해 좀더 중요한 parameter 의 분포를 찾아 낼 수 있어, Random search 가 효과적</li>
</ul>
</li>
</ul>
<h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><p><a href="https://www.youtube.com/watch?v=wEoyxE0GP2M" target="_blank" rel="noopener">Lecture 6 | Training Neural Networks I</a></p>
<p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/CS231n/">CS231n</a> <a class="tag tag--primary tag--small t-link" href="/tags/summary/">summary</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2019/07/22/CS231n-Lecture07-Summary/"
                    data-tooltip="[CS231n]Lecture07-Training Neural Networks part2"
                    aria-label="PREVIOUS: [CS231n]Lecture07-Training Neural Networks part2"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2019/07/15/iterator-generator/"
                    data-tooltip="[Python] 볼 때마다 헷갈리는 Iterable, Iterator, Generator 정리하기"
                    aria-label="NEXT: [Python] 볼 때마다 헷갈리는 Iterable, Iterator, Generator 정리하기"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#disqus_thread"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2021 EmjayAhn(Minjae Ahn). All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2019/07/22/CS231n-Lecture07-Summary/"
                    data-tooltip="[CS231n]Lecture07-Training Neural Networks part2"
                    aria-label="PREVIOUS: [CS231n]Lecture07-Training Neural Networks part2"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2019/07/15/iterator-generator/"
                    data-tooltip="[Python] 볼 때마다 헷갈리는 Iterable, Iterator, Generator 정리하기"
                    aria-label="NEXT: [Python] 볼 때마다 헷갈리는 Iterable, Iterator, Generator 정리하기"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#disqus_thread"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="5">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <h4 id="about-card-name">EmjayAhn(Minjae Ahn)</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>author.job</p>

            </div>
        
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover_blue.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/jquery.js"></script>
<script src="/assets/js/jquery.fancybox.js"></script>
<script src="/assets/js/thumbs.js"></script>
<script src="/assets/js/tranquilpeak.js"></script>
<!--SCRIPTS END-->


    
        <script>
          var disqus_config = function() {
            this.page.url = 'https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/';
              
            this.page.identifier = '2019/07/18/CS231n-Lecuture06-Summary/';
              
          };
          (function() {
            var d = document, s = d.createElement('script');
            var disqus_shortname = 'emjay-blog';
            s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
          })();
        </script>
    




    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
