<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Emjay, 오늘도 새롭게</title>
    <link>https://emjayahn.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>공부한 것, 알게된 것을 기록하는 블로그입니다.</description>
    <pubDate>Sun, 01 Sep 2019 05:46:58 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>[Metrics] PSNR &amp; SSIM</title>
      <link>https://emjayahn.github.io/2019/09/01/psnr-ssim/</link>
      <guid>https://emjayahn.github.io/2019/09/01/psnr-ssim/</guid>
      <pubDate>Sun, 01 Sep 2019 05:41:22 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;PSNR-amp-SSIM&quot;&gt;&lt;a href=&quot;#PSNR-amp-SSIM&quot; class=&quot;headerlink&quot; title=&quot;PSNR &amp;amp; SSIM&quot;&gt;&lt;/a&gt;PSNR &amp;amp; SSIM&lt;/h1&gt;&lt;p&gt;Image Reconstruction을 수행하는 중, Model Selection 을 위한 비교 metric 중 하나인 PSNR, SSIM 을 정리한 글입니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="PSNR-amp-SSIM"><a href="#PSNR-amp-SSIM" class="headerlink" title="PSNR &amp; SSIM"></a>PSNR &amp; SSIM</h1><p>Image Reconstruction을 수행하는 중, Model Selection 을 위한 비교 metric 중 하나인 PSNR, SSIM 을 정리한 글입니다.</p><a id="more"></a><h2 id="1-PSNR"><a href="#1-PSNR" class="headerlink" title="1. PSNR"></a>1. PSNR</h2><h3 id="1-1-정의-및-특징"><a href="#1-1-정의-및-특징" class="headerlink" title="1-1. 정의 및 특징"></a>1-1. 정의 및 특징</h3><p>Peak Signal-to-Noise Ratio</p><p> 영상 정보, 화질을 평가할 때 사용되는 Metric. 현재 나는 Image Reconstruction Task 를 수행하며, 다양한 모델의 비교를 위해 사용하기 위해 공부하는 내용이다.</p><p>품질이 좋은 이미지는 큰 PSNR 값을 가지며, 품질이 좋지 않은 이미지는 작은 PSNR 값을 가지게 된다. </p><ul><li>Peak Signal-to-Noise Ratio</li></ul><p>$$PSNR = 10log_{10}(\frac{MAX_I^2}{MSE})=20log_{10}(\frac{MAX_I}{\sqrt{MSE}}))=20log_{10}(MAX_I)-10log_{10}(MSE)$$</p><ul><li><p>MAXI 는 해당 영상의 최댓값, 해당 채널의 최댓값에서 최솟값을 빼서 구함</p><ul><li>e.g. 8-bit gray scale 의 경우, 255-0 = 255</li></ul></li><li><p>단위: db(log scale)</p></li><li><p>무손실 영상의 경우 MSE 가 0이기 때문에, PSNR 은 정의 되지 않는다.</p><p>  $$MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i, j)]^2$$</p></li><li><p>I : mxn 사이즈의 grayscale image</p></li><li><p>K: I 에 잡음이 포함된 이미지 (왜곡된 이미지)</p></li></ul><h3 id="1-2-한계"><a href="#1-2-한계" class="headerlink" title="1-2. 한계"></a>1-2. 한계</h3><p>PSNR은 intensity 의 값들을 위 식에 의해 종합하여 평가하는 방식이기 때문에, 실제로 사람이 봤을 때 느끼는 것과 다른 점수를 뱉어낼 때가 있다. 즉, <strong>사람의 지각품질을 제대로 반영하지 못하기 때문에</strong> 이를 보완하기 위해 PSNR-HVS, PSNR-HVS-M, SSIM, VIF 등의 Metric 이 개발되었다.</p><h2 id="2-SSIM"><a href="#2-SSIM" class="headerlink" title="2. SSIM"></a>2. SSIM</h2><h3 id="2-1-정의-및-특징"><a href="#2-1-정의-및-특징" class="headerlink" title="2-1. 정의 및 특징"></a>2-1. 정의 및 특징</h3><p>위 PSNR 의 한계를 극복하기 위해, 개발된 metric 으로써, Structural Similarity 의 줄임말이다. 사람의 지각 능력과 metric 을 일치시키는 목적에 개발되었다. 사람은 영상에서 구조 정보를 반영하여 영상을 바라보게 되는데, 영상이 얼마나 그 구조 정보를 변화시키지 않았는가를 살펴보는 metric 이다.</p><p>즉, 원본이미지(x)와 왜곡이미지(y)의 Luminance(l), Contrast(c), Structure(s)를 비교한다.</p><p>contrast: 이미지의 표준편차값</p><p>structure: (이미지-평균밝기) / 표준편차</p><p>$$l(x, y)=\frac{2\mu_x \mu_y+c_1}{\mu_x^2+\mu_y^2+c_1}$$</p><p>$$c(x,y)=\frac{2\sigma_x\sigma_y+c_2}{\sigma_x^2+\sigma_y^2+c_2}$$</p><p>$$s(x,y)=\frac{\sigma_{xy}+c_3}{\sigma_x\sigma_y+c_3}$$</p><p>$$SSIM(x,y)=[l(x,y)^{\alpha}c(x,y)^{\beta}s(x,y)^{\gamma}]$$</p><p>$$c_3=c_2/2$$</p><p>위에 c3 와 c2 의 조건을 추가하면 SSIM 은 다음과 같이 축약된다.<br><img src="Untitled-fee293f9-69f7-49d5-beba-5c035dc7ab87.png" alt></p><h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><ol><li><a href="https://kr.mathworks.com/help/images/ref/ssim.html" target="_blank" rel="noopener">https://kr.mathworks.com/help/images/ref/ssim.html</a></li><li><a href="https://bskyvision.com/" target="_blank" rel="noopener">https://bskyvision.com/</a></li><li><a href="https://en.wikipedia.org/wiki/Structural_similarity" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Structural_similarity</a></li><li><a href="https://ko.wikipedia.org/wiki/" target="_blank" rel="noopener">https://ko.wikipedia.org/wiki/</a></li></ol>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/09/01/psnr-ssim/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Book] Summary of Digital Image Processing Chapter 03</title>
      <link>https://emjayahn.github.io/2019/08/26/DIP-chapter3/</link>
      <guid>https://emjayahn.github.io/2019/08/26/DIP-chapter3/</guid>
      <pubDate>Mon, 26 Aug 2019 12:54:53 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Chapter-3-Intensity-Transformations-and-Spatial-Filtering&quot;&gt;&lt;a href=&quot;#Chapter-3-Intensity-Transformations-and-Spatial-Filtering&quot; class=&quot;headerlink&quot; title=&quot;Chapter 3: Intensity Transformations and Spatial Filtering&quot;&gt;&lt;/a&gt;Chapter 3: Intensity Transformations and Spatial Filtering&lt;/h1&gt;&lt;p&gt;Rafael C.Gonzalez and Richard E.Woods. &lt;i&gt;Digital Image Processing&lt;/i&gt;. PEARSON 을 다시 한번 읽어보며, 개인적으로 정리한 글입니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Chapter-3-Intensity-Transformations-and-Spatial-Filtering"><a href="#Chapter-3-Intensity-Transformations-and-Spatial-Filtering" class="headerlink" title="Chapter 3: Intensity Transformations and Spatial Filtering"></a>Chapter 3: Intensity Transformations and Spatial Filtering</h1><p>Rafael C.Gonzalez and Richard E.Woods. <i>Digital Image Processing</i>. PEARSON 을 다시 한번 읽어보며, 개인적으로 정리한 글입니다.</p><a id="more"></a><h2 id="3-1-Background"><a href="#3-1-Background" class="headerlink" title="3.1 Background"></a>3.1 Background</h2><h3 id="3-1-1-The-Basics-of-Intensity-Transformations-and-Spatial-Filtering"><a href="#3-1-1-The-Basics-of-Intensity-Transformations-and-Spatial-Filtering" class="headerlink" title="3.1.1 The Basics of Intensity Transformations and Spatial Filtering"></a>3.1.1 The Basics of Intensity Transformations and Spatial Filtering</h3><ul><li>모든 spatial domain process 에서 operator  와 input image f(x,y), output image g(x, y) 로 표현된다.</li></ul><p>$$g(x, y) = T[f(x, y)]$$</p><h2 id="3-2-Some-Basic-Intensity-Transformation-Functions"><a href="#3-2-Some-Basic-Intensity-Transformation-Functions" class="headerlink" title="3.2 Some Basic Intensity Transformation Functions"></a>3.2 Some Basic Intensity Transformation Functions</h2><h3 id="3-2-1-Image-Negatives"><a href="#3-2-1-Image-Negatives" class="headerlink" title="3.2.1 Image Negatives"></a>3.2.1 Image Negatives</h3><ul><li>Negative transformation</li><li>이미지 반전 효과</li></ul><p>$$s = L-1-r$$</p><h3 id="3-2-2-Log-Transormations"><a href="#3-2-2-Log-Transormations" class="headerlink" title="3.2.2 Log Transormations"></a>3.2.2 Log Transormations</h3><p>$$s=c*log(1+r)$$</p><ul><li>c : constant, assumed r ≥ 0</li><li>low intensity value of input → wider range of output levels</li></ul><h3 id="3-2-3-Power-Law-Gamma-Transformations"><a href="#3-2-3-Power-Law-Gamma-Transformations" class="headerlink" title="3.2.3 Power-Law(Gamma) Transformations"></a>3.2.3 Power-Law(Gamma) Transformations</h3><p>$$s = cr^\gamma=c(r+\epsilon)^\gamma$$</p><p>c, gamma: positive constants</p><ul><li>gamma 의 값에 따라서, intensity의 어떤 부분이 강조되는지가 다르다.</li><li>Original Image 와 monitor 에 비추는 이미지간의 차이를 중요하게 다룰 때, gamma correction 을 사용하게 된다.</li><li>일반적인 contrast를 다룰 때 중요하게 사용되기도 한다. MRI 사진 sample 예</li></ul><h3 id="3-2-4-Piecewise-Linear-Transformation-Functions"><a href="#3-2-4-Piecewise-Linear-Transformation-Functions" class="headerlink" title="3.2.4 Piecewise-Linear Transformation Functions"></a>3.2.4 Piecewise-Linear Transformation Functions</h3><h3 id="1-Contrast-stretching"><a href="#1-Contrast-stretching" class="headerlink" title="1. Contrast stretching"></a>1. Contrast stretching</h3><ul><li>Low-contrast image 은 sensing 면에서 떨어질 수 있으므로, intensity range를 연장하여, 큰 범위의 intensity 를 사용할 수 있도록 하는 방법</li><li>(input_intensity, output_intensity)→ (r1, s1) 과 (r2, s2)의 관계를 조절하여 contrast transform 형태를 조절한다. 극단적으로 r1=r2, s1=0, s2=L-1 이면, binary image 를 생성한다.(thresholding function)</li></ul><h3 id="2-Intensity-level-slicing"><a href="#2-Intensity-level-slicing" class="headerlink" title="2. Intensity-level slicing"></a>2. Intensity-level slicing</h3><ul><li>관심있는 영역의 Intensity-level 외에는 모두 0으로 처리하거나, 관심있는 영역은 특정 intensity level 로 두고, 나머지 level 은 그대로 두는 형태등이 있을 수 있다.</li></ul><h3 id="3-Bit-plane-slicing"><a href="#3-Bit-plane-slicing" class="headerlink" title="3. Bit-plane slicing"></a>3. Bit-plane slicing</h3><ul><li>8bit 의 이미지 슬라이드 중, significant order의 bit slide 중 특정 부분을 slicing</li></ul><h2 id="3-3-Histogram-Processing"><a href="#3-3-Histogram-Processing" class="headerlink" title="3.3 Histogram Processing"></a>3.3 Histogram Processing</h2><h3 id="3-3-1-Histogram-Equalization"><a href="#3-3-1-Histogram-Equalization" class="headerlink" title="3.3.1 Histogram Equalization"></a>3.3.1 Histogram Equalization</h3><ul><li>Assume monotonic transformation: one-to-one mapping or many-to-one mapping</li><li>output pdf ps, input pdf pr</li></ul><p>$$p_s(s) = p_r(r)|\frac{dr}{ds}|$$</p><ul><li>이 식은?</li></ul><p>$$s=T(r)=(L-1)\int_{0}^{r}{p_r(w)dw}$$</p><ul><li>histogram equalization, histogram linearization</li></ul><p>$$s_k=T(r_k)=(L-1)\sum_{j=0}^{k}p_r(r_j) = \frac{L-1}{MN}\sum_{j=0}^{k}{n_j}$$</p><ul><li>output image 의 p_s,  distribution 이 uniform 되게 하는 형태</li><li>결과적으로, 같은 이미지 형태이지만, 밝기와 contrast 가 모두 다르더라도 일정한 historgram 이 되도록 transform 해준다.</li></ul><h3 id="3-3-2-Histogram-Matching-Specifiaction"><a href="#3-3-2-Histogram-Matching-Specifiaction" class="headerlink" title="3.3.2 Histogram Matching(Specifiaction)"></a>3.3.2 Histogram Matching(Specifiaction)</h3><ul><li>Uniform historgram이 항상 좋은 것은 아니다.</li><li>histogram matching, histogram specification : The method used to generate a processed image that has specified histogram</li></ul><p>(1) histogram p_r(r) 을 계산, s_k 를 구하기 위해 histogram equalization transformation 식을 계산</p><p>(2) 원하는 p_z(z) 를 이용하여, function G 를 계산</p><p>(3)(2)에서 계산한 G를 이용해 z_k (k=0, 1, 2, …L-1) 까지 계산</p><p>(4) inverse of G 계산</p><p>(5)  r → z trasnformation 계산</p><h3 id="3-3-3-Local-Histogram-Processing"><a href="#3-3-3-Local-Histogram-Processing" class="headerlink" title="3.3.3 Local Histogram Processing"></a>3.3.3 Local Histogram Processing</h3><ul><li>이 전 두가지 histogram processing model 은 global  한 영역에서 진행, 즉 전체 이미지에 대해 intensity distribution 을 확인하고, 이를 이용해 transformation 을 진행 하였다.</li><li>이러한 modeling 스킬을 Local enhancement 에 사용가능</li><li>Local enhancement 는 neighborhood 를 정하고, 중심을 이동해가며, neighborhood 안에서의 histogram equalization 이나 histogram specification transformation 등을 이용할 수 있다.</li><li>계산을 줄이기 위해, nonoverlapping region 을 사용하여 위 방법을 동일하게 사용할 수 있으나, blocky effect 를 발생시킬 수 있음</li><li>blocky effect (p.161)</li></ul><h3 id="3-3-4-Using-Histogram-Statistics-for-Image-Enhancement"><a href="#3-3-4-Using-Histogram-Statistics-for-Image-Enhancement" class="headerlink" title="3.3.4 Using Histogram Statistics for Image Enhancement"></a>3.3.4 Using Histogram Statistics for Image Enhancement</h3><h2 id="3-4-Fundamentals-of-Spatial-Filtering"><a href="#3-4-Fundamentals-of-Spatial-Filtering" class="headerlink" title="3.4 Fundamentals of Spatial Filtering"></a>3.4 Fundamentals of Spatial Filtering</h2><h3 id="3-4-1-The-mechanics-of-Spatial-FIltering"><a href="#3-4-1-The-mechanics-of-Spatial-FIltering" class="headerlink" title="3.4.1 The mechanics of Spatial FIltering"></a>3.4.1 The mechanics of Spatial FIltering</h3><ol><li>Neighborhood (typically a small rectangle) 2. predefined operation</li></ol><h3 id="3-4-2-Spatial-Correlation-and-Convolution"><a href="#3-4-2-Spatial-Correlation-and-Convolution" class="headerlink" title="3.4.2 Spatial Correlation and Convolution"></a>3.4.2 Spatial Correlation and Convolution</h3><ul><li>correlation, convolution: 180 degree 반전</li></ul><h3 id="3-4-3-Vector-Representation-of-Linear-Filttering"><a href="#3-4-3-Vector-Representation-of-Linear-Filttering" class="headerlink" title="3.4.3 Vector Representation of Linear Filttering"></a>3.4.3 Vector Representation of Linear Filttering</h3><p>$$R=\sum_{k=1}^{9}w_kz_k=\boldsymbol{w}^T\boldsymbol{z}$$</p><h3 id="3-4-4-Generating-Spatial-Filter-Masks"><a href="#3-4-4-Generating-Spatial-Filter-Masks" class="headerlink" title="3.4.4 Generating Spatial Filter Masks"></a>3.4.4 Generating Spatial Filter Masks</h3><ul><li>mn mask</li><li>Average value of masked values → image smoothing</li><li>위치에 따른 gaussian filter 예 (standard deviation?(p.173))</li></ul><h2 id="3-5-Smoothing-Spatial-Filters"><a href="#3-5-Smoothing-Spatial-Filters" class="headerlink" title="3.5 Smoothing Spatial Filters"></a>3.5 Smoothing Spatial Filters</h2><ul><li>smoothing for blurring, noise reduction</li><li>blurring : removal of small details from an image prior to object extraction, bridging of small gaps in lines or curves</li><li>noise reduction: blurring by linear or non-linear filtering</li></ul><h3 id="3-5-1-Smoothing-Linear-Filters"><a href="#3-5-1-Smoothing-Linear-Filters" class="headerlink" title="3.5.1 Smoothing Linear Filters"></a>3.5.1 Smoothing Linear Filters</h3><ul><li>averaging filters == lowpass filters</li><li>Replace the value of every pixel in an image by the average of the intensity levels in neighborhood defined by the filter mask → reduced “sharp” transitions in intensity</li><li>box filter : all coefficient are euqal in filter</li></ul><h3 id="3-5-2-Order-Statistic-Nonlinear-Filters"><a href="#3-5-2-Order-Statistic-Nonlinear-Filters" class="headerlink" title="3.5.2 Order-Statistic(Nonlinear) Filters"></a>3.5.2 Order-Statistic(Nonlinear) Filters</h3><ul><li>median filter (popular): effective in impulse noise-reduction (salt-pepper noise)</li><li>implement<ol><li>sort values of neighborhood</li><li>determine their median</li><li>assign the value to filtered image</li></ol></li></ul><h2 id="3-6-Sharpening-Spatial-Filters"><a href="#3-6-Sharpening-Spatial-Filters" class="headerlink" title="3.6 Sharpening Spatial Filters"></a>3.6 Sharpening Spatial Filters</h2><ul><li>highlight transitions</li></ul><h3 id="3-6-1-Foundation"><a href="#3-6-1-Foundation" class="headerlink" title="3.6.1 Foundation"></a>3.6.1 Foundation</h3><h3 id="3-6-2-Using-the-Second-Derivative-for-Image-Sharpeing—The-Laplacian"><a href="#3-6-2-Using-the-Second-Derivative-for-Image-Sharpeing—The-Laplacian" class="headerlink" title="3.6.2 Using the Second Derivative for Image Sharpeing—The Laplacian"></a>3.6.2 Using the Second Derivative for Image Sharpeing—The Laplacian</h3><ul><li>capture intensity discontinuities in an image and deemphasize</li></ul><p>$$g(x, y)=f(x, y) + c[\triangledown^2f(x,y)]$$</p><h3 id="3-6-3-Unsharp-Masking-and-Highboost-Filtering"><a href="#3-6-3-Unsharp-Masking-and-Highboost-Filtering" class="headerlink" title="3.6.3 Unsharp Masking and Highboost Filtering"></a>3.6.3 Unsharp Masking and Highboost Filtering</h3><ul><li>unsharp masking<ol><li>Blur the original image</li><li>Subtract the blurred image from the original (the resulting difference is called the mask)</li><li>Add the mask to the original</li></ol></li></ul><p>$$g_{mask}(x,y)=f(x,y)-\bar{f}(x,y)$$</p><p>$$g(x,y) = f(x, y) + k*g_{mask}(x, y), \quad k&gt;=0$$</p><ul><li>k&gt;1 , highboost filtering</li><li>k&lt;1, de-emphasizes unsharp mask</li></ul><h3 id="3-6-4-Using-First-Order-Derivatives-for-Nonlinear-Image-Sharpening—The-gradient"><a href="#3-6-4-Using-First-Order-Derivatives-for-Nonlinear-Image-Sharpening—The-gradient" class="headerlink" title="3.6.4 Using First-Order Derivatives for (Nonlinear) Image Sharpening—The gradient"></a>3.6.4 Using First-Order Derivatives for (Nonlinear) Image Sharpening—The gradient</h3><ul><li>Using the magnitude of the gradient, magnitude M(x, y)</li></ul><p>$$M(x, y)=mag(\triangledown f)=\sqrt{g_x^2+g_y^2}=|g_x|+|g_y|$$</p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/08/26/DIP-chapter3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[논문읽기] Summary of Vision and Rain</title>
      <link>https://emjayahn.github.io/2019/08/20/summary-vision-and-rain/</link>
      <guid>https://emjayahn.github.io/2019/08/20/summary-vision-and-rain/</guid>
      <pubDate>Tue, 20 Aug 2019 12:25:00 GMT</pubDate>
      <description>
      
        &lt;h2 id=&quot;Presentation-for-summary-papers&quot;&gt;&lt;a href=&quot;#Presentation-for-summary-papers&quot; class=&quot;headerlink&quot; title=&quot;Presentation for summary papers&quot;&gt;&lt;/a&gt;Presentation for summary papers&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Initial Model for removing rain and snow from Image system.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="Presentation-for-summary-papers"><a href="#Presentation-for-summary-papers" class="headerlink" title="Presentation for summary papers"></a>Presentation for summary papers</h2><ul><li>Initial Model for removing rain and snow from Image system.</li></ul><a id="more"></a><iframe src="//www.slideshare.net/slideshow/embed_code/key/w9AhGosEGYe6Dc" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ssuser47e145/summary-of-the-paper-detection-and-removal-of-rain-from-video-vision-and-rain" title="Summary of the paper &#x27;detection and removal of rain from video, vision and rain&#x27;" target="_blank">Summary of the paper &#x27;detection and removal of rain from video, vision and rain&#x27;</a> </strong> from <strong><a href="https://www.slideshare.net/ssuser47e145" target="_blank">ssuser47e145</a></strong> </div>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/08/20/summary-vision-and-rain/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[논문읽기] Implementation of Vanilla GAN</title>
      <link>https://emjayahn.github.io/2019/08/13/Vanilla-GAN/</link>
      <guid>https://emjayahn.github.io/2019/08/13/Vanilla-GAN/</guid>
      <pubDate>Tue, 13 Aug 2019 07:00:13 GMT</pubDate>
      <description>
      
        &lt;p&gt;A pytorch implementation of Vanilla GAN using MNIST digits data&lt;br&gt;(&lt;a href=&quot;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;실험 결과와 코드는 &lt;a href=&quot;https://github.com/EmjayAhn/GAN-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/EmjayAhn/GAN-pytorch&lt;/a&gt; 에서 확인 할 수 있습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>A pytorch implementation of Vanilla GAN using MNIST digits data<br>(<a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a>)</p><p>실험 결과와 코드는 <a href="https://github.com/EmjayAhn/GAN-pytorch" target="_blank" rel="noopener">https://github.com/EmjayAhn/GAN-pytorch</a> 에서 확인 할 수 있습니다.</p><a id="more"></a><h2 id="0-목표"><a href="#0-목표" class="headerlink" title="0. 목표"></a>0. 목표</h2><p>GAN 은 최근 인기를 끌고 있는 generative model 중 하나입니다. 다양한 모델이 쏟아져 나오고 있기에, 이 트렌트를 따라가기 위해서, 가장 기본적인 vanilla gan 을 구현하고, 이해하는 것이 목표입니다. 이번 예제에서는 MNIST digit 을 사용하였으나, 이미지 외에도 다양한 데이터를 활용할 수 있습니다.</p><h2 id="1-Model-구조"><a href="#1-Model-구조" class="headerlink" title="1. Model 구조"></a>1. Model 구조</h2><p>GAN 의 아버지 Ian Goodfellow가 제안한 이 모델은 두가지 신경망으로 구성 되어 있습니다. 먼저, 우리가 가지고 있는 데이터와 비슷하게 데이터를 생성하는 것을 학습하는 <strong>Generator</strong> 와 Generator 가 생성한 데이터(fake)와 실제 우리가 가지고 있는 데이터(real)를 fake 인지 real 인지 구분하는 <strong>Discriminator</strong> 를 구분하는 두 신경망으로 구성되어 있습니다. </p><p><img src="Untitled-b4ab047c-a850-427f-8e06-379dce74e281.png" alt></p><h3 id="1-1-Generator"><a href="#1-1-Generator" class="headerlink" title="1-1. Generator"></a>1-1. Generator</h3><ul><li>Generator 는 Gaussian Random Noise (mean=0, std=1) 를 입력으로 받아, 이 noise로 부터 data 를 생성해냅니다.</li><li>이번 구현에서 Generator는 다음과 같이 작성하였습니다.</li></ul><p><img src="Untitled-5a191f6c-b437-4eab-89a7-b34692124ac3.png" alt></p><ul><li>Dense layer 만 사용했으며, gradient vanishing 현상을 막기 위해 activation 을 거친 후, Batch Normalization 을 추가하였습니다.</li></ul><h3 id="1-2-Discriminator"><a href="#1-2-Discriminator" class="headerlink" title="1-2. Discriminator"></a>1-2. Discriminator</h3><ul><li>Discriminator 는 Generator 가 생성해낸 데이터와 기존에 가지고 있는 진짜 데이터를 입력으로 받아, 진짜 데이터를 1, 가짜데이터를 0으로 학습하는 classifier 입니다.</li><li>아래의 Loss Function 을 확인 하겠지만, 진짜 데이터에 대해서는 그 확률 값을 높게 하고, 가짜데이터에서는 그 확률 값을 0에 가깝게 하는 것이 이 모델의 optimize 목표입니다.</li><li>이번 구현에서 Discriminator를 다음과 같이 작성하였습니다.</li></ul><p><img src="Untitled-4de92b80-7e9d-452e-9956-0298c2b493e2.png" alt></p><h2 id="2-Loss-Function"><a href="#2-Loss-Function" class="headerlink" title="2. Loss Function"></a>2. Loss Function</h2><h3 id="2-1-Loss-Function-의-해석"><a href="#2-1-Loss-Function-의-해석" class="headerlink" title="2-1. Loss Function 의 해석"></a>2-1. Loss Function 의 해석</h3><p>$$\underset{G}{\text{min}}\underset{D}{\text{max}}V(D, G)=E_{x<del>p_{data(x)}}[logD(x)]+E_{z</del>p_{z}(z)}[log(1-D(G(z)))]$$</p><p>Vanilla GAN 의 Loss function 은 위와 같습니다. Loss Function 의 구조 자체는 min-max 최적화로써, Discriminator와 Generator 의 loss 함수를 각각 최적화 해 나아가면서 위 식의 균형 향해 다가가는 것입니다.</p><ul><li>먼저, Discriminator에 대한 max 부터 살펴 보면, Real을 입력으로 넣었을 때는, log(D(x))의 기댓값이 최대가 되게 하고, G(z) 즉, 가짜를 가짜라고 할 확률 1-D(G(z))는 최대가 되게끔 학습을 하는 것입니다.</li><li>Generator 에 대한 min 을 살펴보면, G(z) 는 가우시안 랜덤 변수를 받아 생성된 데이터를 D(G(z)), discriminator에 넣었을 때, 1-D(G(z)), 즉 가짜라고 할 확률을 최소화하게끔 학습하는 것입니다. 이는 결국, Discriminator를 속이기 위해 generator의 최적화가 실행된다는 의미입니다.</li></ul><p>이 식에서 중요한 점은, Discriminator 는 학습할 때, Generator 가 생성한 데이터와 진짜 데이터 모두를 보며 학습하지만, Generator 는 그 어디에서도 진짜 데이터가 어떻게 생겼는지는 확인하지 않습니다. 오로지 Discriminator 를 속이기 위해 학습하는 것이지만, 그 결과 우리가 가지고 있는 진짜 데이터와 비슷하게 만들수 있는 모델을 획득하게 될 수 있는 것이라는 점에서 매우 획기적인 모델입니다.</p><h3 id="2-2-실제-구현에서의-변형"><a href="#2-2-실제-구현에서의-변형" class="headerlink" title="2-2. 실제 구현에서의 변형"></a>2-2. 실제 구현에서의 변형</h3><p>Generator 에 대한 loss function 은 log(1-(D(G(z)))를 최소화 하는 것입니다. 하지만, log(1-x) 형태의 식은 x가 0일 때, 그 gradient 가 매우 작아, 학습이 매우 오래 걸리는 문제가 있습니다. 이를 해결하기 위해, log(1-D(G(z)))를 G에 대해 최소화 하는 것은 결국, -log(D(G(z)))를 최소화 하는 것과 같고, 이는 log(D(G(z)))를 최대화 하는 것과 같습니다. 따라서 실제 구현에서의 criterion 은 Discriminator 가 사용하는 criterion(여기선, binary cross enntropy loss)을 동일하게 사용합니다.</p><h2 id="3-학습-및-모델-결과"><a href="#3-학습-및-모델-결과" class="headerlink" title="3. 학습 및 모델 결과"></a>3. 학습 및 모델 결과</h2><p>학습 parameter 는 다음과 같습니다.</p><ul><li>Total epoch: 300</li><li>batch size : 128</li><li>z dimension : 100</li><li>Adam optimizer : lr=0.0002, weight_decay=8e-9</li></ul><p>다음은 generator가 학습이 되가면서, 같은 가우시안 랜덤 노이즈에 대해 mnist 와 닮은 데이터를 생성해 나가는 과정입니다.</p><p>(1) 0 epoch</p><p>약 400개의 배치를 학습한 후, 찍은 사진이기에 가운데에 아주 미세한 형태는 보이지만, 가우시안 노이즈임을 확인 할 수 있습니다.</p><p><img src="Untitled-adec04da-b417-40c9-b511-43d380ca6d78.png" alt></p><p>(2) 20 epoch</p><p>20 epoch 만 되더라도, <del>(마치 뱃속의 아가처럼(?))</del> 가운데에 어떤 형태가 생성되기 시작하는 것을 확인 할 수 있습니다. </p><p><img src="Untitled-0322c576-8ea5-4e73-8268-e2d82fdb4708.png" alt></p><p>(3) 100 epoch</p><p>9, 3, 8, (horizontal flipped) 3, 1.. 아주 힘들게 MNIST 와 비슷해 보이는 숫자를 확인 할 수 있습니다.</p><p><img src="Untitled-fcb15889-a098-48e5-8cd9-29d26dd0fbf7.png" alt></p><h2 id="4-결론"><a href="#4-결론" class="headerlink" title="4. 결론"></a>4. 결론</h2><p>이번 구현의 목표는 나의 첫 vanilla gan 을 논문과 여러 자료를 공부해보며, 구현해 보는 것에 있었기에, 이를 완수하고, 실제로 학습 시켰을 때, generator 로써 기능을 할 수 있다는 점에서 유의미 하였습니다. 다양한 repository 에서 서로 다른 framework 를 사용하여, gan 을 구현하는 것을 참조 할 수 있습니다. 하지만, 직접 공부해보고, loss function의 의미를 해석하여 직접 구현해보며 많은 것을 배울 수 있었습니다. 실제로 loss function 위 처럼 바꾸지 않았을 때는 1000 epoch 를 학습하더라도 generator가 학습 되지 않는 실패 경험을 통해, 자세한 논문 리딩과 분석은 구현에 있어 필수적임을 느낄 수 있었습니다.</p><h2 id="5-Futher-Study"><a href="#5-Futher-Study" class="headerlink" title="5.  Futher Study"></a>5.  Futher Study</h2><p>자원의 제약으로 작은 모델 구조와 hyper parameter tuning을 더 하지 못한게 아쉽습니다. generator 가 생성해내는 모양이 조금더 세밀하게 할 수 있는 것을 더 해보고 싶고, MNIST 데이터 뿐만아니라 본 논문의 참조사진처럼 CIFAR10 이나, TFD 데이터에 대해서도 실험해보고 싶습니다.</p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/08/13/Vanilla-GAN/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS224n]Lecture04-BackPropagation</title>
      <link>https://emjayahn.github.io/2019/08/09/CS224n-Lecture04-Summary/</link>
      <guid>https://emjayahn.github.io/2019/08/09/CS224n-Lecture04-Summary/</guid>
      <pubDate>Fri, 09 Aug 2019 13:26:12 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-04-Backpropagation-and-computation-graphs&quot;&gt;&lt;a href=&quot;#Lecture-04-Backpropagation-and-computation-graphs&quot; class=&quot;headerlink&quot; title=&quot;Lecture 04: Backpropagation and computation graphs&quot;&gt;&lt;/a&gt;Lecture 04: Backpropagation and computation graphs&lt;/h1&gt;&lt;p&gt;Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-04-Backpropagation-and-computation-graphs"><a href="#Lecture-04-Backpropagation-and-computation-graphs" class="headerlink" title="Lecture 04: Backpropagation and computation graphs"></a>Lecture 04: Backpropagation and computation graphs</h1><p>Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.</p><a id="more"></a><p>기본적인 computation graph 와 backpropagation 에 관한 내용은 스킵하도록 하겠습니다. 새롭고, 핵심적인 내용만 간추린 내용입니다.</p><h2 id="1-Word-Vector를-Retraining"><a href="#1-Word-Vector를-Retraining" class="headerlink" title="1. Word Vector를 Retraining?"></a>1. Word Vector를 Retraining?</h2><p>Quetion: Retraining 에 대한 판단의 시작 예: TV, telly, television 이 pre-trained word vector 에서는 비슷한  공간에 분포되어있다고 가정할 때, training data 에는 TV 와 telly 단어만 존재하고, test data 에 television이 존재 할 때, word vector 는 어떻게 될 것인가?</p><ul><li>Answer: Training data 에 있는 TV 와 telly 에 해당하는 word vector 는 back prop을 진행하면서, 미세하게 업데이트 되며, 같은 방향으로 이동하게 된다. 반면, television에 해당하는 word vector는 weight parameter 업데이트가 일어나지 않으므로, 처음에는 비슷한 공간에 분포 했으나, TV 와 telly 와 멀어지게 된다.</li></ul><h3 id="1-1-Word-Vector의-Retraining-여부"><a href="#1-1-Word-Vector의-Retraining-여부" class="headerlink" title="1-1. Word Vector의 Retraining 여부"></a>1-1. Word Vector의 Retraining 여부</h3><ul><li>Word Vector가 학습 할 때는 매우 큰 데이터셋을 가지고 학습하게 된다. 따라서 매우 다양한 단어들이 Corpus 로 존재한다.</li><li>Fine tuning?: 우리가 가지고 있는 training dataset 이 매우 작다면, 위에서 든 예에서 직감할 수 있듯이, pre trained vector 를 fine tuning 하게 되면, training set 에 fitting 되는 효과가 있고, 우리가 의도치 않는 weight 의 업데이트가 되거나 혹은 되지 않을 수 있다.</li></ul><h2 id="2-효율적인-gradient-계산"><a href="#2-효율적인-gradient-계산" class="headerlink" title="2. 효율적인 gradient 계산"></a>2. 효율적인 gradient 계산</h2><ul><li>사실 당연하게 여김에도, 우리가 손으로 계산하는 (upstream network * local gradient) 과정을 아래 식에서도 확인 할 수 있듯이, ds/dh, dh/dz term 은 중복되는 과정이다.</li></ul><p>$$\frac{ds}{dW}=\frac{ds}{dh}\frac{dh}{dz}\frac{dz}{dW}$$</p><p>$$\frac{ds}{db}=\frac{ds}{dh}\frac{dh}{dz}\frac{dz}{db}$$</p><ul><li>따라서 효율적인 computation을 구현하기 위해서, back propagation에서 upstream network 를 저장하고, 각 parameter 에 대한 local gradient를 구해, 동시에 곱해주어 back prop 을 진행 할 수 있다.</li></ul><h2 id="3-Regularization"><a href="#3-Regularization" class="headerlink" title="3. Regularization"></a>3. Regularization</h2><p>우리가 parameter 가 많아지면 많아질 수록, training error 와  test error 는 낮아지기 마련이다. 하지만, 어느 수준을 넘어가게 되면, training set 에 대해서는 매우 정확해지는 반면, test data(validate data)에 대해서는 generalization에서 실패한 그래프나 수치들을 확인할 수 있다. 따라서, 우리는 반드시 우리가 최적화 하려고 하는 Loss 에 대해 Regularization 을 해주어야만 한다.</p><p><img src="Untitled-0b756cac-8f09-434f-8310-8014237e4df0.png" alt></p><p>다음은 L2 Regularization term 이 추가된 loss function 이다. 지겹도록 바왔음에도, 꼭 식을 보면 해석해야겠기에, Model parameter (weight) theta 가 제곱term 으로 너무 커지는 것을 방지하기 위해 lambda 에 비례하여 penalty term 을 추가한다.</p><p><img src="Untitled-8dbb0e2b-833b-4f99-ac77-ff0ad096d113.png" alt></p><h2 id="4-Vectorization"><a href="#4-Vectorization" class="headerlink" title="4. Vectorization"></a>4. Vectorization</h2><p>우리가 forward/backward propagation 을 진행하면서, 각 data의 계산을 looping 하여 계산한다면, 매우 비효율적인 계산 방식이 된다. 우리는 위대한 Vecor/Matrix Multiplication 방법으로, 즉, 모든 data 와 weight을 행렬로 만들어 forward/backward 계산을 진행해야한다. 또한 이렇게 진행했을 때, 가속화 도구인 GPU 활용의 이점을 사용할 수 있다.</p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/08/09/CS224n-Lecture04-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Linux] 자고있을 때도, 알아서.. 리눅스 Crontab</title>
      <link>https://emjayahn.github.io/2019/08/06/linux-crontab/</link>
      <guid>https://emjayahn.github.io/2019/08/06/linux-crontab/</guid>
      <pubDate>Tue, 06 Aug 2019 12:15:20 GMT</pubDate>
      <description>
      
        &lt;p&gt;데이터를 모으기 위해 크롤링을 진행하거나, 머신러닝, 딥러닝 실험을 할 때 Linux 환경의 머신에서 정해진 시간과 주기에 맞추어 크롤링을 실행하고, 학습을 해준다면, 수많은 작업들을 미리 설정해둔 내용을 바탕으로 편하게 작업을 자동화 할 수 있습니다. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>데이터를 모으기 위해 크롤링을 진행하거나, 머신러닝, 딥러닝 실험을 할 때 Linux 환경의 머신에서 정해진 시간과 주기에 맞추어 크롤링을 실행하고, 학습을 해준다면, 수많은 작업들을 미리 설정해둔 내용을 바탕으로 편하게 작업을 자동화 할 수 있습니다. </p><a id="more"></a><h2 id="1-Crontab-스케줄-작성-삭제-목록-확인"><a href="#1-Crontab-스케줄-작성-삭제-목록-확인" class="headerlink" title="1. Crontab 스케줄 작성, 삭제, 목록 확인"></a>1. Crontab 스케줄 작성, 삭제, 목록 확인</h2><p>1-1. Crontab 스케줄 작성하기</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ crontab -e</span><br></pre></td></tr></table></figure><p>-e (edit) 옵션으로 Crontab 의 스케쥴을 설정해 줄 수 있습니다. 작성할 스케쥴은 리눅스의 디폴트 에디터인 vi 에디터를 이용합니다.</p><p>1-2. Crontab 스케줄 지우기</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ crontab -r</span><br></pre></td></tr></table></figure><p>-r (remove) 옵션으로 Crontab 에 등록된 스케줄을 삭제해 줄 수 있습니다.</p><p>1-3. Crontab 스케줄 목록 확인하기</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ crontab -l</span><br></pre></td></tr></table></figure><p>-l (list) 옵션으로 Crontab 에 등록된 스케줄 리스트를 확인 할 수 있습니다.</p><h2 id="2-Crontab-주기"><a href="#2-Crontab-주기" class="headerlink" title="2. Crontab 주기"></a>2. Crontab 주기</h2><p>항상 사용할 때마다, 헷갈리고 잊어버리는 설정 주기 순서입니다.</p><p>마지막의 <code>요일</code> 부분은 0, 7: 일요일, 1: 월요일 ~ 6: 토요일</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*   *    *   *   *</span><br><span class="line">분  시간  일   월  요일</span><br></pre></td></tr></table></figure><br>예를 들어, 매주 월요일에 crawling.py 를 실행한다면, 아래와 같이 crontab edit 창에서 작성하고 저장하면 됩니다.<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * 5 python3 /directory/crawling.py</span><br></pre></td></tr></table></figure><p>이제부터는 조금 복잡한 주기를 설정할 수도 있습니다.<br>2-1. 반복</p><ul><li><p>매시 25분, 45분에 실행하고 싶을 때</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">25,45 * * * * python3 /directory/crawling.py</span><br></pre></td></tr></table></figure></li><li><p>매 20분마다 실행하고 싶을 때</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/20 * * * * python3 /directory/crawling.py</span><br></pre></td></tr></table></figure></li></ul><p>2-2. 범위</p><ul><li>매주 수요일에서 금요일까지 1시 30분마다 실행 시킬 때<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">30 1 * * 3-5 python3 /directory/crawling.py</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-예제"><a href="#3-예제" class="headerlink" title="3. 예제"></a>3. 예제</h2><p>2분마다 “THIS IS CRONTAAAB”을 test.txt 에 기록해보자.</p><ul><li><p>“THIS IS CRONTAAAB”을 test.txt 파일에 기록하는 shell command 를 작성합니다.</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># program.sh파일에 다음과 같이 작성합니다.</span><br><span class="line">echo &quot;THIS IS CRONTAAAB&quot; &gt;&gt; ./test.txt&quot;</span><br></pre></td></tr></table></figure></li><li><p><code>crontab -l</code> 명령어를 통해 다음과 같이 작성합니다</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/2 * * * * /directory/program.sh</span><br></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/08/06/linux-crontab/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 쉽게 쓰여진 Decorator</title>
      <link>https://emjayahn.github.io/2019/07/27/decorator/</link>
      <guid>https://emjayahn.github.io/2019/07/27/decorator/</guid>
      <pubDate>Sat, 27 Jul 2019 09:46:17 GMT</pubDate>
      <description>
      
        &lt;p&gt;오픈소스나, 다른 사람들이 만든 코드를 재수정한 코드, 제가 짠 코드에 대해 다양한 디버깅과 좀 더 다른 기능을 추가하고 싶을 때, 우리는 Decorator 를 자주 접하게 됩니다. Python 실력을 한층 업그레이드 하기 위함과, 코딩의 또 다른 목적이이 &lt;code&gt;귀차니즘의 해결&lt;/code&gt; 이라고 생각할 때 Decorator 에 대한 이해는 그 시작 관문이 됩니다. 이번 글에서는 이 Decorator 에 대한 개념을 쉽게 설명하려고 노력하였습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>오픈소스나, 다른 사람들이 만든 코드를 재수정한 코드, 제가 짠 코드에 대해 다양한 디버깅과 좀 더 다른 기능을 추가하고 싶을 때, 우리는 Decorator 를 자주 접하게 됩니다. Python 실력을 한층 업그레이드 하기 위함과, 코딩의 또 다른 목적이이 <code>귀차니즘의 해결</code> 이라고 생각할 때 Decorator 에 대한 이해는 그 시작 관문이 됩니다. 이번 글에서는 이 Decorator 에 대한 개념을 쉽게 설명하려고 노력하였습니다.</p><a id="more"></a><p>많은 책들에서 Decorator 의 설명을 본격적으로 들어가기에 앞서, python에서의 변수의 범위와 전역변수, 지역변수, 자유변수 등을 설명하고, 자칫 정신을 혼미하게 만들 수 있는 클로져(closure)를 설명한 뒤에 Decorator 를 만나게 됩니다. 물론 모두 Decorator 에서만아니라 파이썬을 다루고, 컴퓨터 과학을 공부하며, 필수적으로 알아야하는 중요한 개념이긴 하나, 이번 글에서는 예제들을 통해 Decorator 를 짜는 방법과 어떻게 구동이 되는지 실용적인 개념에 대해 요약 정리하려고 합니다.</p><h2 id="1-Decorator란"><a href="#1-Decorator란" class="headerlink" title="1. Decorator란?"></a>1. Decorator란?</h2><blockquote><p>Decorator : <strong>호출 가능한</strong> <strong>객체</strong> 로써, <strong>호출 가능한 객체</strong>를 입력으로 받아, <strong>호출 가능한 객체</strong> 를 반환하는 함수</p></blockquote><p>Decorator 를 만들 때, 가장 첫번째로 성립해야하는 구조가 위처럼, Decorator 기능을 하게 될 함수가 호출 가능해야하고, 입력에 호출 가능한 객체를 받아, 반환하는 객체도 호출 가능한 객체로 반환 해주어야 합니다.<br><br><br>위의 정의를 만족하는 세상에서 가장 간단한 Decorator 를 다음의 예제코드로 만들 수 있습니다.</p><script src="https://gist.github.com/EmjayAhn/f0cd4bafba347e3d046b5a742ffc57db.js"></script><p>위의 간단한 코드의 동작 순서를 살펴봅니다.</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(3) 에서 say_hi 라는 함수 객체가 dumb_decorator 함수의 입력으로 들어가, dumb_decorator() 함수를 호출합니다.</span><br><span class="line">(1) 에서 dumb_decorator는 입력받은 함수 객체(say_hi) 자체를 반환합니다.</span><br><span class="line">dumb_decorator 가 반환한 함수객체를 (3)의 decorated 에 저장합니다. </span><br><span class="line">3.에서 decorated는 함수 객체 자체입니다. (4) 에서 decorated() 로, 함수 객체를 실행한 결과를 text 에 저장합니다. </span><br><span class="line">이 코드에서 결국 decorated 함수 객체가 실행되면, say_hi 함수가 실행되고, say_hi함수의 반환값인 &quot;Hi, Hi&quot; 가 text 에 저장되어,</span><br><span class="line">(5)에서 출력됩니다.</span><br></pre></td></tr></table></figure><p>위의 코드의 결과는 <code>Hi, Hi</code> 가 출력되는 아무 기능이 없는 기본 함수(say_hi)와 Decorator 입니다. 위와 같은 결과이지만, Decorator 문법인 <code>@</code>를 사용한 코드는 다음과 같습니다.</p><script src="https://gist.github.com/EmjayAhn/6fbcfcfa64bd107f0e7875867c28aa37.js"></script><p>위의 두 예제 코드는 같은 결과를 내지만, 앞서 정의된 <code>dumb_decorator</code> 가 첫번째 예제에서는 꾸미려고하는 <code>say_hi</code> 함수를 <code>dumb_decorator</code>의 입력으로 넣어주어 코드를 실행 시켜 주어야 했습니다. 두번째 예제에서는 꾸미려고하는 <code>say_hi</code> 함수의 선언시 <code>@dumb_decorator</code> 를 먼저 적어주고, 꾸며진 함수 <code>say_hi()</code> 를 실행하는 점에서 차이가 있습니다.<br><br><br>앞서 처음에 등장한 구조적 정의를 위 예제에 대입하여 살펴봅니다.</p><blockquote><p>Decorator : <strong>호출 가능한</strong> <strong>객체</strong> 로써, <strong>호출 가능한 객체</strong>를 입력으로 받아, <strong>호출 가능한 객체</strong> 를 반환하는 함수</p></blockquote><p>호출 가능한 객체로써 (dumb_decorator), 호출 가능한 객체(say_hi)를 입력으로 받아, 호출 가능한 객체((1)에 return func)를 반환하는 함수 일 때, @decorator로 동작 할 수 있습니다.</p><h2 id="2-Decorator를-Decorator-처럼"><a href="#2-Decorator를-Decorator-처럼" class="headerlink" title="2. Decorator를 Decorator 처럼"></a>2. Decorator를 Decorator 처럼</h2><p>1에서 Decorator의 핵심적인 구조를 살펴보았습니다. 이제 본격적으로 Decorator를 사용되기 위해선, decorator 내부에 입력함수의 기능을 꾸며주는 wrapper 함수가 필요합니다. 예제로 살펴보겠습니다. </p><script src="https://gist.github.com/EmjayAhn/bc8e34c19ebb87446aec5c7645c80f55.js"></script><p>첫번째 예제와 다른 점이라곤, 우리가 꾸미려고 하는 decorator 함수 안에 내부 wrapper 함수를 추가해줌으로써, 우리가 꾸미려는 내용을 선언해주었습니다. </p><h2 id="3-Decorator-의-장점"><a href="#3-Decorator-의-장점" class="headerlink" title="3. Decorator 의 장점?"></a>3. Decorator 의 장점?</h2><p>위에 본 예제를 극단적인 case 로 몰고 가보죠. 우리가 꾸며야할 함수가 많다고 상상해 보겠습니다. 기존에 잘 동작하던 프로그램이 있을 때, 프로그램의 로그가 궁금하다던가, 신경망을 학습시키는 코드에 대해 각 layer 의 gradient 나 출력값을 보고 싶을 수 있습니다. 우리가 decorator 를 사용하지 않는다면, 첫번째 코드 예시 처럼 일일히 다 실행시켜주어야 하는 문제가 발생합니다. 우리는 사용하려고 하는 함수를 선언할때 <code>@decorator</code> 를 붙여 줌으로써, 이러한 <code>귀차니즘</code>을 해결할 수 있습니다. 예제를 통해 살펴보겠습니다.<br><br><br>기존에도 잘 돌아가는 프로그램 3개의 log 를 찍어야 하는 순간이 찾아왔다고 가정합니다. generator 를 사용하는 것과 그렇지 않은 것을 비교해봄으로써 generator 의 고마움을 느껴볼 수 있습니다. 이번 예제에서는 간단하게 그 log 를 프로그램 return 형의 글자(character) 수로 생각해보죠.</p><script src="https://gist.github.com/EmjayAhn/c004ae8798e378a07a94c3b0d33e4814.js"></script><p>기존에 존재하던 프로그램이 <code>program_1</code>, <code>program_2</code>, <code>program_3</code> 이라고 생각해보고, 추가적으로 우리가 my_decorator 라는 코드를 통해 각 프로그램의 로그(여기선, charater 수)를 찍어야 하는 task 가 주어졌습니다. 우리가 decorator 를 알기 전이라면, 위와 같이 코드를 작성해 준 후,<br>실행부분에서 각 프로그램을 decorator 에 넣어주어, 꾸며진 결과 객체를 가지고, 이를 다시 실행해주는 행동을 반복해 주어야 합니다. 이제 decorator 의 고마움을 느껴볼 차례입니다.</p><script src="https://gist.github.com/EmjayAhn/1088964a6c8d53a668b45fe6a6f1e9c7.js"></script><h2 id="4-마치며"><a href="#4-마치며" class="headerlink" title="4. 마치며"></a>4. 마치며</h2><p>간단한 예제를 통해 Decorator 가 어떻게 동작하는지, 어떻게 구성해야하는지 핵심적인 부분이 이해됐길 바랍니다. 조금더 공부할 수 있는 키워드는 <code>클로져 (closure)</code>, <code>free variable</code> 등이 있을 수 있습니다. 위 키워드의 공부를 통해 조금더 자유로운 generator 설계를 할 수 있을 것으로 믿습니다.</p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/07/27/decorator/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture09-CNN Architectures</title>
      <link>https://emjayahn.github.io/2019/07/26/CS231n-Lecture09-Summary/</link>
      <guid>https://emjayahn.github.io/2019/07/26/CS231n-Lecture09-Summary/</guid>
      <pubDate>Fri, 26 Jul 2019 11:40:18 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-09-CNN-Architectures&quot;&gt;&lt;a href=&quot;#Lecture-09-CNN-Architectures&quot; class=&quot;headerlink&quot; title=&quot;Lecture 09: CNN Architectures&quot;&gt;&lt;/a&gt;Lecture 09: CNN Architectures&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-09-CNN-Architectures"><a href="#Lecture-09-CNN-Architectures" class="headerlink" title="Lecture 09: CNN Architectures"></a>Lecture 09: CNN Architectures</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><p>이번 강의에서는 최초의 CNN 모델부터, ImageNet 대회에서 역대 좋은 스코어를 기록한 유명한 convnet 구조에 대해 살펴본다. 강의 summary 는 강의 내용 중심과 수업 중 궁금한 사항을 따로 찾아본 내용 위주로 정리한다. 해당 모델의 자세한 내용은 논문을 참조했다.</p><h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><p>Lenet 이후, 가장 처음으로 나온 large scale convnet model 이다. Architecture 는 다음과 같다. 수업시간에는 CONV1 과  Max Pooling layer 에 대해서만 각 layer 의 output volume size 와 parameter 갯수를 확인했으나, 연습과 공부를 위해 전체 layer 에 대해 계산해본다.</p><ul><li><p>Architecture</p><p>  CONV1</p><p>  MAX POOL1</p><p>  NORM1</p><p>  CONV2</p><p>  MAX POOL2</p><p>  NORM 2</p><p>  CONV3</p><p>  CONV4</p><p>  CONV5</p><p>  MAX POOL3</p><p>  FC6</p><p>  FC7</p><p>  FC8</p></li></ul><ol><li>Input 이 227 x 227 x 3 image 가 들어갈 때, CONV1 (96 11x11 filter, stride 4)의 output volume size ? <ul><li>(227 - 11) / 4 + 1 = 55 이므로, 55 x 55 x 96</li></ul></li><li>Conv1 의 weight 갯수 ?<ul><li>filter size 11 * 11 * 3 (channel) * 96 (filter 갯수) = 34,849개</li></ul></li><li>MAX POOL (3 x 3 filter, stride 2) output volume size?<ul><li>(55 - 3) / 2 + 1 = 27</li><li>27 x 27 x 96</li></ul></li><li>MAX POOL 의 weight 갯수?<ul><li>Pooling 은 weight 이 없으니까, 낚이지말자. (낚일 수 없는 낚시지..)</li></ul></li><li>NORM Layer 는?<ul><li>Normalization 만 해주는 것이므로 output size 는 27 x 27 x 96 으로 동일</li></ul></li><li>CONV2 layer (256 5 x 5 filters, stride 1, padding 2) 의 output volume size 와 parameter 갯수?<ul><li>(27 - 5 + 4) / 1 + 1 = 27</li><li>output volume size : 27 x 27 x 256</li><li>parameter 갯수: 5 * 5 * 96 (channel) * 256 (filter 갯수 ) = 614,400</li></ul></li><li>MAX POOL2 ( 3 x 3 filters, stride 2) output volume size?<ul><li>(27 - 3) / 2 + 1 = 13</li><li>output volume size: 13 x 13 x 256</li></ul></li><li>CONV3 layer ( 384 3 x 3 filters, stride 1, padding 1)의 output volume size 와 parameter 갯수?<ul><li>(13-3+2) / 1 + 1 =  13</li><li>ouput volume size = 13 x 13 x 384</li><li>parameters: 13 * 13 * 256(channels) * 384 = 16,613,376</li></ul></li><li>CONV4 layer (384 3x3 filters stride 1, padding 1)<ul><li>(13 - 3 + 2) / 1 + 1 = 13</li><li>output volume size = 13 x 13 x 384</li><li>parameters: 13 * 13 * 384(channels) * 384 = 24,920,064</li></ul></li><li>CONV5 layer (256 3x3 filters stride 1 , padding 1)<ul><li>(13 - 3 + 2) / 1 + 1 = 13</li><li>output volume size = 13 x 13 x 256</li><li>parameters: 13 * 13 * 384 * 256 = 16,613,376</li></ul></li><li>MAX POOL3 (3 x 3 filters strides 2)<ul><li>(13 - 3) / 2 + 1 = 6</li><li>output volume size: 6 x 6 x 256</li></ul></li></ol><h3 id="AlexNet-의-특이점"><a href="#AlexNet-의-특이점" class="headerlink" title="AlexNet 의 특이점"></a>AlexNet 의 특이점</h3><ol><li>당시 GPU 메모리의 부족으로, CONV1 layer 의 경우 depth 가 96이었으나, 48 개씩 (반반) 다른 GPU 에 올려져 계산이 진행되었다. 이는 서로가 데이터를 바라볼수 없다는 것을 의미한다. 마찬가지 의미로, CONV2, CONV4, CONV5의 경우 서로 다른 gpu 상에 올라가 있는 feature map 을 볼수 없다. 반면, CONV3, FC6, FC7, FC8 에서 서로 cross 됨으로써 feature map 을 바라볼수 없는 문제를 완화하였다</li></ol><h2 id="2-VGG"><a href="#2-VGG" class="headerlink" title="2. VGG"></a>2. VGG</h2><p>VGG 는 AlexNet 과 비교하여, 조금더 깊은 convnet 을 가지고 있다. 이는 filter size 를 작게 가져감으로써 얻을 수 있는 이익이었다. </p><p>7x7 conv layer 1개와 3x3 conv layer 가 3개를 비교해본다. 7x7 conv layer 1개의 원본 이미지로부터 얻는 receptive field 는 7x7 영역이다. 3개의 3 x 3 conv layer가 쌓였을 때, 3번째 layer 입장에서, 원본이미지의 7 x 7 만큼의 receptive field, 즉 같은 양의 receptive field 를 얻을 수 있다. 같은 receptive field 영역을 커버하지만, layer 의 수가 증가함으로써, 더 많은 <strong>non-linearity feature map</strong>을 얻을 수 있다. 또한, 각 layer 의 parameter 수를 살펴 보면, 7x7 conv layer 는 C(이전 layer 의 channel 수)<em>7</em>7<em>C = 49 * C^2만큼의 parameter를 가지고 있고, 3 layer 3x3 conv layer 는 3</em>3<em>3</em>C<em>C = 9 * C^2 만큼의 parameter 를 가지고 있다. 작은 filter size로 layer 수를 늘리는 것이 *</em>parameter 의 갯수** 관점에서도 큰 이득을 가져다 준다.</p><p>cf) 네트워크가 깊어질수록 computation양을 일정하게 유지하기 위해 각 레이어의 입력을 downsampling 한다. &amp; Spatial Area 가 작아질수록 filter 의 depth 를 조금씩 늘려준다.</p><h2 id="3-GoogLeNet"><a href="#3-GoogLeNet" class="headerlink" title="3. GoogLeNet"></a>3. GoogLeNet</h2><h3 id="GoogLeNet-의-특이점"><a href="#GoogLeNet-의-특이점" class="headerlink" title="GoogLeNet 의 특이점"></a>GoogLeNet 의 특이점</h3><p>GoogleNet 은 깊은 신경망 모델에 대해, 계산량의 이점을 가져다주기 위해 고안되었다. 이는 Inception Module 을 설계하여, 이를 연속해 쌓는 방식의 구조이다. Inception Module 의 모양은 다음과 같다.</p><p>다음과 같이 구성할 때, conv layer 를 거친 output 을 depth 방향으로 concatenate 한다. spatial dimension 은 stride 등을 조절한다.</p><p><img src="Untitled-fc505115-1120-4553-895a-ff1017096753.png" alt></p><p>feature map 을 뽑기 위해 각 layer 의 filter 갯수를 조절할텐데, 효과적인 feature map 을 뽑기 위해 filter 갯수를 늘리게되면, 전체 Inception Module 이 쌓이면 쌓일 수록, parameter 수가 지수배 증가하는 단점이 발생한다. 여기서 GoogleNet 의 핵심 idea가 등장하는 듯 하다.</p><p><strong>1x1 convolution layer!</strong></p><p>1x1 convolution layer를 통해 spatial dimension 은 보존하면서, depth 는 줄인다. 즉 이 convolution layer 를 bottle neck 이 되는 conv layer 앞단에 구성하여, 입력을 더 낮은 차원으로 보낸다.</p><p><img src="Untitled-455ba9e5-8395-41e4-a9d3-c8543af2883e.png" alt></p><p><img src="Untitled-74d7882d-4001-440f-a9b1-a3198f244dfc.png" alt></p><p>또한 google net 은 parameter 가 많이 필요한 <strong>fc layer 를 제거</strong>하므로써 computational 이득을 취했다. </p><p><img src="Untitled-699859d1-387d-4936-8a1d-a191fbb5b6ef.png" alt></p><p>또한 Inception Module 이 쌓이면서, 깊게 쌓일 수록 loss 의 grdient  전파가 소실 되는 효과를 보완하기 위해 추가적인 classifier 를 곁가지에 닮으로써 gradient 를 추가적으로 update 한다.</p><h2 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4. ResNet"></a>4. ResNet</h2><p>ResNet 은 conv layer 를 깊게 추가하는 것이 성능에 이득을 주는지에 대한 의문으로 시작한다. 즉, 답은 깊게 쌓는 것이 성능이 좋아지지 않는다는 것이다. </p><p><img src="Untitled-a0f95736-d7c0-4fc8-a918-f420a76b55c4.png" alt></p><p>위 그림의 test error 그래프를 보더라도, 성능면에서 conv layer 의 증가가 좋은 성능을 가져다 주지 않는 것이 아니다. test error 그래프만 본다면, overfitting 된 것이 아닌가? 라는 생각을 할 수 있으나, training error 그래프를 보면, 학습 조차 잘 되지 않았음을 확인 할 수 있다.</p><p>즉, 깊이가 깊은 모델이 어느 순간부터는 얕은 모델보다 성능이 안좋아 질 수 있는 문제가 발생한다. 이 문제를 degradation 문제라고 한다.</p><h3 id="ResNet-의-특이점"><a href="#ResNet-의-특이점" class="headerlink" title="ResNet 의 특이점"></a>ResNet 의 특이점</h3><p>ResNet의 구조적 특이점은 바로 skip connection 이다. skip connection 은 이렇게, layer 의 입력과 출력이 더해져, 다음 layer 에 대한 입력으로 이루어 지는 구조를 의미한다.기존의 Neural Net 의 구조를 remind 해본다. 기존 뉴럴넷은 입력 x 가 들어갈 때, 출력 y 를 얻기 위한 H(x)를 찾아내는 과정이다. H(x)가 y 에 최대한 가깝게 하기 위한 즉, H(x) - y 가 0 이 될 수 있도록 최소화 과정을 거쳐 H(x) 를 찾아낸다. 이에 반해 ResNet 은 layer 를 거친 F(x) 와 x 가 더해진 F(x) + x 를 H(x)로 보고 이를 H(x) - x 를 최소화한다. 이는 residual 로 볼 수 있는 F(x)를 최소화한다는 의미이다. </p><p><img src="Untitled-9906cfd9-51eb-4c7f-a3fe-9cd43b5929b7.png" alt></p><h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p><a href="https://www.youtube.com/watch?v=DAOcjicFr1Y" target="_blank" rel="noopener">Lecture 9 | CNN Architectures</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/07/26/CS231n-Lecture09-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture07-Training Neural Networks part2</title>
      <link>https://emjayahn.github.io/2019/07/22/CS231n-Lecture07-Summary/</link>
      <guid>https://emjayahn.github.io/2019/07/22/CS231n-Lecture07-Summary/</guid>
      <pubDate>Mon, 22 Jul 2019 04:12:21 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-07-Training-Neural-Networks-part2&quot;&gt;&lt;a href=&quot;#Lecture-07-Training-Neural-Networks-part2&quot; class=&quot;headerlink&quot; title=&quot;Lecture 07: Training Neural Networks part2&quot;&gt;&lt;/a&gt;Lecture 07: Training Neural Networks part2&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-07-Training-Neural-Networks-part2"><a href="#Lecture-07-Training-Neural-Networks-part2" class="headerlink" title="Lecture 07: Training Neural Networks part2"></a>Lecture 07: Training Neural Networks part2</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><h2 id="1-Fancier-Optimization"><a href="#1-Fancier-Optimization" class="headerlink" title="1. Fancier Optimization"></a>1. Fancier Optimization</h2><h3 id="1-1-Problems-with-SGD"><a href="#1-1-Problems-with-SGD" class="headerlink" title="1-1. Problems with SGD"></a>1-1. Problems with SGD</h3><ul><li>Loss function 의 gradient 방향이 고르지 못해, 지그재그 형태로 업데이트가 일어나고, 그 업데이트가 느리게 발생된다. 고차원으로 갈수록, 자주 만날 수 있는 문제.</li><li>Local Minima &amp; Saddle point: 높은 차원에서 생각해보면, Local Minima 는 고차원의 모든 gradient 방향에 대해 전부 loss 가 증가하는 방향이므로, 이 경우보다는 Saddle point 문제가 훨씬더 빈번하게 발생한다. Saddle point 의 경우 그 포인트 자체도 문제지만, 그 근처에서 gradient 가 매우 작기 때문에 update 를 해도 매우 느린 문제가 있다.</li><li>SGD 의 S - stochastic! : 미니 배치의 loss 를 가지고 전체 training loss 를 추정하는 것이므로, 노이즈가 포함된 추정일 수 밖에없다.</li></ul><h3 id="1-2-다양한-최적화-알고리즘-기법"><a href="#1-2-다양한-최적화-알고리즘-기법" class="headerlink" title="1-2. 다양한 최적화 알고리즘 기법"></a>1-2. 다양한 최적화 알고리즘 기법</h3><p>최적화 알고리즘은 간단하게 정리한다.</p><ul><li>SGD, SGD + Momentum, Nesterov Momentum : 진행방향과 그 gradient (Momentum) 을 더해 실제로 업데이트 될 방향을 정한다.</li><li>AdaGrad : 이전에 진행했던  gradient 를 제곱하여, 업데이트가 진행 될 수록 학습률을 작게해, 세밀하게 업데이트한다.</li><li>RMSProp : AdaGrad 가 gradient 를 제곱할 때, 학습의 후반부에 갈수록 gradient 가 0 에 가까워 지게 되므로, 제곱하는 term 의 비율을 설정해 0으로 가는 것을 막는다.</li><li>Adam : Momentum 방법과 AdaGrad를 합친 방법으로써, 학습률을 조절하면서, 속도를 조절하는 방법이다.</li></ul><h2 id="2-Regularization"><a href="#2-Regularization" class="headerlink" title="2. Regularization"></a>2. Regularization</h2><h3 id="2-1-Dropout"><a href="#2-1-Dropout" class="headerlink" title="2-1. Dropout"></a>2-1. Dropout</h3><p>앞선 강의에서 다양한 Regularization 방법들을 살펴 보았었다.L2, L1, Elastic Net 등. 이번 강의에서는 신경망에서 자주 사용되는 dropout 방법을 살펴본다. Dropout 이란 forward 진행시, 일부 뉴런을 0으로 만들어 버리는 것을 말한다. dropout layer 는 우리가 그 dropout rate 을 설정하므로써, 어떤 확률로 꺼질지 설정할 수 있다. dropout 은 먼저 각 뉴런이 서로 동화되는 것을 방지함으로써 다양한 표현방식을 지닐 수 있게 한다. 또한, dropout 은 여러 sub model 을 앙상블 하는 효과를 낼 수 있다.</p><p>Dropout Layer 를 사용할 시 주의 할 점은 evaluation, inference 시, 네트워크의 출력에 dropout 확률을 곱해주어야 한다는 점이다. 혹은 test 시에는 기존 출력을 그대로 사용하고, train 할 때 dropout확률로 나눠주는 방법이다. (keras 에는 후자로 구현되어있는 듯 하다.)</p><p>p.s. Batch Normalization 에 regularization 의 효과가 있으므로, 일반적으로 BN 과 dropout 을 같이 사용하지는 않는다. 하지만 dropout 에는 우리가 조절할 수 있는 dropout rate 이 있는 장점이 있다.</p><h3 id="2-2-Data-Augmentation"><a href="#2-2-Data-Augmentation" class="headerlink" title="2-2. Data Augmentation"></a>2-2. Data Augmentation</h3><p>신경망은 기본적으로 데이터가 많으면 많을 수록 학습에 유리한 이점이 있다. 따라서 우리가 가지고 있는 데이터를 조금씩 변형하여, 학습 데이터를 늘려주는 방법을 사용할 수 있다. horizontal flip이나, 사진에 일부분을 자르는 방법, Color jittering(이미지의  contrast, brightness 를 변형해준다)</p><h3 id="2-3-Drop-Connect"><a href="#2-3-Drop-Connect" class="headerlink" title="2-3.  Drop Connect"></a>2-3.  Drop Connect</h3><p>Activation 이 아닌, weight 을 확률적으로 0을 만들어 주는 방법</p><h3 id="2-4-Fractional-Max-Pooling"><a href="#2-4-Fractional-Max-Pooling" class="headerlink" title="2-4. Fractional Max Pooling"></a>2-4. Fractional Max Pooling</h3><p>고정된  Pooling window 를 랜덤으로 정하는 방법. 자주 사용되지는 않는다.</p><h2 id="3-Transfer-Learning"><a href="#3-Transfer-Learning" class="headerlink" title="3. Transfer Learning"></a>3. Transfer Learning</h2><p>유명한 모델과 깊은 네트워크는 대부분 많은량의 데이터와 많은 하드웨어 Resource와 시간이 투입되서 구축된다. 우리가 이 모델구조를 가져와 우리의 목적에 맞게 사용하려고 보면, 우리가 가지고 있는 데이터와 자원의 한계로 학습에 실패하거나 그 시간이 매우 오래 걸리곤 한다. Transfer Learning 은 기존에 학습되어있는 모델 구조와 그 weight 을 가져와, 마지막 Fully Connected Layer 를 우리의 task 에 맞게 수정하고 이 부분만 학습하는 개념을 말한다. </p><p><img src="Untitled-efed7ce8-fb60-4622-ad9c-c01d764a5308.png" alt></p><h2 id="4-Reference"><a href="#4-Reference" class="headerlink" title="4. Reference"></a>4. Reference</h2><p><a href="https://www.youtube.com/watch?v=_JB0AO7QxSA" target="_blank" rel="noopener">Lecture 7 | Training Neural Networks II</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/07/22/CS231n-Lecture07-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture06-Training Neural Networks part1</title>
      <link>https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/</link>
      <guid>https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/</guid>
      <pubDate>Thu, 18 Jul 2019 11:38:56 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-06-Training-Neural-Networks-part1&quot;&gt;&lt;a href=&quot;#Lecture-06-Training-Neural-Networks-part1&quot; class=&quot;headerlink&quot; title=&quot;Lecture 06: Training Neural Networks part1&quot;&gt;&lt;/a&gt;Lecture 06: Training Neural Networks part1&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-06-Training-Neural-Networks-part1"><a href="#Lecture-06-Training-Neural-Networks-part1" class="headerlink" title="Lecture 06: Training Neural Networks part1"></a>Lecture 06: Training Neural Networks part1</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><h2 id="1-One-time-setup"><a href="#1-One-time-setup" class="headerlink" title="1. One time setup"></a>1. One time setup</h2><h3 id="1-1-Activation-Functions"><a href="#1-1-Activation-Functions" class="headerlink" title="1-1. Activation Functions"></a>1-1. Activation Functions</h3><p>Activation Function 마다 각 특성과 trade off 가 있다. 많이 사용되는 activation function 이 있지만, LU 계열의 activation function 은 모두 실험의 parameter가 될 수 있다.</p><p>(1) Sigmoid</p><p>$$\sigma(x) = \frac{1}{1+e^{-x}}$$</p><p><img src="Untitled-f80c60bd-97fe-4db9-a0a3-3a8a8c38907c.png" alt></p><p>sigmoid function 의 경우, 수식의 특성상 실수 space 에 있는 값들을 [0, 1] 범위 내로 좁혀 주는 기능을 해, 역사적으로 오래된 activation function 이다. 하지만 다음과 같은 단점이 있다.</p><ul><li>단점<ul><li>일정 범위(Saturated 되는 범위)부터 gradient가 0에 가까워진다.</li><li>sigmoid 의 출력 결과가 zero-centered 가 되지 않는다.</li><li>minor 한 단점이지만, exponential 의 계산이 비효율적이다.(비싸다고 표현)</li></ul></li></ul><p>왜 <strong>zero-centered</strong> 가 중요한가?</p><p>$$f(\sum_iw_ix_i+b)$$</p><p>강의에서 설명해준 직관적인 방법이 매우 도움이 되었다.</p><p>Backpropagation 을 생각해 보게 되면, Neuron(Node) 안에서 local gradient 와 loss 에서 부터 오는 upstream gradient 가 곱해지게 된다. 위 식에서 w 에 대해 local gradient 를 구하면 x_i 가 되는 것을 확인 할 수 있다. 만약 xi 가 모두 양수라고 가정한다면, f()의 gradient 는 항상 양수 또는 음수이고, 이는 w 가 모두 같은 방향으로 움직인다는 것을 의미한다. 즉, 비효율적인 gradient update 라고 할 수 있다. </p><p>(2) hyper tangent : Tanh(x)</p><p>$$tanh(x)=\frac{e^{2x}-1}{e^{2x}+1}$$</p><p><img src="Untitled-c1d35d05-3ea2-49a8-9e8a-5c85bbc519a7.png" alt></p><p>위와 같은 zero-centered problem 을 해결하기 위해 tanh(x) 를 사용할 수 도 있다. (추가적으로, 수업시간에 언급은 없었으나, tanh의 등장배경을 설명하는 또다른 한가지는 sigmoid  와 tanh 의 gradient의 최댓값이 4배 차이가 나므로, backpropagation 에서 gradient vanishing 현상을 방지하는 기대효과로 설명하기도 한다.)하지만, 이 역시 sigmoid 처럼 saturate 되는 부분에서 gradient 가 0이 되는 현상이 아직 남아있다.</p><p>(3) ReLU</p><p>$$f(x) = max(0, x)$$</p><p><img src="Untitled-5850722c-4016-4335-b77a-d5237ed6d5b6.png" alt></p><p>ReLU 의 경우, 가장 우리가 많이 볼 수 있는 activation function 이다. 다음과 같은 장점이 있다고 설명한다.</p><ul><li>장점<ul><li>양수인 구간에서는 gradient 가 0이 되지 않고</li><li>계산이 효율적이다.</li><li>sigmoid 와 tanh 에 비해 실제로 6배 빠른 converge 성능을 보여준다.</li></ul></li><li>단점<ul><li>zero-centered output이 아니다.</li><li>0보다 작은 구간에서는 gradient 가 0이 된다.</li></ul></li></ul><p>(4) Leaky ReLU</p><p>$$f(x) = max(0.01x, x)$$</p><p><img src="Untitled-ed8a9855-d2ce-4756-93d5-e042e0a7b823.png" alt></p><p>0보다 작은 구간에서 ReLU 처럼 Saturate 되는 단점을 없애고자, 0보다 작은 구간에서 작은 gradient 를 주는 것이 Leary Relu 이다. </p><ul><li>장점<ul><li>양수 구간 뿐만아니라 음수 구간에서 gradient 를 작게 주어 gradient 가 0 이 되지 않게 한다.</li><li>ReLU function 과 마찬가지로, 계산이 효율적이고</li><li>sigmoid 와 tanh 에 비해 빠르게 수렴한다.</li></ul></li></ul><p>Parametric Rectifier(PReLU) 라는 이름으로, 좀더 generalize 된 형태도 사용한다. $max(\alpha x, x)$ 형태로써, alpha 를 고정시키지 않고 학습시키는 형태이다.</p><p>(5) Exponential Linear Units (ELU)</p><p>$$f(x)=\begin{cases}x \quad\quad\quad\quad if \quad x&gt;0 \<br>\alpha(exp(x) -1) \quad if \quad x\leq 0 \end{cases}$$</p><p><img src="Untitled-b527e985-4c1c-475f-a01a-6f8bdb006ceb.png" alt></p><p>(6) Maxout Neuron</p><p>ReLU function 의 Generalize 된 형태라고 생각할 수 있다.</p><p>$$max(w_1^{T}x + b_1, w_2^Tx + b_2)$$</p><p>하지만, 이 형태는 각 뉴런마다 두 배의 parameter를 가지고 그 output 값 간의 비교를 하므로, 연산량이 두배가 많아지는 단점이 있다.</p><p>(6) Summary</p><ul><li>ReLU 를 사용한다!</li><li>Leaky ReLU, Maxout, ELU 를 실험해볼 수 있다.</li><li>tanh 도 실험할 수 있지만, 큰 효능을 기대하기 힘들다</li><li><strong>Don’t Use sigmoid</strong></li></ul><h3 id="1-2-Data-Preprocessing"><a href="#1-2-Data-Preprocessing" class="headerlink" title="1-2. Data Preprocessing"></a>1-2. Data Preprocessing</h3><p>앞서 살펴 보았던 것 처럼, 입력 데이터에 있어서 zero-centered 가 매우 중요한 preprocessing key 라고 생각 할 수 있다. 머신러닝에서 처럼 다양한 normalized 기법과 whitening 기법들이 있지만, 뉴럴넷, 특히 이미지 데이터에 대해서는 zero-centered 까지만 전처리 해준다. 이는 모든 차원의 데이터가 같은 범위안에 있게 함으로써 각 차원이 equally contribute 하게 하기 위함이다.</p><p><img src="Untitled-417f2e92-c3d6-4bc1-825e-7dfc79a52956.png" alt></p><h3 id="1-3-Weight-Initialization"><a href="#1-3-Weight-Initialization" class="headerlink" title="1-3. Weight Initialization"></a>1-3. Weight Initialization</h3><p>우리가 설정하는 각 layer 의 weight 을 어떻게 초기화 해줄 것인가의 문제도 뉴럴넷의 학습과 성능에 영향이 있을 수 있다. 작은 gaussian random number 로 모든 weight 을 초기화 해 줄 경우, layer 를 지날 수록 activation function 을 거친 작은 output 과 초기화 된  작은 w 가 곱해져, 점차 그 분산이 작아 지는 것을 확인 할 수 있다. </p><p><img src="Untitled-96d3f546-0140-4ccf-b6b1-925eb79dba94.png" alt></p><p>따라서, 이런 가우시안 랜덤으로 초기화 해주는 것이 아닌 다른 초기화 방법이 등장하였다.</p><ul><li>Xavier Initialization &amp; He Initialization</li></ul><p>개인적으로 Xavier Initialization 과 He Initialization 을 간단하게 실험해 본 내용은 나의 github에 올려두었다. 비교군 설정에 있어, 미흡하지만 직관적으로 이해하기 쉽게 실험해 본 내용이다. 간단한 api 를 통해 구현하였기에, 자세한 수식과 개념은 해당 논문을 읽어보고, 정리해봐야겠다.</p><p>github: <a href="https://github.com/EmjayAhn/TIL/blob/master/03_Pytorch/02_xavier%26he.ipynb" target="_blank" rel="noopener">Xavier vs He experiment</a></p><h3 id="1-4-Batch-Normalization"><a href="#1-4-Batch-Normalization" class="headerlink" title="1-4. Batch Normalization"></a>1-4. Batch Normalization</h3><p>결국 우리는 모든 activation 이 unit gaussian form  이길 원한다. 위에서도 살펴 보았듯이, activation function  에 들어가기 전에 모든 fully connected layer 의 출력이 saturate 구간에 있게 하고 싶지 않은 것이다. 따라서, 주로 Batch Normalization 은 <strong>Fully Connected Layer 이후 비선형함수 전</strong> 또는 <strong>Convolution Layer 다음</strong> 에 들어가게 된다.</p><p>이 때, 주의 해야할 점은 Convolution Layer 다음에 들어가는 batch normalize 는 activation map 으로 나온 channel 별로 수행해주게 된다.</p><ol><li>batch mean 과 variance 는 각 차원별로 계산해 준다.</li><li>Normalize </li></ol><p>$$\hat{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$</p><p>앞서, batch normalization 의 목적은 network의 forward, backward pass 시 saturation 이 일어나지 않게 하기 위함이었습니다. 하지만 반면에, 이렇게 batch normalization 을 해준 이후에도 우리는 network 이 얼마나 해당 activation 을 얼마나 saturate 시킬지까지 학습 할 수 있다면 얼마나 좋을까요? 따라서 우리는 normalize 이후에 scaling factor 와 shifting factor 를 추가시켜, 얼마나 saturate 시킬지까지 학습할 수 있는 parameter 를 추가합니다.</p><p>$$y^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{k}$$</p><p>Batch Noramlization 에 대한 pseudo 알고리즘이다.</p><p><img src="Untitled-83e0b3e7-d764-4a19-9c28-9fe815096b28.png" alt></p><p>또한, 우리는 이렇게 학습한 batch mean 과 variance  를 학습 때 사용하며, testing (inference) 시에는 다시 계산하지 않는 것을 유의해야한다. testing 시에는 training 시 running average 등의 방식으로 고정된 mean과 variance 등을 사용할 수 있다.</p><h2 id="2-Training-Dynamics"><a href="#2-Training-Dynamics" class="headerlink" title="2. Training Dynamics"></a>2. Training Dynamics</h2><h3 id="2-1-Babysitting-the-Learning-Process"><a href="#2-1-Babysitting-the-Learning-Process" class="headerlink" title="2-1. Babysitting the Learning Process"></a>2-1. Babysitting the Learning Process</h3><ol><li><p>Preprocess the data</p></li><li><p>Build Model</p></li><li><p>Sanity check for model (e.g. weigh이 작을ㄹ 때, loss가 negative log likelihood 와 비슷한지, regularization term이 추가될때 loss 가 증가하는지 등)</p></li><li><p>(regularization term 을 사용하지 않고) 매우 작은 데이터에 대해서, train  을 돌렸을 때, loss 가 떨어지고, 금방 overfitting 되는지 확인</p><br>여기까지가 sanity check 이라면, 이제 본격적인 training!!<br></li><li><p>간단한 몇가지 실험을 통해 learning rate 을 정한다. 큰 값, 작은 값을 넣어보고, epoch 을 10까지 정도로 주었을 때, loss 가 주는 지 확인하여 대략적인 범위를 정한다.</p></li><li><p>Coarse search: learning rate 과 다른 hyper parameter 를 uniform 등과 같은 distribution 을 통해 random search 한다. 강의에서 말했던, 주의사항 : 범위의 양 끝에 가장 좋은 score 가 뿌려져 있다면, 다시 범위를 설정해야한다. 내가 처음 설정한 범위 끝단에 존재한다면 그 주변에서 다시 최적의 값이 존재 할 수 있기 때문이다.</p></li><li><p>Finer search: 최적의 값을 찾기 위해 세세하게 튜닝한다.</p></li></ol><h3 id="2-2-Hyperparameter-Optimization"><a href="#2-2-Hyperparameter-Optimization" class="headerlink" title="2-2. Hyperparameter Optimization"></a>2-2. Hyperparameter Optimization</h3><ul><li>많은 양의 Cross Validation 을 통해 성능을 비교, 검증하며 최적의 hyper parameter 를 찾아야한다!</li><li>Grid Search &lt;&lt; Random Search<ul><li>Random Sampling 을 통해 좀더 중요한 parameter 의 분포를 찾아 낼 수 있어, Random search 가 효과적</li></ul></li></ul><h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><p><a href="https://www.youtube.com/watch?v=wEoyxE0GP2M" target="_blank" rel="noopener">Lecture 6 | Training Neural Networks I</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/07/18/CS231n-Lecuture06-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 볼 때마다 헷갈리는 Iterable, Iterator, Generator 정리하기</title>
      <link>https://emjayahn.github.io/2019/07/15/iterator-generator/</link>
      <guid>https://emjayahn.github.io/2019/07/15/iterator-generator/</guid>
      <pubDate>Mon, 15 Jul 2019 14:03:23 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Iterable-vs-Iterator-vs-Generator&quot;&gt;&lt;a href=&quot;#Iterable-vs-Iterator-vs-Generator&quot; class=&quot;headerlink&quot; title=&quot;Iterable vs Iterator vs Generator&quot;&gt;&lt;/a&gt;Iterable vs Iterator vs Generator&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;다른 분들의 코드를 읽을 때마다, 내가 사용할 때마다, 헷갈리는 Iterable, Iterator, Generator를 이번 글을 작성해보면서, 마지막으로! (라는 다짐으로) 정리해봅니다. 잘 알고 있는 개념이라고 생각했지만, 다른 사람들로부터의 질문을 받았을 때, 나의 설명이 만족스럽지 못해 ‘아 내가 더 정확히 알아야 한다’ 는 메타인지로부터 출발하는 글입니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Iterable-vs-Iterator-vs-Generator"><a href="#Iterable-vs-Iterator-vs-Generator" class="headerlink" title="Iterable vs Iterator vs Generator"></a>Iterable vs Iterator vs Generator</h1><hr><p>다른 분들의 코드를 읽을 때마다, 내가 사용할 때마다, 헷갈리는 Iterable, Iterator, Generator를 이번 글을 작성해보면서, 마지막으로! (라는 다짐으로) 정리해봅니다. 잘 알고 있는 개념이라고 생각했지만, 다른 사람들로부터의 질문을 받았을 때, 나의 설명이 만족스럽지 못해 ‘아 내가 더 정확히 알아야 한다’ 는 메타인지로부터 출발하는 글입니다.</p><a id="more"></a><br>파이썬의 장점으로 꼽히는, '사용하기 쉬운 데이터 구조'들 덕분에 우리는 루프를 돌아야하는 알고리즘에 대해 손쉽게 코드를 작성 할 수 있습니다. 하지만, 때로는 나만의 객체(class)를 만들고 그 객체가 파이썬에 내장 되어있는 데이터 구조 처럼, 동작하기를 바랍니다.<br>요즘 들어, 제가 짜는 코드에서 이 욕구는 신경망 모델링을 할 때, 신경망 모델 객체에 데이터의 배치를 feeding 하는 객체를 생성하고 싶을 때, 넘쳐나게 됩니다. 이럴 때, 파이썬에 대한 기본 개념이 잘 잡혀있지 않은 상태에서 복잡한 코드를 짜려고 하는 시도를 하니, 신경망 모델 자체의 구조에 대해서도 복잡한데 이런 루프를 돌면서 반복적으로 일정 데이터를 넘겨주기 위한 간단한 기능을 가진 코드에 대해서도 비효율적으로 작성하게 됩니다. 이런 제 자신의 문제를 해결하기 위해 이번 기회에 Iterator 와 Generator 에 대해 확실하게 정리해보려고 합니다. (feat. 예제코드)<hr><h2 id="1-이터러블-Iterable"><a href="#1-이터러블-Iterable" class="headerlink" title="1. 이터러블: Iterable"></a>1. 이터러블: Iterable</h2><blockquote><p>Iterable 객체란? : 객체 안에 있는 원소(element)를 하나씩 반환 가능한 객체</p></blockquote><p><img src="iterable_def.png" alt></p><p>파이썬이 제공하는 대부분의 내장 데이터 구조는 이터러블(Iterable)한 객체 입니다. 뿐만 아니라 우리가 만든 객체(class)도 Iterable 객체가 될 수 있습니다. 이터러블(Iterable)객체는 for 문과 같은 루프 뿐만 아니라, zip이나 map 과 같은 순서대로 처리할 입력이 필요한 곳에서도 사용 될 수 있습니다.<br><br><br><br>이터러블(Iterable) 객체는 <code>iter()</code> 라는 함수의 입력으로 들어갑니다. <code>iter()</code> 라는 함수는 다음에 설명될 이터레이터(Iterator)를 반환합니다.</p><hr><h2 id="2-이터레이터-Iterator"><a href="#2-이터레이터-Iterator" class="headerlink" title="2. 이터레이터: Iterator"></a>2. 이터레이터: Iterator</h2><blockquote><p>Iterator 객체란? : Iterator의 <code>__next__()</code> 나 내장 함수인 <code>next()</code>를 부르면서, 원소(element)를 순차적으로 반환 할 숫 있는 객체</p></blockquote><p><img src="iterator_def.png" alt></p><p>앞서, 설명드린대로 이터레이터(Iterator)는 iter()라는 함수가 <strong>반환</strong>하는 객체입니다. 그리고, 이터레이터(Iterator)는 반복적으로 <code>__next__()</code>나, <code>next()</code> 함수의 입력으로 들어가 호출하여, <code>next()</code>의 return 값인 원소(element)를 최종적으로 반환합니다.<br><br><br>이터레이터(Iterator)가 다음 원소를 계속 반환하다가, 끝에 다달아 반환할 원소가 없을 경우 예외문인 <code>StopIteration</code>이 발생하게 됩니다.<br><br><br>즉, 정리하면, <br><br>Iterable 객체 A → iter(A) → Iterator 객체 B → next(B) → element (data)</p><hr><blockquote><p>Question? 우리는 list 같은 이터러블(Iterable) 객체와 for 문을 쓰면서 한번도 iter() 나 next()를 보지 못했는뎁쇼????<br>파이썬의 for 문은 이터러블(Iterable) 객체를 만나, 내부적으로 <code>iter()</code> 함수를 호출하여, 이터레이터(Iterator)를 생성합니다. 이 생성된 이터레이터(Iterator)가 루프가 실행되면서 <code>next()</code> 를 호출하며 반복적인 데이터를 뽑아 낼 수 있게 되는 것입니다. 그리고 모든 원소가 뽑아지고 난 뒤에는 <code>StopIteration</code> 이 발생하며, for 문이 종료 됩니다.</p></blockquote><hr><p>위 설명을 코드로 풀어쓰면, 다음과 같습니다. 우리가 다음과 같은 for 문을 사용하면,</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> iterable_object:</span><br><span class="line">    print(element)</span><br></pre></td></tr></table></figure><p>위 코드는 다음과 같이 파이썬 내부적으로 다음과 같이 동작하게 됩니다.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># iter() 함수를 호출해, iterator 를 생성하고,</span></span><br><span class="line">iterator_object = iter(iterable_object)</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:</span><br><span class="line">    <span class="hljs-comment"># next() 함수를 호출해, element 를 받아옵니다.</span></span><br><span class="line">    <span class="hljs-keyword">try</span>:</span><br><span class="line">        element = next(iterator_object)</span><br><span class="line">        print(element)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># element 가 없을 시, StopIteration Exception 발생</span></span><br><span class="line">    <span class="hljs-keyword">except</span>: StopIteration:</span><br><span class="line">        <span class="hljs-keyword">break</span></span><br></pre></td></tr></table></figure><hr><h2 id="3-이터러블-Iterable-이터레이터-Iterator-와-친해지기"><a href="#3-이터러블-Iterable-이터레이터-Iterator-와-친해지기" class="headerlink" title="3. 이터러블(Iterable), 이터레이터(Iterator)와 친해지기"></a>3. 이터러블(Iterable), 이터레이터(Iterator)와 친해지기</h2><h3 id="3-1-간단-버전"><a href="#3-1-간단-버전" class="headerlink" title="3-1. 간단 버전"></a>3-1. 간단 버전</h3><p>파이썬 이터러블(Iterable) 내장 데이터 구조인 list 를 활용하여, 실습해 봅니다. </p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>iterable_object = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>iterator_object = iter(iterable_object)</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>iterator_object</span><br><span class="line">&lt;list_iterator object at <span class="hljs-number">0x104fe2278</span>&gt;</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">1</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">2</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">3</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">4</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line"><span class="hljs-number">5</span></span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>next(iterator_object)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="hljs-string">"&lt;stdin&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure><h3 id="3-2-나만의-iterable-iterator객체"><a href="#3-2-나만의-iterable-iterator객체" class="headerlink" title="3-2. 나만의 iterable, iterator객체"></a>3-2. 나만의 iterable, iterator객체</h3><p>다음의 코드는 이름과 나이를 받는 데이터 객체입니다. 루프를 돌면서 각각 순서에 맞는 (이름, 나이)형태로 데이터를 반환하는 Iterable 객체입니다. 같은 기능을 하는 더욱 효율적인 코드를 짤 수 있겠지만, 공부한 iterable 과 iterator 를 적용해보는 class 입니다.</p><script src="https://gist.github.com/EmjayAhn/70e4232632f340a8f67e0ba1509029c5.js"></script><hr><h2 id="4-제너레이터-Generator"><a href="#4-제너레이터-Generator" class="headerlink" title="4. 제너레이터: Generator"></a>4. 제너레이터: Generator</h2><blockquote><p>제너레이터(Generator)란?: <strong>특이한</strong> (공식 문서에 <del>~</del> iterator) 이터레이터(Iterator)</p></blockquote><p><img src="generator_def.png" alt></p><p>정의에서도 보다시피, 이터레이터(Iterator)입니다. 즉, <code>next()</code> 함수를 만나 동작하는 함수입니다. 이 때, 제너레이터(Generator) 를 일반 함수와 다르게 하는 것이 <code>yield</code> 라는 문법입니다. <code>yield</code> 는 일반 함수의 <code>return</code>과 같이 값을 반환 하지만, <code>return</code>과 다르게 해당 함수(generator)가 종료하지 않고, 그대로 유지됩니다. 다음 순서의 제너레이터(Generator)가 호출되면, 멈추었던 <code>yield</code> 자리에서 다시 함수가 동작하게 됩니다.<br><br></p><p>요약하자면, <br><br>제너레이터(Generator) → next(제너레이터) → 제너레이터 함수 실행 → <code>yield</code>를 만나 next(제너레이터가) 호출된 곳으로 값을 반환 (제너레이터 종료 ❌) → 다시 next(제너레이터) → 멈추었던 <code>yield</code>부터 재실행 </p><hr><blockquote><p>Question? 이런 특이하고, 어렵고, 처음엔 익숙하지 않은 제너레이터(Generator)를 왜 때문에 쓰는 겁죠???<br>제너레이터는 메모리를 아끼기 위해서 사용합니다!!! 다음의 코드에서 메모리 사용량의 비교를 통해 살펴 보겠습니다.</p></blockquote><hr><h2 id="5-제너레이터-Generator-와-친해지기"><a href="#5-제너레이터-Generator-와-친해지기" class="headerlink" title="5. 제너레이터 (Generator)와 친해지기"></a>5. 제너레이터 (Generator)와 친해지기</h2><p>제너레이터를 사용한 코드가 메모리 사용량 측면에서 얼마나 효율적인지 확인해보겠습니다. 다음의 코드는 999999까지의 숫자를 제곱하여 return 해주는 함수입니다. 첫번째, 일반적인 함수는 end 숫자까지 for 문을 돌면서 그 결과를 저장한 뒤에 return 해줍니다. 두번째 generator 는 for 문이 돌 때, yield 에서 해당 데이터만 리턴하게 되므로, 메모리 사용에서 효율적입니다. 다음의 실험에서도 쉽게 비교할 수 있습니다.</p><script src="https://gist.github.com/EmjayAhn/943cde0f7e9794f76272e531c8fffcca.js"></script><p>위 코드 실행결과: <br></p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Memory Usage when program start: 8.80859375</span><br><span class="line">Memory Usage when program end: 56.0390625</span><br></pre></td></tr></table></figure><script src="https://gist.github.com/EmjayAhn/8ab50ebfdbf3adabeab8f07f20dd0e01.js"></script><p>위 코드 실행결과: <br></p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Memory Usage when program start: 8.78515625</span><br><span class="line">Memory Usage when program end: 8.78515625</span><br></pre></td></tr></table></figure><hr><h2 id="6-마무리"><a href="#6-마무리" class="headerlink" title="6. 마무리"></a>6. 마무리</h2><p>이번 짧은 글에서, Iterable, Iterator, Generator 를 비교해보면서 그 개념과 사용 예제를 간단하게 살펴 보았습니다. 이름에서부터 헷갈릴 수 있는 각 객체에 대한 내용을 이 글을 통해 조금이나마 정리해볼 수 있는 기회가 되셨으면 좋겠으며, 작은 도움이 되셨으면 합니다.</p><hr><h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><ul><li><a href="https://docs.python.org/3.7/glossary.html#term-generator" target="_blank" rel="noopener">Python documentation: generator</a></li><li><a href="https://docs.python.org/3.7/glossary.html#term-iterable" target="_blank" rel="noopener">Python documentation: iterable</a></li><li><a href="https://docs.python.org/3.7/glossary.html#term-iterator" target="_blank" rel="noopener">Python documentation: iterator</a></li></ul>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/07/15/iterator-generator/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS224n]Lecture01-WordVecs</title>
      <link>https://emjayahn.github.io/2019/07/06/CS224n-Lecture01-Summary/</link>
      <guid>https://emjayahn.github.io/2019/07/06/CS224n-Lecture01-Summary/</guid>
      <pubDate>Sat, 06 Jul 2019 08:38:25 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;CS224n-Lecture-1-Introduction-and-Word-Vectors&quot;&gt;&lt;a href=&quot;#CS224n-Lecture-1-Introduction-and-Word-Vectors&quot; class=&quot;headerlink&quot; title=&quot;[CS224n] Lecture 1: Introduction and Word Vectors&quot;&gt;&lt;/a&gt;[CS224n] Lecture 1: Introduction and Word Vectors&lt;/h1&gt;&lt;p&gt;Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="CS224n-Lecture-1-Introduction-and-Word-Vectors"><a href="#CS224n-Lecture-1-Introduction-and-Word-Vectors" class="headerlink" title="[CS224n] Lecture 1: Introduction and Word Vectors"></a>[CS224n] Lecture 1: Introduction and Word Vectors</h1><p>Standford University 의 CS224n 강의를 듣고 정리하는 글입니다.</p><a id="more"></a><h2 id="1-Human-Language-and-Word-Meaning"><a href="#1-Human-Language-and-Word-Meaning" class="headerlink" title="1. Human Language and Word Meaning"></a>1. Human Language and Word Meaning</h2><p><strong>Word meaning</strong> 의 뜻 : symbol → idea or thing : Denotational Semantics</p><p>우리가 ‘의자’라는 단어를 예로 들자면, 말하는 사람과 듣는 사람 모두 의자와 관련된 특정한 이미지와 아이디어 등을 생각해 볼 수 있다. 이렇게 단어에 대해 관념적인 것을 word meaning 이라고 할 수 있다. 그렇다면 컴퓨터에서 사용할 수 있는 <strong>word meaning</strong> 은 어떤 것이 있을까?</p><h3 id="In-Computer-Word-Representations"><a href="#In-Computer-Word-Representations" class="headerlink" title="In Computer Word Representations"></a>In Computer Word Representations</h3><ol><li>WordNet: synonym 과 hypernyms 의 집합으로 표현 (e.g: nltk wordnet)<ul><li>아주 귀한 dataset 이지만,</li><li>단점: Nuance (뉘앙스)를 담아내지 못함, 새로운 단어에 대해 사람이 직접 가공하여 추가해주어야 한다. 단어의 동의어에 대해 계산할 수 없음</li></ul></li><li>Discrete Symbols: One-hot-vectors<ul><li>단점 : 벡터의 차원이 우리가 가지고 있는 단어의 갯수만큼 커지게 된다. Corpus 의 구성 단어가 커질 수록 계산 해야하는 벡터 차원이 매우 커진다. 이는 컴퓨팅 자원의 부족으로 인한 현실적 구현의 어려움을 야기시킨다.</li><li>One-hot-Vector 는 Vector Space 에서 모두 서로 orthogonal (직교)한다. 즉, similarity 가 모두 0으로 단어간의 관계를 알기 힘들다.</li></ul></li><li>By CONTEXT: Word Vecttors<ul><li>“You shall know a word by the company it keeps” (J.R.Firth)</li><li>어떤 특정 단어 w 가 나온다는 것은, 그 주변 단어들(context)이  있기 때문이다.</li><li>Word Vectors == Word Embeddings == Word Representations == Distributed Representation</li></ul></li></ol><h2 id="2-Word2vec-Introduction"><a href="#2-Word2vec-Introduction" class="headerlink" title="2. Word2vec Introduction"></a>2. Word2vec Introduction</h2><p>Word2vec(Mikolov et al. 2013) 은 Word Vectors 를 만들기 위한 알고리즘 중 가장 기초 뼈대를 이루는 알고리즘이다.</p><ol><li>가지고 있는 텍스트 데이터를 구성하는 큰 CORPUS</li><li>Corpus 의 모든 단어는 RandomVector 로 시작한다.</li><li>각 position t 에 대해, 중심단어 c(center) 와 주변단어 o(outside) 를 생각할 수 있다.</li><li>중심단어 c 가 주어질 때, 주변단어 o 의 확률을 구하기 위해(skip-gram)(반대로도 가능(cbow): 주변단어가 주어질 때, 중심단어의 확률을 구하는 방법), 중심단어 c 와 주변단어 o, word 벡터들에 대해 similarity를 구한다.</li><li>위의 확률을 최대화 하기 위해 word vector 를 updating 한다.</li></ol><p><img src="Untitled-cc28ca48-12e4-4a20-a6c4-b0be86d339d8.png" alt></p><h2 id="3-Word2vec-objective-function-gradients"><a href="#3-Word2vec-objective-function-gradients" class="headerlink" title="3. Word2vec objective function gradients"></a>3. Word2vec objective function gradients</h2><p>문장의 특정 위치 t 에 대해 (t = 1, …, T), 중심단어w_j가 주어졌을때, 주변단어를 한정하는 window size m 에 대해  주변단어들을 예측하는 Likelihood 는 다음과 같습니다. likelihood 식을 해석해보면, 중심단어 w_t 에 대해 중심단어를 중심으로 window 사이즈 2m 개의 단어들의 확률을 모두 곱하고, T 개의 단어 갯수에 대해 또 다시 모두 곱합니다.</p><p>$$Likelihood\quad L(\theta)= \prod_{t=1}^{T}\prod_{-m \leq j \leq m}P(w_{t+j}|w_{t};\theta)$$</p><p>목적함수는 likelihood 를 바탕으로 minimize와 average, 계산 편의를 위해 log 를 씌웠다는 것외에 likelihood 와 동일하다.</p><p>$$objective function \quad J(\theta) = - \frac{1}{T}logL(\theta)=-\frac{1}{T}\sum_{t=1}^{T}\sum_{-m \leq j \leq m, j\neq0}logP(w_{t+j} | w_{t};\theta)$$</p><p>단어가 등장할 확률을 구하는 방법은 Softmax Function 을 활용합니다.</p><p><img src="Untitled-99acc04b-91fd-451c-868f-574d995778b3.png" alt></p><h3 id="Objective-Function-의-derivative"><a href="#Objective-Function-의-derivative" class="headerlink" title="Objective Function 의 derivative"></a>Objective Function 의 derivative</h3><p>Objective Function 과 단어 확률 (Softmax) 의 미분을 구하는 과정이다. </p><p><img src="Untitled-d45516b7-8024-457c-bda2-39092a1f6892.png" alt></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/07/06/CS224n-Lecture01-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[GCP] Computing Engine 환경설정</title>
      <link>https://emjayahn.github.io/2019/06/17/gcp-setting/</link>
      <guid>https://emjayahn.github.io/2019/06/17/gcp-setting/</guid>
      <pubDate>Mon, 17 Jun 2019 11:54:01 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Computing-Engine-환경설정&quot;&gt;&lt;a href=&quot;#Computing-Engine-환경설정&quot; class=&quot;headerlink&quot; title=&quot;Computing Engine 환경설정&quot;&gt;&lt;/a&gt;Computing Engine 환경설정&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이번 글은, Google Cloud Platform의 Computing Engine 인스턴스의 기본적인 환경설정 방법을 소개하는 글입니다. 직접 작업환경을 세팅하면서, 정리 하는 목적으로 작성하는 글입니다.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Computing-Engine-환경설정"><a href="#Computing-Engine-환경설정" class="headerlink" title="Computing Engine 환경설정"></a>Computing Engine 환경설정</h1><ul><li>이번 글은, Google Cloud Platform의 Computing Engine 인스턴스의 기본적인 환경설정 방법을 소개하는 글입니다. 직접 작업환경을 세팅하면서, 정리 하는 목적으로 작성하는 글입니다.<br></li></ul><a id="more"></a><ul><li>다음과 같은 환경을 설정합니다.</li></ul><ol><li>인스턴스 생성 및 네트워크 설정</li><li>접속을 위한 ssh 생성 및 접속</li><li>python3, pip3 설치</li><li>CUDA 설치</li><li>cuDNN 설치</li><li>Pytorch 설치</li><li>Jupyter 설치 및 환경설정</li></ol><h2 id="1-인스턴스-생성-및-네트워크-설정"><a href="#1-인스턴스-생성-및-네트워크-설정" class="headerlink" title="1. 인스턴스 생성 및 네트워크 설정"></a>1. 인스턴스 생성 및 네트워크 설정</h2><p>Compute Engine에서 자신의 목적에 맞는 리소스를 정해, 인스턴스를 생성해줍니다.</p><p><img src="Untitled-aa8ea35c-8d35-4efb-8fda-520336241429.png" alt></p><p>Jupyter  notebook 을 사용하기 위해, VPC 네트워크 → 방화벽 규칙 탭에서 방화벽 규칙을 만들어 줍니다. 후에 Tensorboard 와 다른 기타 환경들을 사용할 때 그에 맞는 포트 규칙을 동일한 방법으로 열어주면 됩니다. 아래의 4가지를 설정해주고 ‘만들기’ 클릭</p><p>이름 : jupyter</p><p>소스 IP 범위: 0.0.0.0/0</p><p>대상 태그: jupyter</p><p>tcp: 8888</p><p>http, https: 체크</p><p><img src="Untitled-e502c0fe-c454-4da1-bf99-5ee9c057bab5.png" alt></p><h2 id="2-접속을-위한-RSA-key-pair-생성-및-ssh-접속"><a href="#2-접속을-위한-RSA-key-pair-생성-및-ssh-접속" class="headerlink" title="2. 접속을 위한 RSA key pair 생성 및 ssh 접속"></a>2. 접속을 위한 RSA key pair 생성 및 ssh 접속</h2><ul><li>gcp 에서 제공하는 gcloud로도 접속 할 수 있습니다.</li></ul><ol><li><p>위에서 생성한 인스턴스에 접속하기 위해, RSA key pair 를 이를 통해 접속 해 봅니다. 아래의 명령어를 이용해 키페어를 생성해 줍니다. 이 때, USERNAME 은 gcp에 등록한 이메일로 설정합니다.</p><p> $ ssh-keygen -t rsa -f ~/.ssh/[KEYFILE_NAME] -C “[USERNAME]”</p><p> example) $ ssh-keygen -t rsa -f ~/.ssh/gcp-key -C “<a href="mailto:myemail@mail.com" target="_blank" rel="noopener">myemail@mail.com</a>“</p></li></ol><p>앞으로 사용할 Password 를 입력하고, </p><p>생성된 키페어는 <code>.ssh</code> 폴더 안에서 확인 할 수 있습니다. <code>.ssh/[KEYFILE_NAME].pub</code> 를 확인해 볼 수 있습니다.</p><ol start="2"><li>생성한 키페어를 gcp 의 메타데이터 탭 → SSH 키에 등록합니다.</li></ol><p><img src="Untitled-ac2cd300-7176-46cc-92d2-aa405a955f85.png" alt></p><ol start="3"><li><p>ssh 접속</p><p> $ ssh -i ~/.ssh/[KEYFILE_NAME] [USERNAME]@[GCP외부IP]</p></li></ol><h2 id="3-Python3-PIP-설치"><a href="#3-Python3-PIP-설치" class="headerlink" title="3. Python3, PIP 설치"></a>3. Python3, PIP 설치</h2><p>위에서 서버에 접속했다면, 서버의 개발환경을 설정해 주기만 하면 됩니다. python3 와 pip 부터 설치해 봅니다.</p><ol><li>locale 설정<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install language-pack-ko</span><br><span class="line">$ sudo locale-gen ko_KR.UTF-8</span><br><span class="line"></span><br><span class="line">$ <span class="hljs-built_in">export</span> LC_ALL=<span class="hljs-string">"en_US.UTF-8"</span></span><br><span class="line">$ <span class="hljs-built_in">export</span> LC_CTYPE=<span class="hljs-string">"en_US.UTF-8"</span></span><br><span class="line">$ sudo dpkg-reconfigure locales</span><br></pre></td></tr></table></figure></li></ol><p>en_US.UTF-8이 [*] 로 체크 되어 있는지까지 확인합니다.</p><ol start="2"><li>Python3, PIP 설치<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install python3-pip</span><br></pre></td></tr></table></figure></li></ol><p>설치 후, <code>python3 --version</code> 으로 python 이 잘 설치 되었는지 확인합니다.</p><h2 id="4-CUDA-설치"><a href="#4-CUDA-설치" class="headerlink" title="4. CUDA 설치"></a>4. CUDA 설치</h2><p>우리가 가장 원하는 리소스인 gpu를 활용한 연산을 위해 CUDA 를 설치 해 줍니다.</p><ol><li>먼저, 설치 파일을 다운로드 해줍니다.</li></ol><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 루트에서</span><br><span class="line">$ wget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64</span><br></pre></td></tr></table></figure><ol start="2"><li><code>ls</code> 로 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb 이 잘 다운로드 되었는지 확인 해 줍니다. 다운로드 결과 <code>.deb</code> 확장자가 되어있지 않다면, 파일명에 .deb 를 뒤에 붙여 줍니다.</li></ol><p><code>mv cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64 cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</code></p><ol start="3"><li>설치<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb</span><br><span class="line">$ sudo apt-key add /var/cuda-repo-&lt;version&gt;/7fa2af80.pub</span><br><span class="line"><span class="hljs-comment">### $ sudo apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pub</span></span><br><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install cuda</span><br></pre></td></tr></table></figure></li></ol><p>CUDA 가 정상적으로 설치되었다면,  <code>$ nvidia-smi</code> 를 통해 자신의 gpu 상태를 확인 할 수 있습니다. 또한, <code>/usr/local/cuda/version.txt</code> 에 설치한 CUDA, 현재는 10.0의 version 을 확인 할 수 있습니다.</p><ul><li><code>NVIDA tool-kit 설치</code><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nvidia-cuda-toolkit</span><br></pre></td></tr></table></figure></li></ul><h2 id="5-cuDNN-설치"><a href="#5-cuDNN-설치" class="headerlink" title="5. cuDNN 설치"></a>5. cuDNN 설치</h2><p>다음의 명령어를 통해 cuDNN 을 설치 할 수 있습니다.</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh -c <span class="hljs-string">'echo "deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /" &gt;&gt; /etc/apt/sources.list.d/cuda.list'</span></span><br><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install libcudnn7-dev</span><br></pre></td></tr></table></figure><h2 id="6-Pytorch-설치"><a href="#6-Pytorch-설치" class="headerlink" title="6. Pytorch 설치"></a>6. Pytorch 설치</h2><ul><li>우리는 서버가 https 프로토콜을 사용한다고 체크했으므로 -H  flag 를 주어 sudo pip3 를 활용해야합니다.<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -H pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl</span><br><span class="line">$ sudo -H pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl</span><br></pre></td></tr></table></figure></li></ul><h2 id="7-Jupyter-Notebook-설치-및-환경설정"><a href="#7-Jupyter-Notebook-설치-및-환경설정" class="headerlink" title="7. Jupyter Notebook 설치 및 환경설정"></a>7. Jupyter Notebook 설치 및 환경설정</h2><ol><li>Jupyter를 설치합니다.<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -H pip3 install jupyter</span><br></pre></td></tr></table></figure></li></ol><p>설치 후, <code>$ jupyter notebook</code> 으로 주피터 커널이 켜지는지 확인합니다.</p><ol start="2"><li><p>주피터 config 파일을 생성합니다.</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></li><li><p>비밀번호를 생성합니다. 이 비밀번호는 지금 설치한 주피터 환경에 들어가기 위한 비밀번호 입니다.</p><figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ipython</span><br><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd()</span><br><span class="line">Enter Password: 사용할 비밀번호 입력</span><br><span class="line">Verify Password: 사용할 비밀번호 입력</span><br></pre></td></tr></table></figure></li></ol><p>출력된 비밀번호 해쉬  <code>sha1: ~~~</code> 를 복사해 둡니다.</p><ol start="4"><li>우리의 인스턴스는 https 프로토콜을 사용하므로, SSL 키파일을 생성해야 합니다.<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout cert.pem -out cert.pem</span><br></pre></td></tr></table></figure></li></ol><p>위 명령어를 입력하고, 뒤따라 나오는 정보들을 입력하여, <code>.pem</code> 파일을 생성합니다.</p><ol start="5"><li><ol start="2"><li>에서 생성한 config 파일 수정<figure class="highlight bash hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /home/[USERNAME]/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure></li></ol></li></ol><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// config 파일에 다음의 내용을 추가합니다.</span><br><span class="line">c = get_config()</span><br><span class="line">c.NotebookApp.ip = &apos;내부아이피주소&apos;</span><br><span class="line">c.NotebookApp.open_browser=False</span><br><span class="line">c.NotebookApp.password=&apos;3에서 생성한 비밀번호 해쉬 sha1:~&apos;</span><br><span class="line">c.Notebook.certfile=&apos;4에서 생성한 .pem 파일 경로 /home/USERNAME/cert.pem&apos;</span><br></pre></td></tr></table></figure><p>위까지 완료하게 되었으면, 주피터 서버를 열고,  <code>https://외부아이피:8888</code>  로 주피터를 접속 하는 것을 확인하시면 되겠습니다.</p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/06/17/gcp-setting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Classification Metrics</title>
      <link>https://emjayahn.github.io/2019/06/03/Classification-Metrics/</link>
      <guid>https://emjayahn.github.io/2019/06/03/Classification-Metrics/</guid>
      <pubDate>Mon, 03 Jun 2019 03:18:57 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Classification-Metrics-분류-성능-지표&quot;&gt;&lt;a href=&quot;#Classification-Metrics-분류-성능-지표&quot; class=&quot;headerlink&quot; title=&quot;Classification Metrics: 분류 성능 지표&quot;&gt;&lt;/a&gt;Classification Metrics: 분류 성능 지표&lt;/h1&gt;&lt;p&gt;Kaggle Classification 의 Evaluation 에 자주 사용되는 ROC-AUC를 정리해보면서, 이번 기회에 분류모델에서 자주 사용되는 성능지표(Metric)을 간단하게 정리해봅니다. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Classification-Metrics-분류-성능-지표"><a href="#Classification-Metrics-분류-성능-지표" class="headerlink" title="Classification Metrics: 분류 성능 지표"></a>Classification Metrics: 분류 성능 지표</h1><p>Kaggle Classification 의 Evaluation 에 자주 사용되는 ROC-AUC를 정리해보면서, 이번 기회에 분류모델에서 자주 사용되는 성능지표(Metric)을 간단하게 정리해봅니다. </p><a id="more"></a><p>Confusion Matrix 에서 비롯되는 Metric 들은 이를 이미지로 기억하는 것이 효율적입니다.</p><h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><p>confusion matrix  는 해당 데이터의 정답 클래스(y_true) 와 모델의 예측 클래스(y_pred)의 일치 여부를 갯수로 센 표입니다. 주로, 정답 클래스는 행으로(row), 예측 클래스는 열로(columns) 표현합니다. </p><p><img src="Untitled-170ac3e3-8259-4e1a-80e5-647b5bd326fd.png" alt></p><ul><li>(정의하기에 따라 다르지만) 일반적으로, class 1 을 positive로, class 0 을 negative 로 표기합니다.</li><li>우리가 예측한 클래스를 기준으로 Positive 와 Negative 를 구분합니다.</li><li>그리고 정답과 맞았는지, 틀렸는지를 알려주기 위해 True 와 False 를 각각 붙여줍니다.</li></ul><h2 id="Accuracy-정확도"><a href="#Accuracy-정확도" class="headerlink" title="Accuracy: 정확도"></a>Accuracy: 정확도</h2><ul><li>전체 데이터에 대해 맞게 예측한 비율</li></ul><p>$$<br>\frac{TP+TN}{TP+FN+FP+TN}<br>$$</p><h2 id="Precision-정밀도"><a href="#Precision-정밀도" class="headerlink" title="Precision: 정밀도"></a>Precision: 정밀도</h2><ul><li>class 1 이라고 예측한 데이터 중, 실제로 class 1 인 데이터의 비율</li></ul><p>$$\frac{TP}{TP+FP}$$</p><p>우리의 모델이 기본적인 decision이 class 0 이라고 생각할 때, class 1 은 특별한 경우를 detect 한 경우 일 것입니다. 이 때 class 1 이라고 알람을 울리는 우리의 모델이 얼마나 세밀하게 class를 구분 할 수 있는지의 정도를 수치화 한 것입니다.</p><h2 id="Recall-재현율"><a href="#Recall-재현율" class="headerlink" title="Recall: 재현율"></a>Recall: 재현율</h2><ul><li>실제로 class 1 인 데이터 중에 class 1 이라고 예측한 비율</li><li>= Sensivity = TPR</li></ul><p>$$\frac{TP}{TP+FN}$$</p><p>제가 기억하는 방식은, 자동차에 결함이 발견되서 recall 이 되어야 하는데 (실제 고장 데이터중) 얼마나 recall 됐는지로 생각합니다. </p><h2 id="Fall-out-위양성율"><a href="#Fall-out-위양성율" class="headerlink" title="Fall-out: 위양성율"></a>Fall-out: 위양성율</h2><ul><li>실제로 class 1 이 아닌 데이터 중에 class 1이라고 예측한 비율</li><li>낮을 수록 좋음</li><li>= FPR = 1 - Specificity</li><li>Specificity = 1 - Fall out</li></ul><p>$$\frac{FP}{FP+TN}$$</p><p>실제로 양성데이터가 아닌 데이터에 대해서 우리의 모델이 양성이라고 잘못 예측한 비율을 말합니다. 억울한 데이터의 정도를 측정했다고 생각 할 수 있습니다.</p><h2 id="각-Metric-간의-상관관계"><a href="#각-Metric-간의-상관관계" class="headerlink" title="각 Metric 간의 상관관계"></a>각 Metric 간의 상관관계</h2><p>우리 모델의 decision function 을 f(x) 라 할 때, 우리는 f(x)의 결과와 threshold (decision point)를 기준으로 class를 구분합니다.</p><ul><li><p>Recall vs Fall-out : 양의 상관관계</p><p>  Recall은 위의 정의에 의하듯이, 실제로 positive  인  클래스에 대해 얼마나 positve 라고 예측했는지의 비율입니다. 우리가 Recall 을 높이기 위해서는 고정되어있는 실제 positive 데이터 수에 대해 예측하는 positive 데이터의 갯수 threshold 를 낮춰 늘리면 됩니다. 이에 반해 threshold 를 낮추게 되면, 실제로 positive 가 아닌 데이터에 대해 positive 라고 예측하는 억울한 데이터가 많아지므로 Fall-out 은 커지게 되고 둘은 양의 상관관계를 갖게 됩니다.</p></li><li><p>Recall vs Precision : 대략적인 음의 상관관계</p><p>  위에 설명한 것처럼 threshold 를 낮춰 우리가 예측하는 positive 클래스의 숫자를 늘리게 되면, recall 은 높아지는 반면, 예측한 positive 데이터 중 실제 positive 데이터의 비율은 작아 질 수 있습니다.</p></li></ul><h2 id="F-beta-score"><a href="#F-beta-score" class="headerlink" title="F-beta score"></a>F-beta score</h2><ul><li>precision 과 recall의 가중 조화평균</li></ul><p>$$(\frac{1}{1+\beta^2}\frac{1}{precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{recall})^{-1}$$</p><p>이처럼, 다양한 Metric 에 대해 우리가 초점을 맞추는 것에 따라 모델의 성능은 다르게 바라 볼 수 있습니다. 따라서 모델에 대해 성능을 평가하고 최종 모델을 선택함에 있어, 서로 다른 Metric 을 동시에 비교해야합니다. 이를 위해 precision 과 recall 을 precision 에 beta^2 만큼 가중하여 바라보는 스코어가 F beta score 입니다.</p><p>이 중 beta=1 일 때, score 가 우리가 자주 보는 f1 score 입니다.</p><p>$$F_1=\frac{2<em>precision</em>recall}{precision+recall}$$</p><h2 id="ROC-Curve-Receiver-Operator-Characteristic-Curve"><a href="#ROC-Curve-Receiver-Operator-Characteristic-Curve" class="headerlink" title="ROC Curve: Receiver Operator Characteristic Curve"></a>ROC Curve: Receiver Operator Characteristic Curve</h2><ul><li>Recall vs Fallout 의 plot (TPR vs FPR)</li></ul><p>위의 예시 처럼, 우리가 클래스를 판별하는 기준이 되는 threshold (decision point) 를 올리거나 내리면서, recall 과 fall out 은 바뀌게 됩니다. 이렇게 threshhold 를 변화 해 가면서, recall 과 fall out 을 plotting 한 것이 ROC curve 입니다.</p><ul><li>sklearn.metrics.roc_curve() 의 documentation</li></ul><p><img src="Untitled-1f189e32-c453-43c9-8366-559df3f16464.png" alt></p><h2 id="ROC-AUC-Area-Under-Curve"><a href="#ROC-AUC-Area-Under-Curve" class="headerlink" title="(ROC-)AUC: Area Under Curve"></a>(ROC-)AUC: Area Under Curve</h2><ul><li>위에서 그린 ROC Curve 의 넓이를 점수로써 사용하는 것이 AUC 입니다. AUC 의 유의미한 범위는 class 를 50%의 확률로 random 하게 예측한 넓이인 0.5 보다는 클 것이고, 가장 최대의 AUC 의 넓이는 1 일 것이므로 0.5≤AUC≤1 의 범위를 갖는 score 입니다.</li></ul><p><img src="Untitled-44d5d317-d0ec-4f1a-8aa0-8bdbfe34bd67.png" alt></p><ul><li>ROC 커브와 AUC score 를 보고 모델에 대한 성능을 평가 하기 위해서, ROC 는 같은 Fall out 에 대해 Recall  은 더 높길 바라고, 같은 Recall  에 대해서는, Recall  이 더 작길 바랍니다. 결국, 그래프가 왼쪽 위로 그려지고, AUC 즉 curve  의 넓이는 커지는 것이 더 좋은 성능의 모델이라고 볼 수 있습니다.</li></ul>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/06/03/Classification-Metrics/#disqus_thread</comments>
    </item>
    
    <item>
      <title>SPARK BASIC 1</title>
      <link>https://emjayahn.github.io/2019/05/31/SPARK-BASIC-1/</link>
      <guid>https://emjayahn.github.io/2019/05/31/SPARK-BASIC-1/</guid>
      <pubDate>Fri, 31 May 2019 12:07:52 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Apache-Spark-Basic-1&quot;&gt;&lt;a href=&quot;#Apache-Spark-Basic-1&quot; class=&quot;headerlink&quot; title=&quot;Apache Spark Basic 1&quot;&gt;&lt;/a&gt;Apache Spark Basic 1&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;SPARK 를 공부하면서 실습 과정을 정리해서 남깁니다.&lt;/li&gt;
&lt;li&gt;실습환경&lt;ol&gt;
&lt;li&gt;CentOS&lt;/li&gt;
&lt;li&gt;Spark 2.4.3&lt;/li&gt;
&lt;li&gt;Hadoop 2.7&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Apache-Spark-Basic-1"><a href="#Apache-Spark-Basic-1" class="headerlink" title="Apache Spark Basic 1"></a>Apache Spark Basic 1</h1><ul><li>SPARK 를 공부하면서 실습 과정을 정리해서 남깁니다.</li><li>실습환경<ol><li>CentOS</li><li>Spark 2.4.3</li><li>Hadoop 2.7</li></ol></li></ul><a id="more"></a><h1 id="1-Spark-shell"><a href="#1-Spark-shell" class="headerlink" title="1. Spark-shell"></a>1. Spark-shell</h1><h2 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1-1. Introduction"></a>1-1. Introduction</h2><p>shell의 spark home directory 에서 다음 명령어를 통해 spark shell 을 진입할 수 있습니다.</p><pre><code>$ cd /spark_home_directory/$ ./bin/spark-shell</code></pre><p><img src="Untitled-d3bd9451-d568-49aa-bf5a-0f6b7bf598d9.png" alt></p><ul><li>sc : spark context</li><li>spark : spark session</li></ul><p>spark context 와 spark session 의 경우, spark shell에 띄우면서 내부적으로 선언된 변수명이다. </p><pre><code>$ jps// jps 명령어를 통해 현재 돌고 있는 spark process 를 확인할 수 있다. // spark processor 가 jvm 을 바탕으로 돌기 때문에, jvm 프로세스가 도는 것을 확인하므로써// 확인 할 수 있는 것이다.</code></pre><p><a href="http://localhost:4040" target="_blank" rel="noopener">http://localhost:4040</a>, 즉 해당 서버의 ip:4040 포트를 통해서 드라이버에서 제공되는 웹 UI 를 확인할 수 있다. 이 웹 UI 를 통해 현재 작동하는 프로세서와 클러스터들을 관리 할 수 있다.</p><h2 id="1-2-RDD"><a href="#1-2-RDD" class="headerlink" title="1-2. RDD"></a>1-2. RDD</h2><p>spark는 data를 처리할 때, RDD 와 Spark SQL 을 통해서 data object 를 생성하고 이를 바탕으로 다양한 pipeline 으로 동작 할 수 있다. RDD 를 처음 접해보는 실습.</p><pre><code>//scala shellval data = 1 to 10000val distData = sc.parallelize(data)distData.filter(_ &lt; 10).collect()</code></pre><ul><li><code>data</code> 가 RDD</li><li>sc.parallelize의 return 형 역시 parallelize 된 RDD, 즉 distData 도 RDD</li><li>마지막 command line 은 10보다 작은 data 에 대해 filtering 하고 각 executor 에서 실행된 자료를 collect()</li><li>spark 의 특징은 .collect() 와 같은 action api 가 실행될 때 모든 것이 실행되는 <strong>Lazy Evaluation (RDD)</strong>으로 동작한다.</li></ul><p>드라이버 웹 UI 를 통해 이를 확인 할 수 있다. 이전 command line 에서는 아무 동작도 일어나지 않다가 collect() action api 수행을 통해 실제로 command들이 수행되는 것을 확인 할 수 있다. local 에서 default 로 동작하기 때문에 2개의 partition 으로 동작하며, 어떤 shuffling 도 일어나지 않았기 때문에 1개의 stage 임을 확인 할 수 있다.</p><p><img src="Untitled-a49c2d30-d7d2-43bf-a2d0-7a54ff94d0c6.png" alt></p><pre><code>// scala shell// sc.textFile 을 통해 textfile, md 파일등을 읽어드릴 수 있다.val data = sc.textFile(&quot;file_name&quot;)// rdd 의 .map api 를 통해서 rdd 의 element 마다 val distData = data.map(r =&gt; r + &quot;_experiment!!&quot;)// 앞선 map 이 수행되고, 각 element(data) 갯수를 세개 된다.distData.count</code></pre><p>여기서는 .count 가 action api 이므로, .count 가 수행될 때, 앞선 command 들이 수행되게 된다.</p><p><code>sc.textFile()</code> 의 경우 ‘\n’, newline 을 기준으로 element 를 RDD 에 담게 된다.</p><p><code>RDD.toDebugString</code> 를 통해 해당 RDD 의 Lineage 를 확인 할 수 있다.</p><ul><li>가장 왼쪽에 있는 | 를 통해 stage 정보 역시 확인 할 수 있다. shuffle 이 일어나게 되면, stage가 바뀌므로, 서로 다른 stage  에 있는 command 의 경우, 다른 indent에 있게 된다.</li></ul><p><img src="Untitled-62b02b6f-4150-4dcf-8543-655381c951f7.png" alt></p><p><code>RDD.getNumPartitions</code> 를 통해 해당 RDD의 Partition 갯수 (=Task 의 갯수), 즉 병렬화 수준을 확인 할 수 있다.</p><p><img src="Untitled-a773c2aa-c715-4ccb-8b21-66eaaf1e22b3.png" alt></p><h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle!!"></a>Shuffle!!</h3><p>suffle 이 일어나는 경우는 api 마다 다양할 수 있다. 가장 기본적으로, 우리가 default partition 갯수를 변경하므로써 shuffle 이 일어나는 것을 확인 할 수 있다.</p><pre><code>val data = sc.textFile(&quot;file_name&quot;)data.getNumPartitions// Partition 의 숫자를 확인해보면, default 이므로 2 인 것을 확인 할 수 있다.val newData = data.repartition(10)newData.getNumPartitions// Partition 갯수가 10로 변경된 것을 확인 할 수 있다.newData.toDebugString// newData 의 Lineage 를 확인하면, repartition 이 일어나면서 shuffle 이 되고,// shuffle 로 인해 stage 가 2개가 되는 것을 확인 할 수 있다.(indentation)newData.count// action api 를 수행하여 앞선 command 를 모두 수행</code></pre><p><img src="Untitled-95d981bb-62a7-457f-9378-0b41db4d5b42.png" alt></p><p>위 command line 에 대한 DAG 를 웹 UI 를 통해 확인하면, 다음과 같이 stage 가 repartition을 기점으로 나누어 지는 것을 확인 할 수 있다.</p><p><img src="Untitled-ef367f62-d6f5-47da-9086-cdaa914aa5d7.png" alt></p><p>총 Partition의 갯수 (Task의 갯수)를 확인해 보면,  default 로 수행된 partition 2 개와, 우리가 설정해준 Partition 의 갯수인 10개를 합하여 12개인 것을 확인 할 수 있다.</p><p><img src="Untitled-6bf929e7-3f6d-4f2f-8898-04ca7ba404b6.png" alt></p><p>여기서 한 스텝을 더 들어가보면, spark 만의 특이한 특징을 확인 할 수 있다.</p><pre><code>// 위 코드에 이어서, newData 에 대해// newData RDD를 collect 해서 cli에 찍는 command 를 수행해보자.newData.collect.foreach(println)</code></pre><p>collect api 와 RDD의 element를 print 를 하는 action api 를 수행할 때, 지금까지 공부한 것으로 생각해 보면, text를 읽어서, 2개의 Partition 을 나누고, 다시 10개의 Partition 을 나누는 작업으로 이전의 12 개의 Task 와 다를게 없을 것 같은 느낌이다. 하지만 UI 를 통해 확인해보면, 10개의 Partition 으로 2개가 skipped 되었다고 확인할 수 있다. DAG 에서도, skipped 된 stage에 대해서 회색으로 확인된다.</p><p><img src="Untitled-6e461b14-8e34-4edb-91ad-6dd9ba2ba516.png" alt></p><p><img src="Untitled-85afaaf0-58d6-4533-995d-c5ec919da985.png" alt></p><p>이는 spark 에서 이 커맨드라인을 수행할 때, process 간 통신이 file 을 기반으로한 통신을 했기 때문이다. 제일 처음 <code>newData</code> 에 대해서 수행 될 때, 첫 stage 에서 shuffle 이 수행 될 때, 해당 파일을 각 executor 에서 shuffle write 을 하고 저장해두었다가, 두번째 stage 에서 shuffle  이 수행 될때, shuffle read 를 하는 방식으로 file을 기반으로 processor 가 통신하게 된다.</p><p><img src="Untitled-310cc7d4-4e31-4362-bf74-f0e046a27052.png" alt></p><p>따라서, spark 가 같은 command line 을 수행하게 되면 미리 shuffle write  된 file 을 읽기만 함으로써 앞선 stage 의 동일한 반복 작업을 수행하지 않게 되는 것이다. UI 를 확인 해보아도, shuffle read 만 수행 되었다. </p><p><img src="Untitled-e89e6c5f-89f8-475c-af4a-78c1e3ae8377.png" alt></p><h3 id="SaveFile"><a href="#SaveFile" class="headerlink" title="SaveFile!!!"></a>SaveFile!!!</h3><p><code>RDD.saveAsTextFile(&quot;directory_name&quot;)</code> api 를 활용하여, 어떤 처리가 끝난 RDD 를 저장할 수 있다. 이 때 주의 할 점은 parameter 에 들어 가는 것이 directory_name 이라는 것이다. 또한 partition 별로 파일이 저장된다. (e.g. 10개의 partition 이라면, 10개의 file이 저장된다.)</p><h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache!!!"></a>Cache!!!</h3><p>spark가 자랑하는 가장 큰 특징은, data(RDD) 를 memory에 cache  함으로써 처리의 속도가 매우 빠르다는 점이다. <code>RDD.cache</code> api 를 통해 memory 에 캐시할 수 있다. </p><pre><code>// distData RDD 에 이름을 부여distData.name = &quot;myData&quot;// cache!distData.cache// action : 5 개의 data 를 가져옴distData.take(5)// action : collectdistData.collect</code></pre><p><code>distData.take(5)</code> 까지 한 결과를 UI 에서 cache 를 살펴보면, 다음과 같다.</p><p><img src="Untitled-54196c66-b41c-4216-af72-5a70b030b321.png" alt></p><p>우리가 설정 한 것 처럼, RDD 의 이름이 myData 로 들어간것을 확인 할 수 있고 cache  역시 확인 할 수 있다. 하지만, Cached  된 비율을 확인하면 전체 RDD  에서 50% 만 된 것을 확인 할 수 있다. 반면에, <code>distData.collect</code> action 을 취하게 되면, Fraction Cached  가 100% 가 된 것을 확인 할 수 있다.</p><p><img src="Untitled-8f50bedc-ace7-4c09-9450-fb277ee5830b.png" alt></p><p>이는 우리의 action 에 따라 cache 할 용량이 달라 질 수 있기 때문이다. spark 입장에서 take(5) api 는 전체 RDD 중 5개의 element data 만 가져오면되고, 이 때 2개의 Partition 중 하나의 Partition 만 cache 해도 충분하기 때문에 Fraction Cached가 50%라고 나오는 것이다. 반면 collect api 는 collect 자체가 각 executor 에 있는 data 를 driver 로 모두 가져오는 것이므로 100% cache 하게 된다.</p><p><strong>Cache 에서 중요한 것은, 각 executor 의 cache 를 위한 가용 메모리 공간이 해당 Partition의 용량보다 작을 경우, 저장 할 수 있는 용량만큼 저장되는 것이 아니라, 해당 Partition 은 아예 저장이 안되게 된다. 이 점은 Cache를 할 때, Partition 의 용량과 해당 Executor 의 가용 메모리 공간을 미리 파악하여, 설계해야 한다.</strong></p><h3 id="Word-Count-예제"><a href="#Word-Count-예제" class="headerlink" title="Word Count 예제!!!"></a>Word Count 예제!!!</h3><p>우리가 데이터 분석을 할 때, 가장 basic 한 방법은 해당 데이터의 갯수를 세어 보는 것이다. 본 예제에서는 텍스트 파일을 읽어, 띄어쓰기를 바탕으로 word token을 나누고, 이를 세어보자.</p><p>WordCount 예제는 매우 basic 한 코드이므로, 어떤 로직으로 돌아가는지 완벽한 이해와 코드작성이 필수라고 생각한다.</p><pre><code>val originalDataRDD = sc.textFile(&quot;text-file&quot;)val wordcountRDD = originalDataRDD.flatMap(line =&gt; line.split(&quot; &quot;))                                        .map(word =&gt; (word, 1)).reduceByKey(_ + _)wordcountRDD.collect.foreach(println)</code></pre><ol><li>originalDataRDD 에서 text-file을 읽고,</li><li>line 마다 띄어쓰기를 기준으로 split 하고 이를 .flatMap 을 통해, flatten 하게 됩니다.</li><li>그리고 .map 을 통해 (word, 1) tuple 형태로 mapping 합니다.</li><li>.reduceByKey 를 통해 같은 word 에 대해 그 counting 갯수를 더하게 된 것을 RDD 로 return 하게 됩니다.</li></ol>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/05/31/SPARK-BASIC-1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture05-CNN</title>
      <link>https://emjayahn.github.io/2019/05/20/CS231n-Lecture05-Summary/</link>
      <guid>https://emjayahn.github.io/2019/05/20/CS231n-Lecture05-Summary/</guid>
      <pubDate>Mon, 20 May 2019 08:36:09 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-05-Convolutonal-Neural-Networks&quot;&gt;&lt;a href=&quot;#Lecture-05-Convolutonal-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Lecture 05: Convolutonal Neural Networks&quot;&gt;&lt;/a&gt;Lecture 05: Convolutonal Neural Networks&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-05-Convolutonal-Neural-Networks"><a href="#Lecture-05-Convolutonal-Neural-Networks" class="headerlink" title="Lecture 05: Convolutonal Neural Networks"></a>Lecture 05: Convolutonal Neural Networks</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><h2 id="1-Convolution-Layer"><a href="#1-Convolution-Layer" class="headerlink" title="1. Convolution Layer"></a>1. Convolution Layer</h2><h3 id="1-1-Fully-Connected-Layer-와-Convolution-Layer-의-비교"><a href="#1-1-Fully-Connected-Layer-와-Convolution-Layer-의-비교" class="headerlink" title="1-1. Fully Connected Layer 와 Convolution Layer 의 비교"></a>1-1. Fully Connected Layer 와 Convolution Layer 의 비교</h3><p>32 x 32 x 3 image 가 있다고 하자. network 에 주입하기 위해, 1 x 3072 로 핀 data 를 상상해 보자.</p><p>Fully Connected Layer 의 경우, 아래 그림 처럼, Weight 과 dot product 가 수행되어, activation 값이 나오게 된다. 이 때, activation 의 갯수는 W 의 크기에 따른다.</p><p><img src="Untitled-c7b225ec-cc03-4a1b-900f-558beebc21b5.png" alt></p><p>Fully Connected Layer 의 경우, 사진이라는 공간적 구조(Spatial Structure) 가 중요한 data 에 대해, 공간적인 정보를 다 잃어버리는 문제가 발생한다. 공간적 정보를 보존하기 위해 Convolution Layer 를 사용한다. </p><h3 id="1-2-Convolution-Layer-Overview"><a href="#1-2-Convolution-Layer-Overview" class="headerlink" title="1-2. Convolution Layer Overview"></a>1-2. Convolution Layer Overview</h3><p><img src="Untitled-7729c470-9f6b-4d19-9656-cc46c1dfb812.png" alt></p><p>이 때, 실제 convolution 계산은 image 의 filter 크기 만큼의 matrix 를 vectorize 한 후, filter vector 와 dot product 로 수행한다고 한다. 따라서 이 때 헷갈리지 않도록 할 것은, 한번의 Convolution 연산 결과는 <strong>하나의 scalar value</strong> 가 된다. 따라서, 하나의 filter 가 이미지 한 장을 훑어 내려간다면, 원본 이미지보다는 조금 작은 depth 가 1인 <strong>activation map</strong> 이 결과로 나온다.</p><p><img src="Untitled-fce64257-258e-4b3e-93ab-d0543662984f.png" alt></p><p>따라서 activation map 의 깊이(channel 수)는, <strong>filter 의 갯수</strong> 와 동일하다. 여러개의 필터는 이미지의 각기 다른 특징을 추출하려는 의도에서 사용된다.</p><p>그 후, 이 Convolution 의 결과인 activation map 을 비선형 함수(ReLU 등)에 통과 시킨다.</p><p><strong>여러 유명한 ConvNet 들은 이렇게, Convolution Layer 와 비선형함수를 반복적으로 나열 한 Network  라고 볼수 있다</strong></p><h3 id="1-3-Convolution-Layer-의-결과물"><a href="#1-3-Convolution-Layer-의-결과물" class="headerlink" title="1-3. Convolution Layer 의 결과물"></a>1-3. Convolution Layer 의 결과물</h3><p>이렇게 여러 계층의 Convolution Layer를 쌓는 것은, 가장 아래 Layer 부터 높은 Layer 까지 단순한 feature → 복잡한 feature 를 뽑아 내는 것으로 볼 수 있다.</p><p><img src="Untitled-5fec1f58-a947-449b-8d71-772aac9a6f3e.png" alt></p><h3 id="1-4-Convolution-Layer-연산"><a href="#1-4-Convolution-Layer-연산" class="headerlink" title="1-4. Convolution Layer 연산"></a>1-4. Convolution Layer 연산</h3><p><img src="Untitled-2a8adbad-1f2a-4fac-898d-aac944438a1b.png" alt></p><p>filter 가 이미지를 훑고 지나가면서 convolution 연산을 한 후, 나온 결과는 원본 이미지보다 그 크기가 작아지게 된다. 그 정도는 filter 의 크기와 filter 가 훑고 지나가는 간격인 stride 에 따라 바뀌게 된다. </p><p>$$output ;size = (N-F)/stride + 1$$</p><p><strong>문제점:</strong></p><ol><li><p>convolution 연산의 문제는 이미지의 모서리에 있는 정보는 가운데에 있는 이미지의 정보보다 적게 추출 되는 문제가 있다.(filter 가 모서리를 넘어서는 이동 할 수 없으므로)</p></li><li><p>convolution layer 를 반복적으로 지나가다 보면, map의 크기가 매우 빠르게 작아지게 된다. </p></li></ol><p>이를 위해 적용하는 것이 Padding 이다.</p><h3 id="1-5-Padding"><a href="#1-5-Padding" class="headerlink" title="1-5. Padding"></a>1-5. Padding</h3><p>모서리에 정보를 얻기 위해 이미지이 외곽에 숫자를 채워 주는 방법. 이 때, 많이 사용하는 방법은 zero-padding. zero-padding 외에도 다양한 방법이 있다.</p><h2 id="2-Pooling-Layer"><a href="#2-Pooling-Layer" class="headerlink" title="2. Pooling Layer"></a>2. Pooling Layer</h2><p>Parameter 의 갯수를 줄이기 위해, 우리가 ConvLayer 를 통해 뽑아낸 image 를 작게 만드는 Layer  이다. 즉, Downsampling 을 위한 것.</p><p>Maxpooling 의 intuition : 앞선 layer filter 가 각 region  에서 얼마나 활성 되었는지 보는 것이다. </p><h3 id="3-Typical-Architecture"><a href="#3-Typical-Architecture" class="headerlink" title="3. Typical Architecture"></a>3. Typical Architecture</h3><p>[[(Conv → RELU) * N → Pool] * M → (FC → RELU) * K ] → SOFTMAX</p><ul><li>N : ~ 5</li><li>M : Large</li><li>K : 0 ~ 2<ul><li>ResNet, Google net 등은 이 방식을 훨씬더 뛰어넘음</li></ul></li></ul><h2 id="4-Reference"><a href="#4-Reference" class="headerlink" title="4. Reference"></a>4. Reference</h2><p><a href="https://www.youtube.com/watch?v=bNb2fEVKeEo" target="_blank" rel="noopener">Lecture 5 | Convolutinoal Neural Networks</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/05/20/CS231n-Lecture05-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture04-Backprop/NeuralNetworks</title>
      <link>https://emjayahn.github.io/2019/05/18/CS231n-Lecture04-Summary/</link>
      <guid>https://emjayahn.github.io/2019/05/18/CS231n-Lecture04-Summary/</guid>
      <pubDate>Sat, 18 May 2019 14:21:08 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-04-Backpropagation-and-Neural-Networks&quot;&gt;&lt;a href=&quot;#Lecture-04-Backpropagation-and-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Lecture 04: Backpropagation and Neural Networks&quot;&gt;&lt;/a&gt;Lecture 04: Backpropagation and Neural Networks&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-04-Backpropagation-and-Neural-Networks"><a href="#Lecture-04-Backpropagation-and-Neural-Networks" class="headerlink" title="Lecture 04: Backpropagation and Neural Networks"></a>Lecture 04: Backpropagation and Neural Networks</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><h2 id="1-Backpropagation"><a href="#1-Backpropagation" class="headerlink" title="1. Backpropagation"></a>1. Backpropagation</h2><h3 id="1-1-핵심"><a href="#1-1-핵심" class="headerlink" title="1-1. 핵심"></a>1-1. 핵심</h3><p>Backprop의 한줄요약: 각 parameter 에 대해 Loss Function 의 gradient 를 구하기 위해 사용하는 graphical representation of <strong>ChainRule</strong></p><p>언뜻 보면 어려울 수 도 있지만, just ChainRule</p><p><img src="Untitled-13d44613-a85b-4dcc-a807-8b114b0138c4.png" alt></p><p>위의 그림을 예를 들어 살펴 보면, y 에 대한 f의 gradient 는 q에 대한 f의 gradient * y 에 대한 q 의 gradient 으로 볼 수 있다. 이를 해석하자면, f 에게 미치는 y 의 영향 = f 에게 미치는 q의 영향 * q 에게 미치는 y 의 영향으로 볼 수 있다.</p><p>Backpropagation 의 가장 중요한 특징!!</p><pre><code>gradient 를 구하기 위해 node 를 기준으로 앞과 뒤만 보면 된다.</code></pre><p><img src="Untitled-f2d8c1d9-1a28-4e16-8e35-83ff9f93045f.png" alt></p><p>또한, 위 그림을 보게 되면, Forward passing 과 마찬가지로, Backprop 시에도, 이전 노드에서 전달 되는 Gradient 를 node 에서 local gradient 와의 연산으로 다음 Node 에 gradient 를 전달해 줄 수 있다.</p><h3 id="1-2-Back-prop-시-각-node-의-역할"><a href="#1-2-Back-prop-시-각-node-의-역할" class="headerlink" title="1-2. Back prop 시, 각 node  의 역할"></a>1-2. Back prop 시, 각 node  의 역할</h3><p><img src="Untitled-7a11e194-ea07-4922-a7b5-aa4346476acb.png" alt></p><ol><li><p>Add gate : gradient distributor</p><ul><li>add gate 를 기점으로, 각 입력(forward 방향의 입력)의 gradient로 local gradient 를 구하면 1이므로, 전달 되는 gradient 와 각각 1씩 곱해져 전달 되게 된다. 이 현상을 보게 되면 동등하게 나눠주는 역할을 하므로 gradient distributor 라고 볼 수 있다.</li></ul></li><li><p>Max gate : gradient router</p><ul><li><p>Max gate 는 gradient 를 한쪽에는 전체, 다른 쪽에는 0 을 준다.</p></li><li><p>해석적으로 보자면, max 연산을 통해 forward 방향에서 영향을 준 branch 에게 gradient 를 전달해 주는 것이 합적</p><p>$$max(x, y) = \begin{cases} x \quad\quad if \quad x &gt; y \\ y \quad\quad if \quad x &lt; y \end{cases}$$</p></li><li><p>수식으로 보자면, x 에 대한 gradient, y 에 대한 gradient 가 각각 (1, 0), (0, 1)  로 local gradient 가 계산되기 때문이다.</p></li><li><p>gradient 가 전달될 길을 결정해주는 면에서, 네트워크에서 path 를 설정해 주는 router의 기능과 비슷하다.</p></li></ul></li><li><p>Mul gate : gradient switcher + scaler</p><ul><li>곱셈연산의 gradient 의 경우, x 에 대한 gradient 는 y 가 되므로, 서로 바꿔주는 역할을 한다. 이 때, forward 상에서의 결과 값으로 곱해주므로, scale 역할까지 함께 하게 된다.</li></ul></li></ol><h2 id="2-Neural-Networks"><a href="#2-Neural-Networks" class="headerlink" title="2. Neural Networks"></a>2. Neural Networks</h2><p>강의에서는 Neural Network 에 대한 intuition 을 위해, biological neuron 과 비교하였다. 모델 architecture 로서의 neuron 과 biological neuron 의 공통점은 다음과 같다.</p><ol><li>input impulse</li><li>input axon → dendrite</li><li>(cell body)activation &amp; activation function</li><li>output axon</li></ol><p>이러한 비교는, 나의 개인적인 Neural Network에 대한 공부와 이해에 도움이 되지 않기에 큰 감동은 없다. </p><ul><li>4강에 대한 핵심 사항은, Backpropagation 에 대한 수식적 이해와 그 이해를 통해 Backpropagation 이 gradient를 구함에 있어, 얼마나 편한 representation 인지이다. 공부하며 어려웠던 것은, backprop in <strong>vectorized</strong> 에서, Jacobian Matrix 의 표현이 scalar  backprop 때와는 달리 한번에 머릿속으로 상상되지 않았기에, 손으로 써가며 따라 갔어야만 했다.</li></ul><h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><p><a href="https://www.youtube.com/watch?v=d14TUNcbn1k" target="_blank" rel="noopener">Lecture 4 | Introduction to Neural Networks</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/05/18/CS231n-Lecture04-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture03-LossFunction/Optimization</title>
      <link>https://emjayahn.github.io/2019/05/16/CS231n-Lecture03-Summary/</link>
      <guid>https://emjayahn.github.io/2019/05/16/CS231n-Lecture03-Summary/</guid>
      <pubDate>Thu, 16 May 2019 01:17:11 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-03-Loss-Function-amp-Optimization&quot;&gt;&lt;a href=&quot;#Lecture-03-Loss-Function-amp-Optimization&quot; class=&quot;headerlink&quot; title=&quot;Lecture 03: Loss Function &amp;amp; Optimization&quot;&gt;&lt;/a&gt;Lecture 03: Loss Function &amp;amp; Optimization&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-03-Loss-Function-amp-Optimization"><a href="#Lecture-03-Loss-Function-amp-Optimization" class="headerlink" title="Lecture 03: Loss Function &amp; Optimization"></a>Lecture 03: Loss Function &amp; Optimization</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ol><li>Loss Function : 우리가 가지고 있는 W matrix 가 <strong>얼마나 안좋은지 정량화(Quantify)</strong> </li><li>Optimization : 위의 Loss Function 을 minimize 해서 가장 좋은 parameter (W) 를 찾는 과정</li></ol><h2 id="2-Loss-Function"><a href="#2-Loss-Function" class="headerlink" title="2. Loss Function"></a>2. Loss Function</h2><p>주어진 data 가 다음과 같을 때, </p><p>$${(x_i, y_i)}_{i=1}^{N}$$</p><p> Loss 는 “Average of over examples” 즉, </p><p>$$L = \frac{1}{N}\sum_{i}L_i(f(x_i, W), y_i)$$</p><ul><li>딥러닝 알고리즘의 General Setup<ul><li>W 가 얼마나 좋고, 나쁜지를 정량화하는 손실함수 만들기</li><li>W 공간을 탐색하면서  이 loss를 minimize 하는 W 를 찾기</li></ul></li></ul><h3 id="2-1-Loss-Example-Multiclass-SVM-Loss"><a href="#2-1-Loss-Example-Multiclass-SVM-Loss" class="headerlink" title="2-1. Loss Example: Multiclass SVM Loss"></a>2-1. Loss Example: Multiclass SVM Loss</h3><p>SVM Loss 는 다음과 같다. 주어진 data example (x_i, y_i) 에 대해서, score vector <strong>s</strong> 는 다음과 같다. </p><p>$$s = f(x_i, W)$$</p><p>이 때, SVM loss는 </p><p>$$L_i = \sum_{j\neq y_i}\begin{cases} 0 \quad\quad\quad\quad\quad\quad\quad if ;s_{y_i} \geq s_j +1 \ s_j - s_{y_i} + 1 \quad\quad otherwise \end{cases} \ = \sum_{j\neq y_i}max(0, s_j-s_{y_i}+ 1)$$</p><p>x_i 의 정답이 아닌 클래스의 score (s_j) + 1 (safety margin) 과 정답 클래스 score s_yi 를 비교하여, Loss 를 계산한다.</p><p><img src="Untitled-d804129d-082d-4cfc-819c-4fd3e14ddf17.png" alt></p><ol><li>SVM Loss 의 최대, 최솟값은 ? min : 0, max :  infinite</li><li>W 를 작게 초기화 하면, s 가 거의 0에 가까워 진다. 이 때, SVM Loss 는 어떻게 예상되는가?<ul><li>정답이 아닌 class, 즉 class - 1 개의 score 원소들을 순회하면서 모두 더할 때, score 는 0에 가깝고, 이를 average 취하면 <strong>class 갯수 - 1</strong> 만큼의 Loss 값이 나온다.</li><li>이 특징은 debugging strategy 로 사용할 수 있다. 초기 loss 가 C-1 에 가깝지 않으면 bug 가 있는 것으로 의심해볼 수 있다.</li></ul></li><li>만약 include j = y_i 이면, SVM Loss 는 어떻게 되는가? <ul><li>Loss Funtion 이 바뀌는 것은 아니다. 단지 전체 loss의 minimum 이 1이 될 뿐이므로 해석의 관점에서 관례상 맞지 않아 정답 class 는 빼고 계산한다.</li></ul></li><li>우리가 average 를 취하지 않으면?<ul><li>이 역시 바뀌는 것이 없다. 전체 class 수는 정해져 있고, 이를 나누는 average 는 scaling 만 할 뿐이다.</li></ul></li><li>Loss 를 max(0, s_j - s_yi + 1) ^2 를 사용하면?<ul><li>이는 squared hinge function 으로 때에 따라서 사용할 수 있는 loss function 이다. 다른 Loss function 이며, 이는 위의 loss 와 다르게 해석 할 수 있다. 기존의 SVM loss 는 class score 가 각각 얼마나 차이가 나는지에 대해서는 고려하지 않는 것이라고 한다면, squared 가 들어감으로써, 차이가 많이 나는 score class 에 대해서는 좀더 가중하여 고려하겠다는 의미로 해석 할 수 있다.</li></ul></li></ol><h3 id="2-2-Regularization"><a href="#2-2-Regularization" class="headerlink" title="2-2. Regularization"></a>2-2. Regularization</h3><p>만약 위 Loss Function 에 대해서, L = 0 으로 만드는 W 를 찾았다고 할때, 과연 이 W 는 유일한가? 그렇지 않다. W 일 때, L=0  이라면, 2W 역시 L=0 이다. 또한 L을 0으로 만드는 다양한 W 중에서 단지  training  data 에만 fit 하는 classifier 를 원하는 것이 아니라, test data에서 좋은 성능을 발휘하는 classifier 를 찾기를 원한다. 이런 Overfitting 을 막기 위해서는 모델의 W 를 다른 의미에서 조절해줄 수 있는 <strong>Regularization</strong> term 을 추가할 수 있다.</p><p><strong>즉, Model이 training set 에 완벽하게  fit 하지 못하도록 Model 의 복잡도에 penalty 를 부여하는 것을 말한다.</strong></p><p><img src="Untitled-253fa150-bbe6-473f-bd43-e3fd6fe0009d.png" alt></p><p>Regularization 의 종류들:</p><ul><li>L2 Regularization</li><li>L1 Regularization</li><li>Elastic net(L1 + L2)</li><li>Max norm Regularization</li><li>Dropout</li><li>Batch normalization, stochastic depth</li></ul><h3 id="2-3-Loss-example-Softmax-Classifier-Multinomial-Logistic-Regression"><a href="#2-3-Loss-example-Softmax-Classifier-Multinomial-Logistic-Regression" class="headerlink" title="2-3 Loss example: Softmax Classifier (Multinomial Logistic Regression)"></a>2-3 Loss example: Softmax Classifier (Multinomial Logistic Regression)</h3><p>deeplearning 에서 훨씬 더 자주 보게 되는 loss 의 종류 중 하나이다.  위에서 살펴본 SVM loss 의 단점은 그 값 자체에 어떤 의미를 부여하기는 힘들다는 점이다. 반면에, Softmax Classifier 는 그 값 자체를 확률적 해석이 가능하기 때문이다. (cf. 콜모고로프의 공리를 통해 softmax 의 layer 의 output 이 확률로 해석 될 수 있다.)</p><p>Softmax Function 은 다음과 같다.</p><p>$$P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}},\quad where \quad s = f(x_i;W)$$</p><h2 id="3-Opitmization"><a href="#3-Opitmization" class="headerlink" title="3. Opitmization"></a>3. Opitmization</h2><p>Optimization 을 한마디로 요약하자면, <strong>우리의  loss 를 최소화 하는 W 를 찾기</strong> 가 되겠다. 그 방법에는,</p><ul><li>(바보 같은 접근인: 강의표현) Random Search</li><li>Gradient 를 구하는 방법<ul><li>Numerical Gradient 수치적 접근 : 이 방법은 근사치를 구하는 것이며, 매우 느린 단점이 있다. 하지만, 쉽게 작성할 수 있다는 장점이 있다.</li><li>Analytic Gradient 해석적 접근 : 미분식을 구해야하는 단점이 있다. 하지만 빠르고 정확하다.</li></ul></li></ul><p>실제로는, Analytic Gradient 방법을 사용한다. 하지만 debugging 을 위해 numerical gradient 를 사용한다. 이를 <strong>gradient check</strong>이라 한다.</p><h3 id="3-1-Gradient-Descent-amp-Stochastic-Gradient-Descent"><a href="#3-1-Gradient-Descent-amp-Stochastic-Gradient-Descent" class="headerlink" title="3-1. Gradient Descent &amp; Stochastic Gradient Descent"></a>3-1. Gradient Descent &amp; Stochastic Gradient Descent</h3><p>Gradient Descent 를 방법을 이용해서 optimization 을 진행할 수 있다. 하지만 데이터의 숫자와 차원이 매우 큰 경우, parameter (W) 를 update 하는데 그 연산량이 매우 큰 단점과 위험이 있다. 이를 해결하기 위해 minibatch 를 사용하여 확률적 접근을 사용한다.</p><h2 id="4-Image-Feature-Extraction"><a href="#4-Image-Feature-Extraction" class="headerlink" title="4. Image Feature Extraction"></a>4. Image Feature Extraction</h2><p>CNN 등이 등장하기 전에 Image 에서  Feature 를 뽑아내는 방법에 대해 소개한다. Feature를 뽑아내는 개념으로 생각할 수 도 있지만, Feature Transform 이라는 표현을 사용한다. </p><ol><li>Color Histogram :  이미지의 color distribution 을 사용하여 해당 이미지의 feature 로 사용할 수 있다. (출처: <a href="https://en.wikipedia.org/wiki/Color_histogram" target="_blank" rel="noopener">wikipedia</a> ) For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image’s color space, the set of all possible colors.</li><li>Histogram of Oriented Gradients (HoG) : CNN 이 등장하기 전, 매우 인기있는 Image Feature 중 하나라고 알고 있다. Edge 를 검출하는 방법이다. pixel 사이에, 값의 gradient 가 가장 큰 neighbor 가 edge 일 것이다라는 개념을 사용하여 edge 를 검출한다. 사진을 8 x 8 patch 를 만들어, 각 patch 마다 9 directional oriented gradients 를 계산하여, 이를 feature 로 사용하는 방법이다.</li><li>Bag of Words : NLP 에서도 자주 사용되는 개념인 BoW 에서 차용한 개념으로, 이미지 데이터들에서 일정 크기의 patch 를 모아 clustering 을 통해 visual words (codebook)  을 만든다. 그리고 feature 뽑아내고 싶은 image 를 patch 형태로 바꾸고, codebook 에서 찾아 histogram 을 만들어 이를 feature 로 사용한다.</li></ol><h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p><a href="https://www.youtube.com/watch?v=h7iBpEHGVNc" target="_blank" rel="noopener">Lecture 3 | Loss Functions and Optimization</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/05/16/CS231n-Lecture03-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[CS231n]Lecture02-Image Classification Pipeline</title>
      <link>https://emjayahn.github.io/2019/05/13/CS231n-Lecture02-Summary/</link>
      <guid>https://emjayahn.github.io/2019/05/13/CS231n-Lecture02-Summary/</guid>
      <pubDate>Mon, 13 May 2019 03:46:04 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Lecture-02-Image-Classification-Pipeline&quot;&gt;&lt;a href=&quot;#Lecture-02-Image-Classification-Pipeline&quot; class=&quot;headerlink&quot; title=&quot;Lecture 02: Image Classification Pipeline&quot;&gt;&lt;/a&gt;Lecture 02: Image Classification Pipeline&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lecture-02-Image-Classification-Pipeline"><a href="#Lecture-02-Image-Classification-Pipeline" class="headerlink" title="Lecture 02: Image Classification Pipeline"></a>Lecture 02: Image Classification Pipeline</h1><ul><li>이 글은, Standford University 의 CS231n 강의를 듣고 스스로 정리 목적을 위해 적은 글입니다.</li></ul><a id="more"></a><h2 id="1-Image-Classification-의-기본-TASK"><a href="#1-Image-Classification-의-기본-TASK" class="headerlink" title="1. Image Classification 의 기본 TASK"></a>1. Image Classification 의 기본 TASK</h2><p><img src="Untitled-dfb244ee-00d4-4801-9db3-2247269d65db.png" alt></p><ul><li>위 사진을 보고, → ‘CAT’ 혹은 ‘고양이’ 라고 classification</li><li>자연스럽게 따라오는 문제는 <strong>“Sementic Gap”</strong> : 우리가 준 data (pixel 값  [0, 255]) 와 Label 간의  gap</li><li>또한, 이 과정에서 극복해야 하는 Challenges<ul><li>Viewpoint Variation ( 같은 객체에 대해 시점이 이동해도 robust)</li><li>Illumination ( 빛, 밝기, 명암 등에도 robust)</li><li>Deformation ( 다양한 Position, 형태의 변형에도 robust)</li><li>Occlusion ( 다른 물체나 환경에 의해 가려지는 data 에도 robust)</li><li>Background Clutter ( 배경과 비슷하게 보이는 객체에도 robust)</li><li>Intraclass Variation ( 한 종류의 클랫스에도 다양한 색과 모습의 객체가 있을 수 있다.)</li></ul></li></ul><h2 id="2-기존의-시도와-New-Era"><a href="#2-기존의-시도와-New-Era" class="headerlink" title="2. 기존의 시도와 New Era"></a>2. 기존의 시도와 New Era</h2><ul><li>Hard Coded Algorithm 과 여러 규칙 (rule-based로 해석된다) 들을 통해서 Image를 Classify 하는 노력들이 있어왔다. 이들의 문제는, (1) 위에 언급한 문제들이 Robust  하지 않다. (2) 객체가 달라지면, (고양이, 호랑이, 비행기 등) 객체마다 다 다른 규칙을 성립해줘야 한다. 즉 한마디로 요약하자면, Algorithm의 확장성이 없다.</li><li>이런 문제에 좀더 강한 방법이 지금 우리가 공부하고 있는, <strong>Data-Driven Approach</strong><ol><li>Image 와 Label pair 의 dataset 을 모은다.</li><li>Machine Learning 알고리즘을 이용해 classifier 를 학습시킨다.</li><li>Classifier 를 new images 에 테스트에 평가한다.</li></ol></li></ul><h2 id="3-First-Classifier-Nearest-Neighbor"><a href="#3-First-Classifier-Nearest-Neighbor" class="headerlink" title="3. First Classifier : Nearest Neighbor"></a>3. First Classifier : Nearest Neighbor</h2><h3 id="3-1-Nearest-Neighbor-의-기본-알고리즘"><a href="#3-1-Nearest-Neighbor-의-기본-알고리즘" class="headerlink" title="3-1. Nearest Neighbor 의 기본 알고리즘"></a>3-1. Nearest Neighbor 의 기본 알고리즘</h3><ol><li>train set 에서의 모든 data 와 label 을 기억한다.</li><li>test Image 와 <strong>가장 가까운</strong> train Image 의 label로 test image를 predict 한다.<ul><li>이 때 ‘<strong>가장 가까운’</strong> 을 계산 할 때<strong>, L1 distance</strong> 와 <strong>L2 distance</strong> 가 쓰일 수 있다. 이 외에도, 다양한 distance 지표가 쓰일 수 있다.</li></ul></li></ol><h3 id="3-2-Nearest-Neighbor-Classifier-의-문제"><a href="#3-2-Nearest-Neighbor-Classifier-의-문제" class="headerlink" title="3-2. Nearest Neighbor Classifier 의 문제"></a>3-2. Nearest Neighbor Classifier 의 문제</h3><ul><li>이미지 classification 에는 잘 사용되지 않는다.</li><li>train 보다 predict 하는데 훨씬 오래 걸린다.<ul><li>train 은 train data set 의 기억만 하면 되지만, predict 할 때는 전체 train data 에 대해 거리를 측정해야하고, sorting 해야하는 문제가 발생한다.</li><li>Time Complexity - train O(1), predict O(N) (N은 train data 수)</li></ul></li></ul><p><img src="Untitled-2fdb78f0-11b7-49cf-99f8-164d6b79fc2c.png" alt></p><p>위 그림을 보면, 연두색 공간에 노란색  class 가 포함 되어 있는 것을 볼 수 있다. 이는 generalize 면에서 부족한 모델이라고 볼 수 있다. 같은 알고리즘 이지만, 이를 해결 하는 방법은 K 개의 가까운 neighbor 로 부터 majority voting 을 받은 것으로 classify 를 하는 것이다.</p><h3 id="3-3-K-Nearest-Neightbors"><a href="#3-3-K-Nearest-Neightbors" class="headerlink" title="3-3. K-Nearest Neightbors"></a>3-3. K-Nearest Neightbors</h3><p>Single Nearest 만 보는 것이 아니라, K 개의 가까운 point 의 투표를 통해 해당 test data  의 label 을 예측한다.</p><ul><li>이 때, Voting 하는 방법에는 majority voting ( 다수결 ) 과 weighted voting ( 가중치를 주어 투표: distance 가 가까운 것에 가중치를 준다.)</li><li>가중치를 주는 방법에는 distance 가 커지면 곱해지는 weight 을 줄이는 방법으로 1 / (1+distance) 등을 weight 을 곱해준다.</li></ul><p><img src="Untitled-469c81dd-01a3-4934-8fec-429c34285d0e.png" alt></p><h3 id="3-4-k-Nearest-Neighbor-on-images-NEVER-USED"><a href="#3-4-k-Nearest-Neighbor-on-images-NEVER-USED" class="headerlink" title="3-4. k-Nearest Neighbor on images NEVER USED"></a>3-4. k-Nearest Neighbor on images NEVER USED</h3><ul><li>차원의 저주 문제</li><li>knn 이 잘 동작하기 위해서는 dataset 공간을 조밀하게 커버할 만큼의 충분한 training space 가 필요하다. 하지만, data 의 차원이 늘어날 수록 그 충분한 data 의 수가 exponential 하게 늘어난다.</li></ul><h2 id="4-Setting-Hyperparameters"><a href="#4-Setting-Hyperparameters" class="headerlink" title="4. Setting Hyperparameters"></a>4. Setting Hyperparameters</h2><p>Model 최적의 hyperparameter 를 찾기 위해서는 data set 을 구분하여, unseened data 를 사용하여 성능 검증을 하고, model selection 을 해야한다. 이는 단순이 hyperparameter 를 찾는 용도 뿐만 아니라 우리가 세운 가설을 서로 비교 할 때는 data set 을 정확히 구분하고, test set 을 통해 비교하고, 선택해야한다. 그 방법에는 train, validation, test set 으로 dataset 을 나누는 방법과 cross validation 방법이 있다.</p><ul><li>첫 번째 방법으로는, Validation set 을 통해 hyperparameter(가설)를 검증하고 선택하여, Test set 을 사용하여 Evaluate 과 Reporting 등을 한다. 딥러닝 모델링에서는 이 방법으로 많이 사용한다.</li></ul><p><img src="Untitled-7d99fc52-7613-4bbf-8cfc-5e036f905602.png" alt></p><ul><li>두 번째 방법은, data set 의 크기가 크지 않을 때, Train set 안에서 folds 들을 나누어 각 fold 가 돌아가며 validation set 이 되며, 이들의 평균값으로 가설을 비교한다. 이는 딥러닝 모델에서는 적합하지 않은 형태이다. 모델 자체의 연산이 많은데다가, 같은 모델에 대해 많은 validation 이 효율적이지 않기 때문이다. 또한 data가 많지 않은 상태에서 딥러닝 모델을 선택하는 것은 옳지 않다.</li></ul><p><img src="Untitled-1e7d1cc8-fcee-4215-a721-345c8f25e8f5.png" alt></p><h2 id="5-Second-Classifier-Linear-Classifier"><a href="#5-Second-Classifier-Linear-Classifier" class="headerlink" title="5. Second Classifier : Linear Classifier"></a>5. Second Classifier : Linear Classifier</h2><p>Linear Classifier 는 Neural Network 의 기본 골격이다. (1) image data 와 W (parameters or weights) 을 통해 연산을 해주고, (2) function 을 통과해 Classification 을 해준다.</p><p>특히 Linear Classifier 의 경우 아래 와 같이, (1) image data 와 W 를 dot product 를 해주고 (2) linear function f 를 통과한다.</p><p>$$f(x, W) = Wx$$</p><p><img src="Untitled-c15a20fd-b906-44ce-af25-6810bc57751a.png" alt></p><h2 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h2><p><a href="https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2" target="_blank" rel="noopener">Lecture 2 | Image Classification</a></p><p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">Syllabus | CS 231N</a></p>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/05/13/CS231n-Lecture02-Summary/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Basic Classification with Pytorch</title>
      <link>https://emjayahn.github.io/2019/05/06/Basic-Classification-with-Pytorch/</link>
      <guid>https://emjayahn.github.io/2019/05/06/Basic-Classification-with-Pytorch/</guid>
      <pubDate>Mon, 06 May 2019 03:39:33 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;Basic-Classification-with-Pytorch&quot;&gt;&lt;a href=&quot;#Basic-Classification-with-Pytorch&quot; class=&quot;headerlink&quot; title=&quot;Basic Classification with Pytorch&quot;&gt;&lt;/a&gt;Basic Classification with Pytorch&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이번 post 는 pytorch 를 활용해 기초적인 분류 모델링을 해보면서, pytorch에 익숙함을 높이는 것이 목적입니다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Basic-Classification-with-Pytorch"><a href="#Basic-Classification-with-Pytorch" class="headerlink" title="Basic Classification with Pytorch"></a>Basic Classification with Pytorch</h1><ul><li>이번 post 는 pytorch 를 활용해 기초적인 분류 모델링을 해보면서, pytorch에 익숙함을 높이는 것이 목적입니다.</li></ul><a id="more"></a><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn</span><br><span class="line"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F</span><br><span class="line"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="hljs-string">'retina'</span></span><br></pre></td></tr></table></figure><h2 id="1-Binary-Classification"><a href="#1-Binary-Classification" class="headerlink" title="1. Binary Classification"></a>1. Binary Classification</h2><ol><li>Modeling</li><li>Sigmoid</li><li>Loss : Binary Cross Entropy</li></ol><h3 id="1-1-Generate-Data"><a href="#1-1-Generate-Data" class="headerlink" title="1.1 Generate Data"></a>1.1 Generate Data</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># plotting function</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_scatter</span><span class="hljs-params">(W_, xy, labels)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">for</span> k, color <span class="hljs-keyword">in</span> [(<span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">'r'</span>)]:</span><br><span class="line">        idx = labels.flatten() == k</span><br><span class="line">        plt.scatter(xy[idx, <span class="hljs-number">0</span>], xy[idx, <span class="hljs-number">1</span>], c=color)</span><br><span class="line">        </span><br><span class="line">    x1 = np.linspace(<span class="hljs-number">-0.1</span>, <span class="hljs-number">1.1</span>)</span><br><span class="line">    x2 = -W_[<span class="hljs-number">1</span>] / W_[<span class="hljs-number">2</span>] * x1 - W_[<span class="hljs-number">0</span>] / W_[<span class="hljs-number">2</span>]</span><br><span class="line">    plt.plot(x1, x2, <span class="hljs-string">'--k'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Generate data</span></span><br><span class="line"></span><br><span class="line">W = np.array([<span class="hljs-number">-4.</span>/<span class="hljs-number">5</span>, <span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">1.0</span>])</span><br><span class="line"></span><br><span class="line">xy = np.random.rand(<span class="hljs-number">30</span>, <span class="hljs-number">2</span>)</span><br><span class="line">labels = np.zeros(len(xy))</span><br><span class="line">labels[W[<span class="hljs-number">0</span>] + W[<span class="hljs-number">1</span>] * xy[:, <span class="hljs-number">0</span>] + W[<span class="hljs-number">2</span>] * xy[:, <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_scatter(W, xy, labels)</span><br></pre></td></tr></table></figure><p><img src="output_6_0.png" alt="png"></p><h3 id="1-2-Train-data"><a href="#1-2-Train-data" class="headerlink" title="1.2 Train data"></a>1.2 Train data</h3><ul><li>Generate 한 data 로 부터, x 축 값, y 축 값, augmented term 으로 3가지 column 을 만들어 train data 로 만들어 줍니다.</li><li>또한 대응 되는 label 도 model 에 적합한 모양으로 바꾸어 줍니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[<span class="hljs-number">1.0</span>, xval, yval] <span class="hljs-keyword">for</span> xval, yval <span class="hljs-keyword">in</span> xy])</span><br><span class="line">y_train = torch.FloatTensor(labels).view(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">print(x_train[:<span class="hljs-number">5</span>])</span><br><span class="line">print(y_train[:<span class="hljs-number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>tensor([[1.0000, 0.0192, 0.6049],        [1.0000, 0.0485, 0.2529],        [1.0000, 0.2412, 0.9115],        [1.0000, 0.9764, 0.1665],        [1.0000, 0.9021, 0.5825]])tensor([[0.],        [0.],        [1.],        [1.],        [1.]])</code></pre><h3 id="1-3-Modeling"><a href="#1-3-Modeling" class="headerlink" title="1.3 Modeling"></a>1.3 Modeling</h3><ul><li>Linear Model 형태와 Sigmoid 함수, Loss function 은 cross entropy 를 활용해 모델링을 합니다.</li><li>여기선, 내장되어있는 함수들을 되도록 사용하지 않고, Low level 로 코드를 작성해 보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Low level modeling</span></span><br><span class="line">parameter_W = torch.FloatTensor([[<span class="hljs-number">-0.5</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">1.8</span>]]).view(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">parameter_W.requires_grad_(<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([parameter_W], lr=<span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    <span class="hljs-comment"># Prediction</span></span><br><span class="line">    y_hat = F.sigmoid(torch.matmul(x_train, parameter_W))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Loss function</span></span><br><span class="line">    loss = (-y_train * torch.log(y_hat) - (<span class="hljs-number">1</span> - y_train) * torch.log((<span class="hljs-number">1</span> - y_hat))).sum().mean()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Backprop &amp; update</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch 1000 -- loss 6.368619441986084epoch 2000 -- loss 4.5249152183532715epoch 3000 -- loss 3.654862403869629epoch 4000 -- loss 3.122910261154175epoch 5000 -- loss 2.7545464038848877epoch 6000 -- loss 2.4800000190734863epoch 7000 -- loss 2.2651939392089844epoch 8000 -- loss 2.091233253479004epoch 9000 -- loss 1.9466761350631714epoch 10000 -- loss 1.8241318464279175</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameter_W.data.numpy()</span><br></pre></td></tr></table></figure><pre><code>array([[-16.748823],       [ 16.618748],       [ 17.622692]], dtype=float32)</code></pre><ul><li>아래 그림을 보면, Train이 잘 된 것을 알 수 있습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_scatter(parameter_W.data.numpy(), xy, labels)</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><h2 id="2-Multiclass-Classification"><a href="#2-Multiclass-Classification" class="headerlink" title="2. Multiclass Classification"></a>2. Multiclass Classification</h2><h3 id="2-1-Generate-Data"><a href="#2-1-Generate-Data" class="headerlink" title="2.1 Generate Data"></a>2.1 Generate Data</h3><ul><li>이번에는 3개의 label 을 가지고 있는 classification 을 Modeling 해 보겠습니다.</li><li>또한, High Level 로 pytorch 의 추상 클래스를 이용해 모델링 해보겠습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_scatter</span><span class="hljs-params">(W1, W2, xy, labels)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">for</span> k, color <span class="hljs-keyword">in</span> [(<span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">'r'</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">'y'</span>)]:</span><br><span class="line">        idx = labels.flatten() == k</span><br><span class="line">        plt.scatter(xy[idx, <span class="hljs-number">0</span>], xy[idx, <span class="hljs-number">1</span>], c=color)</span><br><span class="line">        </span><br><span class="line">    x1 = np.linspace(<span class="hljs-number">-0.6</span>, <span class="hljs-number">1.6</span>)</span><br><span class="line">    x2 = -W1[<span class="hljs-number">1</span>] / W1[<span class="hljs-number">2</span>] * x1 - W1[<span class="hljs-number">0</span>] / W1[<span class="hljs-number">2</span>]</span><br><span class="line">    x3 = -W2[<span class="hljs-number">1</span>] / W2[<span class="hljs-number">2</span>] * x1 - W2[<span class="hljs-number">0</span>] / W2[<span class="hljs-number">2</span>]</span><br><span class="line">    plt.plot(x1, x2, <span class="hljs-string">'--k'</span>)</span><br><span class="line">    plt.plot(x1, x3, <span class="hljs-string">'--k'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Generate data</span></span><br><span class="line"></span><br><span class="line">W1 = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">1.0</span>])</span><br><span class="line">W2 = np.array([<span class="hljs-number">-1.</span>/<span class="hljs-number">5</span>, <span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">1.0</span>])</span><br><span class="line"></span><br><span class="line">xy = <span class="hljs-number">2</span> * np.random.rand(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>) - <span class="hljs-number">0.5</span></span><br><span class="line">labels = np.zeros(len(xy))</span><br><span class="line">labels[(W1[<span class="hljs-number">0</span>] + W1[<span class="hljs-number">1</span>] * xy[:, <span class="hljs-number">0</span>] + W1[<span class="hljs-number">2</span>] * xy[:, <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>)] = <span class="hljs-number">1</span></span><br><span class="line">labels[(W2[<span class="hljs-number">0</span>] + W2[<span class="hljs-number">1</span>] * xy[:, <span class="hljs-number">0</span>] + W2[<span class="hljs-number">2</span>] * xy[:, <span class="hljs-number">1</span>] &lt; <span class="hljs-number">0</span>)] = <span class="hljs-number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_scatter(W1, W2, xy, labels)</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><h3 id="2-2-Train-data"><a href="#2-2-Train-data" class="headerlink" title="2.2 Train data"></a>2.2 Train data</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[<span class="hljs-number">1.0</span>, xval, yval] <span class="hljs-keyword">for</span> xval, yval <span class="hljs-keyword">in</span> xy])</span><br><span class="line">y_train = torch.LongTensor(labels)</span><br><span class="line">print(x_train[<span class="hljs-number">-5</span>:])</span><br><span class="line">print(y_train[<span class="hljs-number">-5</span>:])</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 1.0000,  0.9641,  1.3851],        [ 1.0000, -0.4445,  1.0595],        [ 1.0000,  1.0854, -0.1216],        [ 1.0000,  0.8707,  0.1640],        [ 1.0000,  0.7043,  1.3483]])tensor([1, 0, 0, 0, 1])</code></pre><h3 id="2-3-Modeling"><a href="#2-3-Modeling" class="headerlink" title="2.3 Modeling"></a>2.3 Modeling</h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiModel</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model = MultiModel()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="hljs-number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):</span><br><span class="line">    y_hat = model(x_train)</span><br><span class="line">    </span><br><span class="line">    loss = F.cross_entropy(y_hat, y_train)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"epoch &#123;&#125; -- loss &#123;&#125;"</span>.format(epoch, loss.data))</span><br></pre></td></tr></table></figure><pre><code>epoch 1000 -- loss 0.6635385155677795epoch 2000 -- loss 0.5513403415679932epoch 3000 -- loss 0.4890784025192261epoch 4000 -- loss 0.44675758481025696epoch 5000 -- loss 0.4151267111301422epoch 6000 -- loss 0.39017024636268616epoch 7000 -- loss 0.369760125875473epoch 8000 -- loss 0.35262930393218994epoch 9000 -- loss 0.3379631042480469epoch 10000 -- loss 0.3252090811729431</code></pre><h3 id="2-4-Accuracy-계산"><a href="#2-4-Accuracy-계산" class="headerlink" title="2.4 Accuracy 계산"></a>2.4 Accuracy 계산</h3><ul><li>Accuracy 가 96 % 로 비교적 잘 분류 된 것을 확인 할 수 있습니다.</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy = (torch.ByteTensor(model(x_train).max(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] == y_train)).sum().item() / len(y_train)</span><br><span class="line">print(<span class="hljs-string">"Accuracy: &#123;&#125;"</span>.format(accuracy))</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 0.96</code></pre>]]></content:encoded>
      
      <comments>https://emjayahn.github.io/2019/05/06/Basic-Classification-with-Pytorch/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
